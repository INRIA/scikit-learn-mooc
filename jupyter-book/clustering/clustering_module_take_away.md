# Main take-away

## Wrap-up

<!-- Quick wrap-up for the module -->

In this module, we presented the framework used in unsupervised learning with
clustering, focusing on k-means and how to evaluate its results using both
unsupervised and supervised metrics.

We explored the concept of cluster stability, addressed the limitations of
k-means when clusters are not convex, and introduced HDBSCAN as an alternative.

Finally, we showed how clustering can be integrated into supervised pipelines
to perform unsupervised feature engineering.

## To go further

<!-- Some extra links of content to go further -->

You can refer to the following scikit-learn examples which are related to
the concepts approached in this module:

- [Adjustment for chance in clustering performance evaluation](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html)
- [Demonstration of k-means assumptions](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)
- [Clustering text documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html)
