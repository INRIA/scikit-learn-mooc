# Main take-away

## Wrap-up

<!-- Quick wrap-up for the module -->

In this module, we presented the framework used in unsupervised learning with
clustering, focusing on KMeans and how to evaluate its results using both
internal and supervised metrics.

We explored the concept of cluster stability, addressed the limitations of
KMeans when clusters are not convex, and introduced HDBSCAN as an alternative.

Finally, we showed how clustering can be integrated into supervised pipelines
through feature engineering.

## To go further

<!-- Some extra links of content to go further -->

You can refer to the following scikit-learn examples which are related to
the concepts approached in this module:

- [Adjustment for chance in clustering performance evaluation](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html)
- [Demonstration of k-means assumptions](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)
- [Clustering text documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html)
