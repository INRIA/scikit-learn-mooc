1. What is the accuracy score ?


2. What is true positive:

a) the number of positive prediction (sample classified positive by the model)
b) the number of positive label (sample with positive label)
c) the number of positive prediction which have positive label
d) the accuracy


2. What is false positive:

a) 1 - true positive
b) the number of positive prediction which have negative label
c) the number of negative prediction which have positive label
d) neither of the above


3. the confusion matrix is useful if:

a) the class label are imbalanced
b) there are few features
c) there are few samples
d) we want to get insight of the model failure


4. Precision correspond to:

a) true positive rate
b) accuracy
c) ratio of positive prediction over positive label
d) ratio of correct positive prediction over the number of positive prediction
e) ratio of correct prediction over the number of prediction


5. F1 is useful if:

a) the accuracy is too high
b) class label are imbalanced
c) there are few samples


6. the area under the ROC curve:

a) characterize a dataset
b) characterize the performance of a classification model
c) characterize the performance of a regression model
