# âœ… Quiz

```{admonition} Question
What is the accuracy score ?
TODO need choices here
```

```{admonition} Question
What is true positive:

- a) the number of positive prediction (sample classified positive by the model)
- b) the number of positive label (sample with positive label)
- c) the number of positive prediction which have positive label
- d) the accuracy
```

```{admonition} Question
What is false positive:

- a) 1 - true positive
- b) the number of positive prediction which have negative label
- c) the number of negative prediction which have positive label
- d) neither of the above
```

```{admonition} Question
the confusion matrix is useful if:

- a) the class label are imbalanced
- b) there are few features
- c) there are few samples
- d) we want to get insight of the model failure
```

```{admonition} Question
Precision correspond to:

- a) true positive rate
- b) accuracy
- c) ratio of positive prediction over positive label
- d) ratio of correct positive prediction over the number of positive prediction
- e) ratio of correct prediction over the number of prediction
```

TODO: I think we removed f1-score?

```{admonition} Question
F1 is useful if:

- a) the accuracy is too high
- b) class label are imbalanced
- c) there are few samples
```

```{admonition} Question
the area under the ROC curve:

- a) characterize a dataset
- b) characterize the performance of a classification model
- c) characterize the performance of a regression model
```
