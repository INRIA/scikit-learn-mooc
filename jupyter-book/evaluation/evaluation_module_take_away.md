# Main take-away

## Wrap-up

<!-- Quick wrap-up for the module -->

In this notebook, we presented the framework used in machine-learning to
evaluate a predictive model's performance: the cross-validation.

Besides, we presented several splitting strategies that can be used in the
general cross-validation framework. These strategies should be used wisely
when encountering some specific patterns or types of data.

Finally, we show how to perform nested cross-validation to select an optimal
model and evaluate its generalization performance.

## To go further

<!-- Some extra links of content to go further -->
