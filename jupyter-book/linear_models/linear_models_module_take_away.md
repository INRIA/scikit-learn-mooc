# Main take-away

## Wrap-up

<!-- Quick wrap-up for the module -->

In this module, we saw that:

- a linear model has a specific parametrization defined by some weights and an
  intercept;
- linear models require to scale the data before to be trained;
- regularization allows to fight over-fitting;
- the regularization parameter needs to be fine tuned for each application;
- linear models can be used with data presenting non-linear links but require
  extra work such as the use of data augmentation or kernel trick.

## To go further

<!-- Some extra links of content to go further -->

You can refer to the following scikit-learn examples which are related to
the concepts approached during this module:

- [Example of linear regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py)
- [Comparison between a linear regression and a ridge regressor](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py)
