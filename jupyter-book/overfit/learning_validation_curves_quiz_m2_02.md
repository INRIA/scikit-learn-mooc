# âœ… Quiz M2.02

```{admonition} Question
A model is overfitting when:

- a) both the train and test errors are high
- b) train error is low but test error is high
- c) train error is high but the test error is low
- d) both train and test errors are low
```

+++

```{admonition} Question
Assuming that we have a dataset with little noise, a model is underfitting when:

- a) both the train and test errors are high
- b) train error is low but test error is high
- c) train error is high but the test error is low
- d) both train and test errors are low
```

+++

```{admonition} Question
For a fixed training set, by sequentially adding parameters to give more
flexibility to the model, we are more likely to observe:

- a) a wider difference between train and test errors
- b) a reduction in the difference between train and test errors
- c) an increased or steady train error
- d) a decrease in the train error
```

+++

```{admonition} Question
For a fixed choice of model parameters, if we increase the number of labeled
observations in the training set, are we more likely to observe:

- a) a wider difference between train and test errors
- b) a reduction in the difference between train and test errors
- c) an increased or steady train error
- d) a decrease in the train error
```

+++

```{admonition} Question
Polynomial models with a high degree parameter:

- a) always have the best test error (but can be slow to train)
- b) underfit more than linear regression models
- c) get lower training error than lower degree polynomial models
- d) are more likely to overfit than lower degree polynomial models
```

+++

```{admonition} Question
One can always reach zero test error by:

- a) choosing the model parameters to find the best overfitting/underfitting tradeoff
- b) day-dreaming ;)
```
