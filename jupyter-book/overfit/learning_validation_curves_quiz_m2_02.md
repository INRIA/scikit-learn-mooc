# âœ… Quiz M2.02

```{admonition} Question
A model is overfitting when:

- a) both the train and test errors are high
- b) train error is low but test error is high
- c) train error is high but the test error is low
- d) both train and test errors are low

_Select a single answer_
```

+++

```{admonition} Question
Assuming that we have a dataset with little noise, a model is underfitting when:

- a) both the train and test errors are high
- b) train error is low but test error is high
- c) train error is high but the test error is low
- d) both train and test errors are low

_Select a single answer_
```

+++

```{admonition} Question
For a fixed training set, by sequentially adding parameters to give more
flexibility to the model, we are more likely to observe:

- a) a wider difference between train and test errors
- b) a reduction in the difference between train and test errors
- c) an increased or steady train error
- d) a decrease in the train error

_Select all answers that apply_
```

+++

```{admonition} Question
For a fixed choice of model parameters, if we increase the number of labeled
observations in the training set, are we more likely to observe:

- a) a wider difference between train and test errors
- b) a reduction in the difference between train and test errors
- c) an increased or steady train error
- d) a decrease in the train error

_Select all answers that apply_
```

+++

```{admonition} Question
Polynomial models with a high degree parameter:

- a) always have the best test error (but can be slow to train)
- b) underfit more than linear regression models
- c) get lower training error than lower degree polynomial models
- d) are more likely to overfit than lower degree polynomial models

_Select all answers that apply_
```

+++

```{admonition} Question
If we chose the parameters of a model to get the best overfitting/underfitting
tradeoff, we will always get a zero test error.

- a) True
- b) False

_Select a single answer_
```
