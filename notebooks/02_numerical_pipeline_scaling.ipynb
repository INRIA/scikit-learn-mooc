{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for numerical features\n",
    "\n",
    "In this notebook, we present how to build predictive models on tabular\n",
    "datasets, with only numerical features.\n",
    "\n",
    "In particular we will highlight:\n",
    "\n",
    "* an example of preprocessing, namely the **scaling numerical variables**;\n",
    "* using a scikit-learn **pipeline** to chain preprocessing and model\n",
    "  training;\n",
    "* assessed the performance of our model via **cross-validation** instead of\n",
    "  a single train-test split.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Let's charge the full adult census dataset and split the target from the\n",
    "actual data used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first split our dataset to have the target separated from the data\n",
    "used to train our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by selecting only the numerical columns as seen in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    \"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can divide our dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting without preprocessing\n",
    "\n",
    "We will use the logistic regression classifier as in the previous notebook.\n",
    "This time, besides fitting the model, we will also compute the time needed\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not have issues with our model training: it converged in 59 iterations\n",
    "while we gave maximum number of iterations of 100. However, we can give an\n",
    "hint that this model could converge and train faster. In some case, we might\n",
    "even get a `ConvergenceWarning` using the above pattern. We will show such\n",
    "example by reducing the number of maximum iterations allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=50)\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the score is closed to the previous case because our algorithm\n",
    "is closed to the same solution. The warning suggested by scikit-learn\n",
    "provides two solutions to solve this issue:\n",
    "\n",
    "* increase `max_iter` which is indeed what happens in the former case. We let\n",
    "  the algorithm converge, it will take more time but we are sure to get an\n",
    "  optimal model;\n",
    "* standardize the data which is expected to improve convergence.\n",
    "\n",
    "We will investigate the second option.\n",
    "\n",
    "## Model fitting with preprocessing\n",
    "\n",
    "A range of preprocessing algorithms in scikit-learn allow us to transform\n",
    "the input data before training a model. In our case, we will standardize the\n",
    "data and then train a new logistic regression model on that new version of\n",
    "the dataset.\n",
    "\n",
    "Let's start by checking some feature statistics or the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that features in the dataset have very different range. Indeed, having\n",
    "normalized feature is important. Indeed, some algorithms make assumptions\n",
    "regarding feature distributions and having normalized feature is usually one\n",
    "of them.\n",
    "\n",
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p>Some of the reasons for scaling features are:</p>\n",
    "<ul class=\"last simple\">\n",
    "<li>predictor using Euclidean distance (e.g k-nearest  neighbors) should have\n",
    "normalized feature such that each feature contribute equally distance\n",
    "computation;</li>\n",
    "<li>predictor internally using gradient-descent based algorithms\n",
    "(e.g. logistic regression) to find optimal parameters work better\n",
    "and faster simplify choice of parameters as learning-rate;</li>\n",
    "<li>predictor using regularization (e.g. logistic regression) require\n",
    "normalized features such that the penalty is properly applied to the\n",
    "weights.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "We show how to apply such normalization using a scikit-learn transformer\n",
    "called `StandardScaler`. This transformer intend to transform feature such\n",
    "that they will all have a zero mean and a unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled,\n",
    "                                 columns=data_train.columns)\n",
    "data_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily combine these sequential operations with a scikit-learn\n",
    "`Pipeline`, which chains together operations and can be used like any other\n",
    "classifier or regressor. The helper function `make_pipeline` will create a\n",
    "`Pipeline` by giving as arguments the successive transformations to perform\n",
    "followed by the classifier or regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model[-1].n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training time and the number of iterations is much\n",
    "shorter while the predictive performance (accuracy) slightly improved.\n",
    "\n",
    "## Model evaluation using cross-validation\n",
    "\n",
    "In the previous example, we split the original data into a training set\n",
    "and a testing set. This strategy has several issues: in the setting where\n",
    "the amount of data is limited, the subset of data used to train or test will\n",
    "be small; and the splitting was done in a random manner and we have no\n",
    "information regarding the confidence of the results obtained.\n",
    "\n",
    "Instead, we can use cross-validation. Cross-validation consists of repeating\n",
    "this random splitting into training and testing sets and aggregating the\n",
    "model performance. By repeating the experiment, one can get an estimate of\n",
    "the variability of the model performance.\n",
    "\n",
    "The function `cross_val_score` allows for such experimental protocol by\n",
    "giving the model, the data and the target. Since there exists several\n",
    "cross-validation strategies, `cross_val_score` takes a parameter `cv` which\n",
    "defines the splitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, data_numeric, target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by computing the standard-deviation of the cross-validation scores\n",
    "we can get an idea of the uncertainty of our estimation of the predictive\n",
    "performance of the model: in the above results, only the first 2 decimals\n",
    "seem to be trustworthy. Using a single train / test split would not allow us\n",
    "to know anything about the level of uncertainty of the accuracy of the model.\n",
    "\n",
    "Setting `cv=5` created 5 distinct splits to get 5 variations for the training\n",
    "and testing sets. Each training set is used to fit one model which is then\n",
    "scored on the matching test set. This strategy is called K-fold\n",
    "cross-validation where `K` corresponds to the number of splits.\n",
    "\n",
    "The figure helps visualize how the dataset is partitioned into train and test\n",
    "samples at each iteration of the cross-validation procedure:\n",
    "\n",
    "![Cross-validation diagram](../figures/cross_validation_diagram.png)\n",
    "\n",
    "For each cross-validation split, the procedure trains a model on the\n",
    "concatenation of the red samples and evaluate the score of the model by using\n",
    "the blue samples. Cross-validation is therefore computationally intensive\n",
    "because it requires training several models instead of one.\n",
    "\n",
    "Note that the `cross_val_score` method above discards the 5 models that were\n",
    "trained on the different overlapping subset of the dataset. The goal of\n",
    "cross-validation is not to train a model, but rather to estimate\n",
    "approximately the generalization performance of a model that would have been\n",
    "trained to the full training set, along with an estimate of the variability\n",
    "(uncertainty on the generalization accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have:\n",
    "\n",
    "* seen the importance of **scaling numerical variables**;\n",
    "* used a **pipeline** to chain scaling and logistic regression training;\n",
    "* assessed the performance of our model via **cross-validation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
