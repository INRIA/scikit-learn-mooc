{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andreas Mueller, Kyle Kastner, Sebastian Raschka \n",
      "last updated: 2016-06-29 \n",
      "\n",
      "CPython 3.5.1\n",
      "IPython 4.2.0\n",
      "\n",
      "numpy 1.11.0\n",
      "scipy 0.17.1\n",
      "matplotlib 1.5.1\n",
      "scikit-learn 0.17.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a 'Andreas Mueller, Kyle Kastner, Sebastian Raschka' -v -p numpy,scipy,matplotlib,scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy 2016 Scikit-learn Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Model Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issues associated with validation and \n",
    "cross-validation are some of the most important\n",
    "aspects of the practice of machine learning.  Selecting the optimal model\n",
    "for our data is vital -- unfortunately, it is sometimes neglected by machine learning practitioners.\n",
    "\n",
    "Of core importance is the following question:\n",
    "\n",
    "**If our estimator is underperforming, how should we move forward?**\n",
    "\n",
    "- Should we switch to simpler or more complicated models?\n",
    "- Would adding more features to each observed data point help, or should we rather reduce the dimensionality of the dataset?\n",
    "- Would our model benefit from more training samples?\n",
    "\n",
    "The answers are not always intuitive, maybe, are even counter-intuitive.  In particular, **sometimes using a\n",
    "more complicated model will give _worse_ results.**  Also, **sometimes adding\n",
    "training data will not improve your results.**  The ability to determine\n",
    "what steps will improve your model is what separates the successful machine\n",
    "learning practitioners from the unsuccessful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves and Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to address this issue is to use what are often called **Learning Curves**.\n",
    "Given a particular dataset and a model (e.g., using feature creation and linear regression), we'd like to tune the value of our chosen *hyperparameter* ``kernel`` so that the learning algorithm gives us the *best* fit. We can visualize the different regimes with the following plot, modified from the sklearn examples [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import cross_validation\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "n_samples = 200\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "true_fun = lambda X: X ** 3\n",
    "X = np.sort(5 * (rng.rand(n_samples) - .5))\n",
    "y = true_fun(X) + .01 * rng.randn(n_samples)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(kernels)):\n",
    "    ax = plt.subplot(1, len(kernels), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "    model = SVR(kernel=kernels[i], C=5)\n",
    "    model.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_validation.cross_val_score(model,\n",
    "        X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(3 * -.5, 3 * .5, 100)\n",
    "    plt.plot(X_test, model.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((-3 * .5, 3 * .5))\n",
    "    plt.ylim((-1, 1))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Kernel {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        kernels[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves\n",
    "============\n",
    "\n",
    "What the right model for a dataset is depends critically on how much data we have. More data allows us to be more confident about building a complex model. Let's build some intuition on why that is. Look at the following datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "n_samples = 200\n",
    "\n",
    "true_fun = lambda X: X ** 3\n",
    "X = np.sort(5 * (rng.rand(n_samples) - .5))\n",
    "y = true_fun(X) + .01 * rng.randn(n_samples)\n",
    "X = X[:, None]\n",
    "y = y\n",
    "f, axarr = plt.subplots(1, 3)\n",
    "axarr[0].scatter(X[::20], y[::20])\n",
    "axarr[0].set_xlim((-3 * .5, 3 * .5))\n",
    "axarr[0].set_ylim((-1, 1))\n",
    "axarr[1].scatter(X[::10], y[::10])\n",
    "axarr[1].set_xlim((-3 * .5, 3 * .5))\n",
    "axarr[1].set_ylim((-1, 1))\n",
    "axarr[2].scatter(X, y)\n",
    "axarr[2].set_xlim((-3 * .5, 3 * .5))\n",
    "axarr[2].set_ylim((-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all come from the same underlying process. But if you were asked to make a prediction, you would be more likely to draw a straight line for the left-most one, as there are only very few datapoints, and no real rule is apparent. For the dataset in the middle, some structure is recognizable, though the exact shape of the true function is maybe not obvious. With even more data on the right hand side, you would probably be very comfortable with drawing a curved line with a lot of certainty.\n",
    "\n",
    "A great way to explore how a model fit evolves with different dataset sizes are learning curves.\n",
    "A learning curve plots the validation error for a given model against different training set sizes.\n",
    "\n",
    "But first, take a moment to think about what we're going to see:\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- **For an increasing number of training samples, what behavior would you expect to see for the training error?  For the validation error?**\n",
    "- **Would you expect the training error to be higher or lower than the validation error?  Would you ever expect this to change with respect to different training set sizes?**\n",
    "\n",
    "We can run the following code to plot the learning curve for a ``kernel = linear`` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.svm import SVR\n",
    "# This is actually negative MSE!\n",
    "training_sizes, train_scores, test_scores = learning_curve(SVR(kernel='linear'), X, y, cv=10,\n",
    "                                                           scoring=\"mean_squared_error\",\n",
    "                                                           train_sizes=[.6, .7, .8, .9, 1.])\n",
    "# Use the negative because we want to maximize score\n",
    "print(train_scores.mean(axis=1))\n",
    "plt.plot(training_sizes, train_scores.mean(axis=1), label=\"training scores\")\n",
    "plt.plot(training_sizes, test_scores.mean(axis=1), label=\"test scores\")\n",
    "#plt.ylim((0, 50))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for the model with ``kernel = linear``, the validation score doesn't really improve as more data is given.\n",
    "\n",
    "Notice that the validation error *generally improves* with a growing training set,\n",
    "while the training error *generally gets worse* with a growing training set.  From\n",
    "this we can infer that as the training size increases, they will converge to a single\n",
    "value.\n",
    "\n",
    "From the above discussion, we know that `kernel = linear`\n",
    "underfits the data. This is indicated by the fact that both the\n",
    "training and validation errors are very poor. When confronted with this type of learning curve,\n",
    "we can expect that adding more training data will not help matters: both\n",
    "lines will converge to a relatively high error.\n",
    "\n",
    "**When the learning curves have converged to a poor error, we have an underfitting model.**\n",
    "\n",
    "An underfitting model can be improved by:\n",
    "\n",
    "- Using a more sophisticated model (i.e. in this case, increase complexity of the ``kernel`` parameter)\n",
    "- Gather more features for each sample.\n",
    "- Decrease regularization in a regularized model.\n",
    "\n",
    "A underfitting model cannot be improved, however, by increasing the number of training\n",
    "samples (do you see why?)\n",
    "\n",
    "Now let's look at a model that suffers from overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "training_sizes, train_scores, test_scores = learning_curve(SVR(kernel='rbf'), X, y, cv=10,\n",
    "                                                           scoring=\"mean_squared_error\",\n",
    "                                                           train_sizes=[.6, .7, .8, .9, 1.])\n",
    "# Use the negative because we want to minimize squared error\n",
    "plt.plot(training_sizes, train_scores.mean(axis=1), label=\"training scores\")\n",
    "plt.plot(training_sizes, test_scores.mean(axis=1), label=\"test scores\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the learning curve for `kernel = rbf`. From the above\n",
    "discussion, we know that `kernel = rbf` is an estimator\n",
    "which mildly **overfits** the data. This is indicated by the fact that the\n",
    "training error is **much** better than the validation error. As\n",
    "we add more samples to this training set, the training error will\n",
    "continue to worsen, while the cross-validation error will continue\n",
    "to improve, until they meet in the middle. We can infer that adding more\n",
    "data will allow the estimator to very closely match the best\n",
    "possible cross-validation error.\n",
    "\n",
    "**When the learning curves have not yet converged with our full training set, it indicates an overfit model.**\n",
    "\n",
    "An overfitting model can be improved by:\n",
    "\n",
    "- Gathering more training samples.\n",
    "- Using a less-sophisticated model (i.e. in this case, make ``kernel`` less complex with ``kernel = poly``)\n",
    "- Increasing regularization strength (aka a \"penalty against complexity\" -- for example, by decreasing the parameter ``C`` for SVM/SVR, which is an inverse coefficient of the regularization strength).\n",
    "\n",
    "Note that in a scenario where our models suffers from overfitting, gathering more features for each sample will not help the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ve seen above that an under-performing algorithm can be due\n",
    "to two possible situations: underfitting and overfitting.\n",
    "Using the technique of learning curves, we can train on progressively\n",
    "larger subsets of the data, evaluating the training error and\n",
    "cross-validation error to determine whether our algorithm is overfitting or underfitting. But what do we do with this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "If our algorithm is **underfitting**, the following actions might help:\n",
    "\n",
    "- **Add more features**. It may be helpful to add more informative features to the dataset. For example, to predict housing prices,\n",
    "  features such as the neighborhood\n",
    "  the house is in, the year the house was built, the size of the lot, etc.\n",
    "  can help the model by providing additional dimensions that may provide useful, discriminatory information to predict house prices. Adding these features to the training and test sets can improve\n",
    "  the fit.\n",
    "- **Use a more sophisticated model**. Adding complexity to the model can\n",
    "  help improving the fit. For a SVR fit, this can be accomplished\n",
    "  by increasing the kernel complexity (generally ``linear`` << ``poly`` << ``rbf``).\n",
    "  Each learning technique has its own methods of adding complexity.\n",
    "- **Use fewer samples**. Although this will not improve the classification performance,\n",
    "  an underfitting algorithm can attain nearly the same error with a smaller\n",
    "  training set size. For algorithms that are computationally expensive,\n",
    "  reducing the training set size can lead to very large runtime improvements.\n",
    "- **Decrease regularization**. Regularization is a technique used to impose\n",
    "  simplicity in some machine learning models by adding a penalty term that\n",
    "  depends on the characteristics of the parameters. If a model is underfitting,\n",
    "  decreasing the regularization can lead to better results.\n",
    "  \n",
    "### Overfitting\n",
    "\n",
    "If our algorithm shows signs of **overfitting**, the following actions might help:\n",
    "\n",
    "- **Use fewer features**. Using a feature selection technique may be\n",
    "  useful and decrease the overfitting of the estimator.\n",
    "- **Use a simpler model**.  Model complexity and overfitting go hand-in-hand.\n",
    "   For example, models like unpruned decision trees tend to overfit\n",
    "   much more readily than linear models and SVMs.\n",
    "- **Use more training samples**. Adding training samples can reduce\n",
    "  the effect of overfitting.\n",
    "- **Increase Regularization**. Regularization is designed to prevent\n",
    "  overfitting. So increasing regularization\n",
    "  can lead to better results for overfitting models.\n",
    "\n",
    "These choices become very important in real-world situations, since data collection usually\n",
    "costs time, money, or other resources. If the model is underfitting, spending weeks or months collecting more data could be a colossal waste of time! However, more data (usually) gives us a better view of the true nature of the problem; so, these issues should always be carefully considered before going on a \"data foraging expedition\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
