{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02\n",
    "The goal is to find the best set of hyper-parameters which maximize the\n",
    "performance on a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "# Or use the local copy:\n",
    "# df = pd.read_csv('../datasets/adult-census.csv')\n",
    "\n",
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "data = df.drop(columns=target_name)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: create your machine learning pipeline\n",
    "\n",
    "You should:\n",
    "* preprocess the categorical columns using a `OneHotEncoder` and use a\n",
    "  `StandardScaler` to normalize the numerical data.\n",
    "* use a `LogisticRegression` as a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Start by defining the columns and the preprocessing pipelines to be applied\n",
    "on each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'native-country', 'sex']\n",
    "\n",
    "categories = [data[column].unique()\n",
    "              for column in data[categorical_columns]]\n",
    "\n",
    "numerical_columns = [\n",
    "    'age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_processor = OneHotEncoder(categories=categories)\n",
    "numerical_processor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Subsequently, create a `ColumnTransformer` to redirect the specific columns\n",
    "a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [('cat-preprocessor', categorical_processor, categorical_columns),\n",
    "     ('num-preprocessor', numerical_processor, numerical_columns)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Finally, concatenate the preprocessing pipeline with a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `RandomizedSearchCV` to find the best set of hyper-parameters by tuning\n",
    "the following parameters for the `LogisticRegression` model:\n",
    "- `C` with values ranging from 0.001 to 10. You can use a reciprocal\n",
    "  distribution (i.e. `scipy.stats.reciprocal`);\n",
    "- `solver` with possible values being `\"liblinear\"` and `\"lbfgs\"`;\n",
    "- `penalty` with possible values being `\"l2\"` and `\"l1\"`;\n",
    "In addition, try several preprocessing strategies with the `OneHotEncoder`\n",
    "by always (or not) dropping the first column when encoding the categorical\n",
    "data.\n",
    "\n",
    "Notes: You can accept failure during a grid-search or a randomized-search\n",
    "by settgin `error_score` to `np.nan` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal\n",
    "\n",
    "param_distributions = {\n",
    "    \"logisticregression__C\": reciprocal(0.001, 10),\n",
    "    \"logisticregression__solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"logisticregression__penalty\": [\"l2\", \"l1\"],\n",
    "    \"columntransformer__cat-preprocessor__drop\": [None, \"first\"]\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions,\n",
    "    n_iter=200, error_score=np.nan, n_jobs=-1)\n",
    "model_random_search.fit(df_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_results = [f\"param_{name}\"for name in param_distributions.keys()]\n",
    "column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "\n",
    "cv_results = pd.DataFrame(model_random_search.cv_results_)\n",
    "cv_results = cv_results[column_results].sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results = cv_results.rename(\n",
    "    columns={\"param_logisticregression__C\": \"C\",\n",
    "             \"param_logisticregression__solver\": \"solver\",\n",
    "             \"param_logisticregression__penalty\": \"penalty\",\n",
    "             \"param_columntransformer__cat-preprocessor__drop\": \"drop\",\n",
    "             \"mean_test_score\": \"mean test accuracy\",\n",
    "             \"rank_test_score\": \"ranking\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "cv_results[\"drop\"] = cv_results[\"drop\"].fillna(\"None\")\n",
    "cv_results = cv_results.dropna(\"index\").drop(columns=[\"solver\"])\n",
    "encoding = {}\n",
    "for col in cv_results:\n",
    "    if cv_results[col].dtype.kind == 'O':\n",
    "        labels, uniques = pd.factorize(cv_results[col])\n",
    "        cv_results[col] = labels\n",
    "        encoding[col] = uniques\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    cv_results.drop(columns=[\"ranking\", \"std_test_score\"]),\n",
    "    color=\"mean test accuracy\",\n",
    "    dimensions=[\"C\", \"penalty\", \"drop\",\n",
    "                \"mean test accuracy\"],\n",
    "    color_continuous_scale=px.colors.diverging.Tealrose,\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "python_scripts//py:percent,notebooks//ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
