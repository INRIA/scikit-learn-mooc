{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to hyper-parameter tuning\n",
    "\n",
    "The process of learning a predictive model is driven by a set of internal\n",
    "parameters and a set of training data. These internal parameters are called\n",
    "hyper-parameters and are specific for each family of models. In addition, a\n",
    "specific set of hyper-parameters are optimal for a specific dataset and thus\n",
    "they need to be optimized. In this notebook we will use the words\n",
    "\"hyper-parameters\" and \"parameters\" interchangeably\n",
    "\n",
    "This notebook shows:\n",
    "* the influence of changing model hyper-parameters;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reload the dataset as we did previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=[target_name, \"fnlwgt\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we split it into a training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the preprocessing pipeline to transform differently\n",
    "the numerical and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'native-country', 'sex']\n",
    "\n",
    "categories = [\n",
    "    data[column].unique() for column in data[categorical_columns]]\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(categories=categories)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat-preprocessor', categorical_preprocessor,\n",
    "     categorical_columns),], remainder='passthrough',\n",
    "                                 sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a tree-based classifier (i.e. histogram gradient-boosting) to\n",
    "predict whether or not a person earns more than 50,000 dollars a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# for the moment this line is required to import HistGradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",\n",
    "     HistGradientBoostingClassifier(random_state=42))])\n",
    "model.fit(df_train, target_train)\n",
    "\n",
    "print(\n",
    "    f\"The test accuracy score of the gradient boosting pipeline is: \"\n",
    "    f\"{model.score(df_test, target_test):.2f}\")\n",
    "\n",
    "# In the previous example, we created an histogram gradient-boosting classifier\n",
    "# using the default parameters by omitting to explicitely set these parameters.\n",
    "#\n",
    "# However, there is no reason that these parameters are optimal for our\n",
    "# dataset. For instance, we can try to set the `learning_rate` parameter and\n",
    "# see how it changes the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",\n",
    "     HistGradientBoostingClassifier(random_state=42, learning_rate=1e-3))\n",
    "])\n",
    "model.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The test accuracy score of the gradient boosting pipeline is: \"\n",
    "    f\"{model.score(df_test, target_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",\n",
    "     HistGradientBoostingClassifier(random_state=42, learning_rate=10))\n",
    "])\n",
    "model.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The test accuracy score of the gradient boosting pipeline is: \"\n",
    "    f\"{model.score(df_test, target_test):.2f}\")\n",
    "\n",
    "# # %% [markdown]\n",
    "# ## Quizz\n",
    "#\n",
    "# 1. What is the default value of the `learning_rate` parameter of the\n",
    "# `HistGradientBoostingClassifier` class? ([link to the API documentation](\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html))\n",
    "#\n",
    "# 2. Decrease progressively value of `learning_rate`: can you find a value that\n",
    "# yields an accuracy higher than with the default learning rate?\n",
    "#\n",
    "# 3. Fix `learning_rate` to 0.05 and try setting the value of `max_leaf_nodes`\n",
    "# to the minimum value of 2. Does it improve the accuracy?\n",
    "#\n",
    "# 4. Try to progressively increase the value of `max_leaf_nodes` to 256 by\n",
    "# taking powers of 2. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "title": "Actually scikit-learn estimators have a `set_params` method"
   },
   "source": [
    "that allows you to change the parameter of a model after it has been created.\n",
    "For example, we can set the `learning_rate=1e-3` and check that we get the\n",
    "same score as previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model.set_params(classifier__learning_rate=1e-3)\n",
    "model.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The test accuracy score of the gradient boosting pipeline is: \"\n",
    "    f\"{model.score(df_test, target_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model of interest is a `Pipeline`, the parameter names are of the\n",
    "form `<model_name>__<parameter_name>` (note the double underscore in the\n",
    "middle). In our case, `classifier` comes from the `Pipeline` definition and\n",
    "`learning_rate` is the parameter name of `HistGradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you can use the `get_params` method on scikit-learn models to\n",
    "list all the parameters with their values. For example, if you want to\n",
    "get all the parameter names, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.get_params():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.get_params` returns a `dict` whose keys are the parameter names and whose\n",
    "values are the parameter values. If you want to get the value of a single\n",
    "parameter, for example `classifier__learning_rate`, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()['classifier__learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have seen:\n",
    "- how hyper-parameters can affect the performance of a model\n",
    "- how to use `get_params` and `set_params` to get the parameters of a model\n",
    "  and set them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
