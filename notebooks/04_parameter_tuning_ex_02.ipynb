{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02\n",
    "\n",
    "The goal is to find the best set of hyper-parameters which maximize the\n",
    "performance on a training set.\n",
    "\n",
    "Here again with limit the size of the training set to make computation\n",
    "run faster. Feel free to increase the `train_size` value if your computer\n",
    "is powerful enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/adult-census.csv\")\n",
    "\n",
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=[target_name, \"fnlwgt\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: create your machine learning pipeline\n",
    "\n",
    "You should:\n",
    "* preprocess the categorical columns using a `OneHotEncoder` and use a\n",
    "  `StandardScaler` to normalize the numerical data.\n",
    "* use a `LogisticRegression` as a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Start by defining the columns and the preprocessing pipelines to be applied\n",
    "on each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Subsequently, create a `ColumnTransformer` to redirect the specific columns\n",
    "a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Finally, concatenate the preprocessing pipeline with a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: make your random search\n",
    "\n",
    "Use a `RandomizedSearchCV` to find the best set of hyper-parameters by tuning\n",
    "the following parameters for the `LogisticRegression` model:\n",
    "- `C` with values ranging from 0.001 to 10. You can use a reciprocal\n",
    "  distribution (i.e. `scipy.stats.reciprocal`);\n",
    "- `solver` with possible values being `\"liblinear\"` and `\"lbfgs\"`;\n",
    "- `penalty` with possible values being `\"l2\"` and `\"l1\"`;\n",
    "\n",
    "In addition, try several preprocessing strategies with the `OneHotEncoder`\n",
    "by always (or not) dropping the first column when encoding the categorical\n",
    "data.\n",
    "\n",
    "Notes: some combinations of the hyper-parameters proposed above are invalid.\n",
    "You can make the parameter search accept such failures by setting `error_score`\n",
    "to `np.nan`. The warning messages give more details on which parameter\n",
    "combinations but the computation will proceed.\n",
    "\n",
    "Once the computation has completed, print the best combination of parameters\n",
    "stored in the `best_params_` attribute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
