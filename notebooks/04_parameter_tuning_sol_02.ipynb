{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 02\n",
    "\n",
    "The goal is to find the best set of hyperparameters which maximize the\n",
    "performance on a training set.\n",
    "\n",
    "Here again with limit the size of the training set to make computation\n",
    "run faster. Feel free to increase the `train_size` value if your computer\n",
    "is powerful enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/adult-census.csv\")\n",
    "\n",
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=[target_name, \"fnlwgt\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, train_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should:\n",
    "* preprocess the categorical columns using a `OneHotEncoder` and use a\n",
    "  `StandardScaler` to normalize the numerical data.\n",
    "* use a `LogisticRegression` as a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Start by defining the columns and the preprocessing pipelines to be applied\n",
    "on each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "categories = [data[column].unique()\n",
    "              for column in data[categorical_columns]]\n",
    "\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "numerical_columns = numerical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_processor = OneHotEncoder(categories=categories,\n",
    "                                      drop=\"if_binary\")\n",
    "numerical_processor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Subsequently, create a `ColumnTransformer` to redirect the specific columns\n",
    "a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [('cat-preprocessor', categorical_processor, categorical_columns),\n",
    "     ('num-preprocessor', numerical_processor, numerical_columns)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Finally, concatenate the preprocessing pipeline with a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `RandomizedSearchCV` to find the best set of hyperparameters by tuning\n",
    "the following parameters for the `LogisticRegression` model:\n",
    "- `C` with values ranging from 0.001 to 10. You can use a reciprocal\n",
    "  distribution (i.e. `scipy.stats.reciprocal`);\n",
    "- `solver` with possible values being `\"liblinear\"` and `\"lbfgs\"`;\n",
    "- `penalty` with possible values being `\"l2\"` and `\"l1\"`;\n",
    "\n",
    "In addition, try several preprocessing strategies with the `OneHotEncoder`\n",
    "by always (or not) dropping the first column when encoding the categorical\n",
    "data.\n",
    "\n",
    "Notes: some combinations of the hyperparameters proposed above are invalid.\n",
    "You can make the parameter search accept such failures by setting\n",
    "`error_score` to `np.nan`. The warning messages give more details on which\n",
    "parameter combinations but the computation will proceed.\n",
    "\n",
    "Once the computation has completed, print the best combination of parameters\n",
    "stored in the `best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal\n",
    "\n",
    "param_distributions = {\n",
    "    \"logisticregression__C\": reciprocal(0.001, 10),\n",
    "    \"logisticregression__solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"logisticregression__penalty\": [\"l2\", \"l1\"],\n",
    "    \"columntransformer__cat-preprocessor__drop\": [None, \"first\"]\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions,\n",
    "    n_iter=20, error_score=np.nan, n_jobs=2, verbose=1)\n",
    "model_random_search.fit(df_train, target_train)\n",
    "model_random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `cv_results = model_random_search.cv_results_` in the plot at\n",
    "the end of this notebook (you are more than welcome to try!). Instead we are\n",
    "going to load the results obtained from a similar search with many more\n",
    "iterations (200 instead of 20).\n",
    "\n",
    "This way we can have a more detailed plot while being able to run this\n",
    "notebook in a reasonably short amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if you want to regenerate the results csv file. This\n",
    "# can take a long time to execute.\n",
    "#\n",
    "# model_random_search = RandomizedSearchCV(\n",
    "#     model, param_distributions=param_distributions,\n",
    "#     n_iter=200, error_score=np.nan, n_jobs=-1)\n",
    "# _ = model_random_search.fit(df_train, target_train)\n",
    "# cv_results =  pd.DataFrame(model_random_search.cv_results_)\n",
    "# cv_results.to_csv(\"../figures/randomized_search_results_logistic_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.read_csv(\n",
    "    \"../figures/randomized_search_results_logistic_regression.csv\",\n",
    "    index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_results = [f\"param_{name}\"for name in param_distributions.keys()]\n",
    "column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "\n",
    "cv_results = cv_results[column_results].sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results = (\n",
    "    cv_results\n",
    "    .rename(columns={\n",
    "        \"param_logisticregression__C\": \"C\",\n",
    "         \"param_logisticregression__solver\": \"solver\",\n",
    "         \"param_logisticregression__penalty\": \"penalty\",\n",
    "         \"param_columntransformer__cat-preprocessor__drop\": \"drop\",\n",
    "         \"mean_test_score\": \"mean test accuracy\",\n",
    "         \"rank_test_score\": \"ranking\"})\n",
    "    .astype(dtype={'C': 'float64'})\n",
    ")\n",
    "cv_results['log C'] = np.log(cv_results['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"drop\"] = cv_results[\"drop\"].fillna(\"None\")\n",
    "cv_results = cv_results.dropna(\"index\").drop(columns=[\"solver\"])\n",
    "encoding = {}\n",
    "for col in cv_results:\n",
    "    if cv_results[col].dtype.kind == 'O':\n",
    "        labels, uniques = pd.factorize(cv_results[col])\n",
    "        cv_results[col] = labels\n",
    "        encoding[col] = uniques\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    cv_results.drop(columns=[\"ranking\", \"std_test_score\"]),\n",
    "    color=\"mean test accuracy\",\n",
    "    dimensions=[\"log C\", \"penalty\", \"drop\",\n",
    "                \"mean test accuracy\"],\n",
    "    color_continuous_scale=px.colors.diverging.Tealrose,\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
