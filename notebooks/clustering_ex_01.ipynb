{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcdd Exercise M4.01\n",
    "\n",
    "In this exercise we investigate the stability of the k-means algorithm. For\n",
    "such purpose, we use the RFM Dataset. RFM is a method used for analyzing\n",
    "customer value and the acronym RFM stands for the three dimensions:\n",
    "\n",
    "- Recency: How recently did the customer purchase;\n",
    "- Frequency: How often do they purchase;\n",
    "- Monetary Value: How much do they spend.\n",
    "\n",
    "It is commonly used in marketing and has received particular attention in\n",
    "retail and professional services industries as well. Here we subsample the\n",
    "dataset to ease the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/rfm_segmentation.csv\")\n",
    "data = data.sample(n=2000, random_state=0).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the data using a seborn `pairplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As k-means clustering relies on computing distances between samples, in\n",
    "general we need to scale our data before training the clustering model.\n",
    "\n",
    "Modify the color of the `pairplot` to represent the cluster labels as\n",
    "predicted by `KMeans` without any scaling. Try different values for\n",
    "`n_clusters`, for instance, `n_clusters_values=[2, 3, 4]`. Do all features\n",
    "contribute equally to forming the clusters in their original scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline composed by a `StandardScaler` followed by a `KMeans` step\n",
    "# as the final predictor. Set the `random_state` of `KMeans` for\n",
    "# reproducibility. Then, make a plot of the WCSS or inertia for `n_clusters`\n",
    "# varying from 1 to 10. You can use the following helper function for such\n",
    "# purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def plot_n_clusters_scores(\n",
    "    model,\n",
    "    data,\n",
    "    score_type=\"inertia\",\n",
    "    n_clusters_values=None,\n",
    "    alpha=1.0,\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots clustering scores (inertia or silhouette) for a range of n_clusters.\n",
    "\n",
    "    Parameters:\n",
    "        model: A pipeline whose last step has a `n_clusters` hyperparameter.\n",
    "        data: The input data to cluster.\n",
    "        score_type: \"inertia\" or \"silhouette\" to decide which score to compute.\n",
    "        alpha: Transparency of the plot line, useful when several plots overlap.\n",
    "        title: Optional title to set; default title used if None.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    if n_clusters_values is None:\n",
    "        if score_type == \"inertia\":\n",
    "            n_clusters_values = range(1, 11)\n",
    "        else:\n",
    "            n_clusters_values = range(2, 11)\n",
    "\n",
    "    for n_clusters in n_clusters_values:\n",
    "        model[-1].set_params(n_clusters=n_clusters)\n",
    "        if score_type == \"inertia\":\n",
    "            ylabel = \"WCSS (inertia)\"\n",
    "            model.fit(data)\n",
    "            scores.append(model[-1].inertia_)\n",
    "        elif score_type == \"silhouette\":\n",
    "            ylabel = \"Silhouette score\"\n",
    "            cluster_labels = model.fit_predict(data)\n",
    "            data_transformed = model[:-1].transform(data)\n",
    "            score = silhouette_score(data_transformed, cluster_labels)\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"score_type must be either 'inertia' or 'silhouette'\"\n",
    "            )\n",
    "\n",
    "    plt.plot(n_clusters_values, scores, color=\"tab:blue\", alpha=alpha)\n",
    "    plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    _ = plt.title(title or f\"{ylabel} for varying n_clusters\", y=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the best choice of n_clusters remains stable when resampling\n",
    "the dataset. For such purpose:\n",
    "- Keep a fixed `random_state` for the `KMeans` step to isolate the effect of\n",
    "  data resampling.\n",
    "- Generate resamplings consisting of 50% of the data by using\n",
    "  `train_test_split` with `train_size=0.5`. Changing the `random_state`\n",
    "  to do the split leads to different resamplings.\n",
    "- Use the `plot_n_clusters_scores` function inside a `for` loop to make\n",
    "  multiple overlapping plots of the inertia, each time using a different\n",
    "  resampling. 10 resamplings should be enough to draw conclusions.\n",
    "\n",
    "Is the elbow (optimal number of clusters) stable across all different\n",
    "resamplings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `KMeans` uses a smart selection of the initial centroids called\n",
    "\"k-means++\". Instead of picking points completly at random, it tries several\n",
    "candidate centroids at each step and picks the best ones based on an\n",
    "estimation of how much they would help reduce the overall inertia. This method\n",
    "improves the chances of finding better cluster centroids and speeds up\n",
    "convergence compared to random initialization.\n",
    "\n",
    "Because \"k-means++\" already does a good job of finding suitable centroids, a\n",
    "single initialization is typically sufficient for most cases. That is why the\n",
    "parameter `n_init` in scikit-learn (which controls the number of times the\n",
    "algorithm is run with different centroid initializations) is set to 1 by\n",
    "default when `init=\"k-means++\"`. Nevertheless, there may be cases (as when\n",
    "data is unevenly distributed) where increasing `n_init` may help ensuring a\n",
    "global minimal inertia.\n",
    "\n",
    "Repeat the previous example but setting `n_init=5`. Remeber to fix the\n",
    "`random_state` for the `KMeans` initialization to only estimate the\n",
    "variability related to resamplings of the data. Are the resulting inertia\n",
    "curves more stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment, but this time determine if the optimal number of\n",
    "clusters (with `StandarScaler` and `n_init=5`) is stable across subsamplings\n",
    "in terms of the `silhouette_score`. Be aware that computing the silhouette\n",
    "score is more computationally costly than computing the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again repeat the experiment to determine the stability of the optimal\n",
    "number of clusters. This time, instead of using a `StandardScaler`, use a\n",
    "`QuantileTransformer` with default parameters as the preprocessing step in the\n",
    "pipeline. For the `KMeans` step, keep `n_init=5` and a fixed `random_state`.\n",
    "What happens in terms of silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "model = make_pipeline(QuantileTransformer(), KMeans(n_init=5, random_state=0))\n",
    "for random_state in range(1, 11):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data, train_size=0.5, random_state=random_state\n",
    "    )\n",
    "    plot_n_clusters_scores(\n",
    "        model,\n",
    "        data_subsample,\n",
    "        score_type=\"silhouette\",\n",
    "        alpha=0.2,\n",
    "        title=\"Stability of silhouette score\\nwith n_init=5 and QuantileTransformer\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}