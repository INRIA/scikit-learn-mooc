{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcdd Exercise M4.01\n",
    "\n",
    "In this exercise we investigate the stability of the k-means algorithm. For\n",
    "such purpose, we use the RFM Dataset. RFM is a method used for analyzing\n",
    "customer value and the acronym RFM stands for the three dimensions:\n",
    "\n",
    "- Recency: How recently did the customer purchase;\n",
    "- Frequency: How often do they purchase;\n",
    "- Monetary Value: How much do they spend.\n",
    "\n",
    "It is commonly used in marketing and has received particular attention in\n",
    "retail and professional services industries as well. Here we subsample the\n",
    "dataset to ease the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/rfm_segmentation.csv\")\n",
    "data = data.sample(n=2000, random_state=0).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the data using a seaborn `pairplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As k-means clustering relies on computing distances between samples, in\n",
    "general we need to scale our data before training the clustering model.\n",
    "\n",
    "Modify the color of the `pairplot` to represent the cluster labels as\n",
    "predicted by `KMeans` without any scaling. Try different values for\n",
    "`n_clusters`, for instance, `n_clusters_values=[2, 3, 4]`. Do all features\n",
    "contribute equally to forming the clusters in their original scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline composed by a `StandardScaler` followed by a `KMeans` step\n",
    "as the final predictor. Set the `random_state` of `KMeans` for\n",
    "reproducibility. Then, make a plot of the WCSS or inertia for `n_clusters`\n",
    "varying from 1 to 10. You can use the following helper function for such\n",
    "purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def plot_n_clusters_scores(\n",
    "    model,\n",
    "    data,\n",
    "    score_type=\"inertia\",\n",
    "    n_clusters_values=None,\n",
    "    alpha=1.0,\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots clustering scores (inertia or silhouette) for a range of n_clusters.\n",
    "\n",
    "    Parameters:\n",
    "        model: A pipeline whose last step has a `n_clusters` hyperparameter.\n",
    "        data: The input data to cluster.\n",
    "        score_type: \"inertia\" or \"silhouette\" to decide which score to compute.\n",
    "        alpha: Transparency of the plot line, useful when several plots overlap.\n",
    "        title: Optional title to set; default title used if None.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    if n_clusters_values is None:\n",
    "        if score_type == \"inertia\":\n",
    "            n_clusters_values = range(1, 11)\n",
    "        else:\n",
    "            n_clusters_values = range(2, 11)\n",
    "\n",
    "    for n_clusters in n_clusters_values:\n",
    "        model[-1].set_params(n_clusters=n_clusters)\n",
    "        if score_type == \"inertia\":\n",
    "            ylabel = \"WCSS (inertia)\"\n",
    "            model.fit(data)\n",
    "            scores.append(model[-1].inertia_)\n",
    "        elif score_type == \"silhouette\":\n",
    "            ylabel = \"Silhouette score\"\n",
    "            cluster_labels = model.fit_predict(data)\n",
    "            data_transformed = model[:-1].transform(data)\n",
    "            score = silhouette_score(data_transformed, cluster_labels)\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"score_type must be either 'inertia' or 'silhouette'\"\n",
    "            )\n",
    "\n",
    "    plt.plot(n_clusters_values, scores, color=\"tab:blue\", alpha=alpha)\n",
    "    plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    _ = plt.title(title or f\"{ylabel} for varying n_clusters\", y=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find one or more stable candidates for `n_clusters` using\n",
    "the elbow method when resampling the dataset. For such purpose:\n",
    "- Generate randomly resampled data consisting of 50% of the data by using\n",
    "  `train_test_split` with `train_size=0.5`. Changing the `random_state` to do\n",
    "  the split leads to different samples.\n",
    "- Use the `plot_n_clusters_scores` function inside a `for` loop to make\n",
    "  multiple overlapping plots of the inertia, each time using a different\n",
    "  resampling. 10 resampling iterations should be enough to draw conclusions.\n",
    "- You can choose to set the `random_state` value of the `KMeans` step, but be\n",
    "  aware that even if we fix `random_state=0` in all resampling iterations,\n",
    "  k-means will still choose different initial centroids for different data\n",
    "  samples, so fixing it or not should not change the conclusions w.r.t. to\n",
    "  stability to resampling.\n",
    "\n",
    "Is the elbow (optimal number of clusters) stable when resampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `KMeans` uses a smart selection of the initial centroids called\n",
    "\"k-means++\". Instead of picking points completely at random, it tries several\n",
    "candidate centroids at each step and picks the best ones based on an\n",
    "estimation of how much they would help reduce the overall inertia. This method\n",
    "improves the chances of finding better cluster centroids and speeds up\n",
    "convergence compared to random initialization.\n",
    "\n",
    "Because \"k-means++\" already does a good job of finding suitable centroids, a\n",
    "single initialization is typically sufficient for most cases. That is why the\n",
    "parameter `n_init` in scikit-learn (which controls the number of times the\n",
    "algorithm is run with different centroid initializations) is set to 1 by\n",
    "default when `init=\"k-means++\"`. Nevertheless, there may be cases (as when\n",
    "data is unevenly distributed) where increasing `n_init` may help ensuring a\n",
    "global minimal inertia.\n",
    "\n",
    "Repeat the previous example but setting `n_init=5`. Remember to fix the\n",
    "`random_state` for the `KMeans` initialization to only estimate the\n",
    "variability related to the resampling of the data. Are the resulting inertia\n",
    "curves more stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment, but this time determine if the optimal number of\n",
    "clusters (with `StandarScaler` and `n_init=5`) is stable across subsamplings\n",
    "in terms of the `silhouette_score`. Be aware that computing the silhouette\n",
    "score is more computationally costly than computing the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once again repeat the experiment to determine the stability of the optimal\n",
    "number of clusters. This time, instead of using a `StandardScaler`, use a\n",
    "`QuantileTransformer` with default parameters as the preprocessing step in\n",
    "the pipeline. Contrary to `StandardScaler`, `QuantileTransformer` is a\n",
    "nonlinear transformation that maps the features with a long tail\n",
    "distributions to a uniform distribution, which is the case for the\n",
    "\"frequency\" and \"monetary\" features in the RFM dataset.\n",
    "\n",
    "For the `KMeans` step, keep `n_init=5`.\n",
    "\n",
    "What happens in terms of silhouette score? Does this make it possible to\n",
    "identify stable and qualitatively interesting clusters in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}