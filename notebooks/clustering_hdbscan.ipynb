{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-convex clustering using HDBSCAN\n",
    "\n",
    "We have previously mentioned that k-means consists of minimizing the samples\n",
    "euclidean distances to their assigned centroid. As a consequence, k-means is\n",
    "more appropriate for clusters that are isotropic and normally distributed\n",
    "(look like blobs). In this notebook we introduce another clustering technique\n",
    "named HDBSCAN, an acronym which stands for \"Hierarchical Density-Based Spatial\n",
    "Clustering of Applications with Noise\".\n",
    "\n",
    "Let's explain each of those tearms. HDBSCAN is hierarchical, which means it\n",
    "handles data with clusters nested within each other. The user can decide at\n",
    "what level of the hierarchy want to cut off the clusters.\n",
    "\n",
    "It is density-based (and therefore non-parametric, contrary to K-means)\n",
    "because it does not assume a specific shape or number of clusters. Instead, it\n",
    "automatically finds the clusters based on areas where data points are densely\n",
    "packed together. In other words, it looks for regions of high density (many\n",
    "data points close to each other) and forms clusters around them. This allows\n",
    "it to find clusters of varying shapes and sizes.\n",
    "\n",
    "HDBSCAN assigns a label of -1 to points that do not have enough neighbors (low\n",
    "density) to be considered part of a cluster or are too far from any dense\n",
    "region (too isolated from core points). They are usually considered to be\n",
    "noise.\n",
    "\n",
    "Let's first illustrate those concepts with a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "centers = np.array([[-4.8, 2.0], [-3.5, -4.5]])\n",
    "X_gaussian, _ = make_blobs(\n",
    "    n_samples=[200, 60],\n",
    "    centers=centers,\n",
    "    cluster_std=[1.0, 0.5],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Two anisotropic blobs\n",
    "centers = np.array([[1.0, 5.1], [3.0, 0.9]])\n",
    "X_aniso_base, y_aniso_base = make_blobs(\n",
    "    n_samples=200, centers=centers, random_state=0\n",
    ")\n",
    "\n",
    "# Define two different transformations\n",
    "transformation_0 = np.array([[0.6, -0.6], [-0.4, 0.8]])\n",
    "transformation_1 = np.array([[1.5, 0], [0, 0.3]])\n",
    "\n",
    "# Apply different transformations to each blob\n",
    "X_aniso = np.copy(X_aniso_base)\n",
    "X_aniso[y_aniso_base == 0] = np.dot(\n",
    "    X_aniso_base[y_aniso_base == 0], transformation_0\n",
    ")\n",
    "X_aniso[y_aniso_base == 1] = np.dot(\n",
    "    X_aniso_base[y_aniso_base == 1], transformation_1\n",
    ")\n",
    "\n",
    "\n",
    "def make_wavy_blob(n_samples, shift=0.0, noise=0.2, freq=3):\n",
    "    \"Make wavy blobs in feature space\"\n",
    "    x = np.linspace(-3, 3, n_samples)\n",
    "    y = np.sin(freq * x) + shift\n",
    "    x += rng.normal(scale=noise, size=n_samples)\n",
    "    y += rng.normal(scale=noise, size=n_samples)\n",
    "    return np.vstack((x, y)).T\n",
    "\n",
    "\n",
    "X_wave1 = make_wavy_blob(100, shift=4.7, freq=1)\n",
    "transformation = np.array([[0.6, -0.6], [0.4, 0.8]])\n",
    "X_wave1 = np.dot(X_wave1, transformation)\n",
    "X_wave2 = make_wavy_blob(200, shift=-2.0, freq=2)\n",
    "\n",
    "\n",
    "X_noise = rng.uniform(low=-8, high=8, size=(100, 2))  # background noise\n",
    "\n",
    "X_all = np.vstack((X_gaussian, X_aniso, X_wave1, X_wave2, X_noise))\n",
    "\n",
    "plt.scatter(X_all[:, 0], X_all[:, 1], alpha=0.6)\n",
    "_ = plt.title(\"Synthetic dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to find a cluster structure using K-means with 6 clusters to\n",
    "match our data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_labels = KMeans(n_clusters=6, random_state=0).fit_predict(X_all)\n",
    "_ = plt.scatter(X_all[:, 0], X_all[:, 1], c=cluster_labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to increase the number of clusters to avoid grouping\n",
    "unrelated points in the same cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = KMeans(n_clusters=10, random_state=0).fit_predict(X_all)\n",
    "_ = plt.scatter(X_all[:, 0], X_all[:, 1], c=cluster_labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can observe that too many cluster divides the high density region\n",
    "too much, while it continues grouping unrelated points together. Therefore,\n",
    "adjusting the number of clusters is not enough to get good results in this\n",
    "kind of data.\n",
    "\n",
    "Let's now repeat the experiment using HDBSCAN instead. For this clustering\n",
    "technique, the most important hyperparameter is `min_cluster_size`, which\n",
    "controls the minimum number of samples for a group to be considered a cluster;\n",
    "groupings smaller than this size will be left as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "cluster_labels = HDBSCAN(min_cluster_size=10).fit_predict(X_all)\n",
    "_ = plt.scatter(X_all[:, 0], X_all[:, 1], c=cluster_labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply HDBSCAN to a more realistic use-case: the geospatial columns\n",
    "of the California Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use plotly to first visualize the housing prices across the state of\n",
    "California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def plot_map(df, color_feature):\n",
    "    fig = px.scatter_mapbox(\n",
    "        df,\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        color=color_feature,\n",
    "        zoom=5,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        mapbox_center={\n",
    "            \"lat\": df[\"Latitude\"].mean(),\n",
    "            \"lon\": df[\"Longitude\"].mean(),\n",
    "        },\n",
    "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_map(data, target)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first use K-means to group data points into different spatial regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "geo_columns = [\"Latitude\", \"Longitude\"]\n",
    "geo_data = data[geo_columns]\n",
    "\n",
    "kmeans_pipeline = make_pipeline(\n",
    "    StandardScaler(), KMeans(n_clusters=20, random_state=0)\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans_pipeline.fit_predict(geo_data)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_map(data, cluster_labels.astype(\"str\"))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that results are really influenced by the K-means that favors\n",
    "\"blobby\"-shaped clusters. Let's try again with HDBSCAN which should not suffer\n",
    "from the same bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan_pipeline = make_pipeline(HDBSCAN(min_cluster_size=100))\n",
    "\n",
    "cluster_labels = hdbscan_pipeline.fit_predict(geo_data)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_map(data, cluster_labels.astype(\"str\"))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDBSCAN automatically detect highly populated areas that match urban centers,\n",
    "potentially increasing the housing prices. In addition we observe that points\n",
    "lying in low density regions are labeled `-1` instead of being forced into a\n",
    "cluster.\n",
    "\n",
    "The number of resulting clusters is a consequence of the choice of\n",
    "`min_cluster_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(cluster_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing `min_cluster_size` will increase the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_pipeline = make_pipeline(HDBSCAN(min_cluster_size=30))\n",
    "cluster_labels = hdbscan_pipeline.fit_predict(geo_data)\n",
    "fig = plot_map(data, cluster_labels.astype(\"str\"))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(cluster_labels))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}