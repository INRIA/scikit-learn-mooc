{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-convex clustering using HDBSCAN\n",
    "\n",
    "We have previously mentioned that k-means consists of minimizing the samples\n",
    "euclidean distances to their assigned centroid. As a consequence, k-means is\n",
    "more appropriate for clusters that are isotropic and normally distributed\n",
    "(look like spherical blobs). When this assumption is not met, k-means can\n",
    "lead to unstable clustering results that do not qualitatively match the\n",
    "cluster we seek. On possible way is to use a more general variant of k-means\n",
    "named Gaussian Mixture Models (GMM), which allows for elongated clusters with\n",
    "strong correlation between features as explained in [this tutorial of the\n",
    "scikit-learn\n",
    "documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html).\n",
    "However, GMM still assumes that clusters are convex, which is not always the\n",
    "case in practice.\n",
    "\n",
    "In this notebook we introduce another clustering technique named HDBSCAN, an\n",
    "acronym which stands for \"Hierarchical Density-Based Spatial Clustering of\n",
    "Applications with Noise\" which further allows for non-convex clusters.\n",
    "\n",
    "Let's explain each of those terms. HDBSCAN is hierarchical, which means it\n",
    "handles data with clusters nested within each other. The user controls the\n",
    "level in the hierarchy at which clusters are formed.\n",
    "\n",
    "It is non-parametric, density-based method that does not assume a specific\n",
    "shape or number of clusters. Instead, it automatically finds the clusters\n",
    "based on areas where data points are densely packed together. In other words,\n",
    "it looks for regions of high density (many data points close to each other)\n",
    "and forms clusters around them. This allows it to find clusters of varying\n",
    "shapes and sizes.\n",
    "\n",
    "HDBSCAN assigns a label of -1 to points that do not have enough neighbors\n",
    "(low density) to be considered part of a cluster or are too far from any\n",
    "dense region (too isolated from core points). They are usually considered to\n",
    "be noise.\n",
    "\n",
    "Let's first illustrate those concepts with a toy dataset generated using the\n",
    "code below. You do not need to understand the details of the data generation\n",
    "process, and instead pay attention to the resulting scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "centers = np.array([[-4.8, 2.0], [-3.5, -4.5]])\n",
    "X_gaussian, _ = make_blobs(\n",
    "    n_samples=[200, 60],\n",
    "    centers=centers,\n",
    "    cluster_std=[1.0, 0.5],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Two anisotropic blobs\n",
    "centers = np.array([[1.0, 5.1], [3.0, 0.9]])\n",
    "X_aniso_base, y_aniso_base = make_blobs(\n",
    "    n_samples=200, centers=centers, random_state=0\n",
    ")\n",
    "\n",
    "# Define two different transformations\n",
    "transformation_0 = np.array([[0.6, -0.6], [-0.4, 0.8]])\n",
    "transformation_1 = np.array([[1.5, 0], [0, 0.3]])\n",
    "\n",
    "# Apply different transformations to each blob\n",
    "X_aniso = np.copy(X_aniso_base)\n",
    "X_aniso[y_aniso_base == 0] = np.dot(\n",
    "    X_aniso_base[y_aniso_base == 0], transformation_0\n",
    ")\n",
    "X_aniso[y_aniso_base == 1] = np.dot(\n",
    "    X_aniso_base[y_aniso_base == 1], transformation_1\n",
    ")\n",
    "\n",
    "\n",
    "def make_wavy_blob(n_samples, shift=0.0, noise=0.2, freq=3):\n",
    "    \"Make wavy blobs in feature space\"\n",
    "    x = np.linspace(-3, 3, n_samples)\n",
    "    y = np.sin(freq * x) + shift\n",
    "    x += rng.normal(scale=noise, size=n_samples)\n",
    "    y += rng.normal(scale=noise, size=n_samples)\n",
    "    return np.vstack((x, y)).T\n",
    "\n",
    "\n",
    "X_wave1 = make_wavy_blob(100, shift=4.7, freq=1)\n",
    "transformation = np.array([[0.6, -0.6], [0.4, 0.8]])\n",
    "X_wave1 = np.dot(X_wave1, transformation)\n",
    "X_wave2 = make_wavy_blob(200, shift=-2.0, freq=2)\n",
    "\n",
    "\n",
    "X_noise = rng.uniform(low=-8, high=8, size=(100, 2))  # background noise\n",
    "\n",
    "X_all = np.vstack((X_gaussian, X_aniso, X_wave1, X_wave2, X_noise))\n",
    "\n",
    "plt.scatter(X_all[:, 0], X_all[:, 1], alpha=0.6)\n",
    "_ = plt.title(\"Synthetic dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can observe that the dataset contains:\n",
    "- four Gaussian blobs with different sizes and densities, some of which\n",
    "  are elongated and other more spherical;\n",
    "- two non-convex clusters with wavy shapes;\n",
    "- a background noise of points uniformly distributed in the feature space.\n",
    "\n",
    "Let's first try to find a cluster structure using K-means with 6 clusters to\n",
    "match our data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_labels = KMeans(n_clusters=6, random_state=0).fit_predict(X_all)\n",
    "_ = plt.scatter(X_all[:, 0], X_all[:, 1], c=cluster_labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try to increase the number of clusters to avoid grouping\n",
    "unrelated points in the same cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = KMeans(n_clusters=10, random_state=0).fit_predict(X_all)\n",
    "_ = plt.scatter(X_all[:, 0], X_all[:, 1], c=cluster_labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can observe this cluster assignment divides the high density regions\n",
    "while also grouping unrelated points together. Furthermore, the background\n",
    "noise data points are always assigned to the nearest centroids and thus\n",
    "treated as cluster members. Therefore, adjusting the number of clusters is\n",
    "not enough to get good results in this kind of data.\n",
    "\n",
    "We can compute the silhouette score for this number of clusters and keep it\n",
    "in mind for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "kmeans_score = silhouette_score(X_all, cluster_labels)\n",
    "print(f\"Silhouette score for k-means clusters: {kmeans_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now repeat the experiment using HDBSCAN instead. For this clustering\n",
    "technique, the most important hyperparameter is `min_cluster_size`, which\n",
    "controls the minimum number of samples for a group to be considered a cluster;\n",
    "groupings smaller than this size are considered as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "cluster_labels = HDBSCAN(min_cluster_size=10).fit_predict(X_all)\n",
    "_ = plt.scatter(X_all[:, 0], X_all[:, 1], c=cluster_labels, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters found using HDBSCAN better match our intuition of how data points\n",
    "should be grouped. We can compute the corresponding silhouette score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_score = silhouette_score(X_all, cluster_labels)\n",
    "print(f\"Silhouette score for HDBSCAN clusters: {hdbscan_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this score is lower than the score using k-means, even if HDBSCAN\n",
    "seems to do a better job when grouping the data points. The reason here is\n",
    "that points considered as noise (labeled with `-1` by HDBSCAN) do not follow a\n",
    "cluster-like structure. We can test that hypothesis as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cluster_labels != -1  # mask is TRUE for entries that are NOT -1\n",
    "cluster_labels_filtered = cluster_labels[mask]\n",
    "X_all_filtered = X_all[mask]\n",
    "\n",
    "hdbscan_score = silhouette_score(X_all_filtered, cluster_labels_filtered)\n",
    "print(\n",
    "    f\"Silhouette score for HDBSCAN clusters without noise: {hdbscan_score:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we do obtain a better silhouette score, but in general we **do\n",
    "not** suggest dropping samples labeled as noise.\n",
    "\n",
    "Also, keep in mind that HDBSCAN does not optimize intra- or inter-cluster\n",
    "distances, which are the basis of the silhouette score. It is then more\n",
    "appropriate to use the silhouette score when clusters are compact and roughly\n",
    "convex. Otherwise, if the clusters are elongated, wavy, or even wrap around\n",
    "other clusters, comparing average distances becomes less meaningful.\n",
    "\n",
    "## Clustering of geospatial data\n",
    "\n",
    "Let's now apply HDBSCAN to a more realistic use-case: the geospatial columns\n",
    "of the California Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use plotly to first visualize the housing prices across the state of\n",
    "California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def plot_map(df, color_feature, colorbar_label=\"cluster label\"):\n",
    "    fig = px.scatter_map(\n",
    "        df,\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        color=color_feature,\n",
    "        zoom=5,\n",
    "        height=600,\n",
    "        labels={\"color\": colorbar_label},\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        mapbox_center={\n",
    "            \"lat\": df[\"Latitude\"].mean(),\n",
    "            \"lon\": df[\"Longitude\"].mean(),\n",
    "        },\n",
    "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "    )\n",
    "    return fig.show(renderer=\"notebook\")\n",
    "\n",
    "\n",
    "fig = plot_map(data, target, colorbar_label=\"price (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can try to use K-means to group data points into different spatial regions\n",
    "(irrespective of the housing prices) and visualize the results on a map.\n",
    "\n",
    "Note that the Geospatial columns are `Latitude` and `Longitude` are already\n",
    "on the same scale so there is no need to standardize them before clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "geo_columns = [\"Latitude\", \"Longitude\"]\n",
    "geo_data = data[geo_columns]\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=0)\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(geo_data)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_map(data, cluster_labels.astype(\"str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that results are really influenced by the fact that K-means favors\n",
    "spherical-shaped clusters. Let's try again with HDBSCAN which should not suffer\n",
    "from the same bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=100)\n",
    "\n",
    "cluster_labels = hdbscan.fit_predict(geo_data)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_map(data, cluster_labels.astype(\"str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDBSCAN automatically detects highly populated areas that match urban centers,\n",
    "potentially increasing the housing prices. In addition we observe that points\n",
    "lying in low density regions are labeled `-1` instead of being forced into a\n",
    "cluster.\n",
    "\n",
    "The number of resulting clusters is a consequence of the choice of\n",
    "`min_cluster_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of clusters: {len(np.unique(cluster_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing `min_cluster_size` increases the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(min_cluster_size=30)\n",
    "cluster_labels = hdbscan.fit_predict(geo_data)\n",
    "fig = plot_map(data, cluster_labels.astype(\"str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of clusters: {len(np.unique(cluster_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously mentioned that the user can control the level in the hierarchy\n",
    "at which clusters are formed. This can be done without retraining the model by\n",
    "using the `dbscan_clustering` method, and is an indirect way to control the\n",
    "number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cut_distance in [0.1, 0.3, 0.5]:\n",
    "    cluster_labels = hdbscan.dbscan_clustering(\n",
    "        cut_distance=cut_distance, min_cluster_size=30\n",
    "    )\n",
    "    plot_map(data, cluster_labels.astype(\"str\"))\n",
    "    print(f\"Number of clusters: {len(np.unique(cluster_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "In this notebook we have introduced HDBSCAN, a clustering technique that\n",
    "allows for non-convex clusters and does not require the user to specify the\n",
    "number of clusters.\n",
    "\n",
    "Keep in mind however, that despite its flexibility, even HDBSCAN can still\n",
    "fail to find relevant clusters in some datasets: sometimes there is no\n",
    "meaningful cluster structure in the data."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}