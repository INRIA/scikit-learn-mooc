{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering with scikit-learn\n",
    "\n",
    "So far we have only addressed supervised learning models, namely regression\n",
    "and classification. In this module we introduce unsupervised learning for the\n",
    "first time.\n",
    "\n",
    "In this notebook we explore the k-means algorithm, which seeks to group data\n",
    "by a certain notion of similarity. To illustrate the different concepts, we\n",
    "use the Mall Customers dataset.\n",
    "\n",
    "Here we use clustering to group customers with similar profiles based on some\n",
    "characteristics, which then can be used for customer segmentation, and\n",
    "therefore, for designing better targeted campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/mall_customers.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this dataset includes the following information:\n",
    "\n",
    "- Gender: The gender of the customer.\n",
    "- Age: The age of the customer.\n",
    "- Annual Income (k$): The annual income of the customer (in thousands of\n",
    "  dollars).\n",
    "- Spending Score (1\u2013100): The score ranges from 1 to 100, with a higher score\n",
    "  indicating a customer who spends more.\n",
    "\n",
    "In this case we cannot assign any of those columns to be the target. These are\n",
    "all features, each of them representing different aspects of a customer\n",
    "profile. We can verify that such features do not have a direct, predictable\n",
    "relationship with each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.pairplot(data, hue=\"Genre\", height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could feel inclined to assigning labels to translate the task into a\n",
    "classification problem, instead of using clustering.\n",
    "\n",
    "One approach could be simple labeling: Low spenders, mid spenders, and high\n",
    "spenders (3 labels). But a priori nothing prevents us from defining multiple\n",
    "combinations, such as:\n",
    "- Young - Low spender - High income\n",
    "- Young adult - High spender - Mid income\n",
    "- Older adult - Mid spender - Low income\n",
    "\n",
    "We divided each numerical feature into 3 bins, leading to `3 ** n_features`\n",
    "possible combinations. But we could also have used different amounts of bins\n",
    "to define those labels and the problem rapidly becomes complex and subjective.\n",
    "The choice of how to bin or categorize customers features can introduce\n",
    "arbitrary boundaries, and the labels may not capture the nuances of customer\n",
    "behavior effectively or can lead to oversimplification. For some settings we\n",
    "rather let cluster labels emerge from the analysis, not from prior knowledge.\n",
    "\n",
    "Let's keep only the numerical values for the rest of this notebook. Having 3\n",
    "features is something we can still easily visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"Genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a k-means algorithm\n",
    "\n",
    "Intuitively, a good cluster should be compact (with points close to each\n",
    "other), dense (with a high concentration of data points), and well-separated\n",
    "from other clusters. In client segmentation, this means that different\n",
    "clusters should clearly represent well-defined differences in their profiles.\n",
    "\n",
    "First let's define a helper function to gain a visual intuition of the\n",
    "clusters as obtained provided a `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def plot_clusters(model, ax):\n",
    "\n",
    "    cluster_labels = model.fit_predict(data)\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "    ax.scatter(\n",
    "        data[\"Annual Income (k$)\"],\n",
    "        data[\"Spending Score (1-100)\"],\n",
    "        data[\"Age\"],\n",
    "        c=cluster_labels,\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.set_box_aspect(None, zoom=0.84)\n",
    "    ax.set_xlabel(\"Annual Income (k$)\", labelpad=15)\n",
    "    ax.set_ylabel(\"Spending Score (1-100)\", labelpad=15)\n",
    "    ax.set_zlabel(\"Age\", labelpad=15)\n",
    "    ax.set_title(f\"n_clusters={n_clusters}\", y=0.99)\n",
    "    _ = plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p class=\"last\">Here we used the <tt class=\"docutils literal\">fit_predict</tt> method, which does both steps at once: it\n",
    "learns from the data just as using <tt class=\"docutils literal\">fit</tt>, and immediately outputs labels (or\n",
    "cluster labels) to those same data points as would be the case using <tt class=\"docutils literal\">predict</tt>.</p>\n",
    "</div>\n",
    "\n",
    "In the plots below we use different numbers of clusters, by changing the\n",
    "hyperparameter `n_clusters`. Here the `random_state` controls the centroid\n",
    "initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_values = [2, 4, 6, 8]\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=2, figsize=(17, 15), subplot_kw={\"projection\": \"3d\"}\n",
    ")\n",
    "\n",
    "for ax, n_clusters in zip(axes.flatten(), n_clusters_values):\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    plot_clusters(model, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In non-supervised learning, such as clustering, not having ground truth labels\n",
    "can make the model evaluation challenging. However, as we have discussed,\n",
    "it is still possible to define metrics that provide insight into the quality of\n",
    "the formed clusters.\n",
    "\n",
    "One common metric for evaluating clusters is Within-Cluster Sum of Squares\n",
    "(WCSS), also known as **inertia**, which measures how compact the clusters\n",
    "are. A lower WCSS indicates that the data points within each cluster are close\n",
    "to the cluster's centroid, suggesting that the cluster is well-formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "n_clusters_values = range(2, 11)\n",
    "\n",
    "for n_clusters in n_clusters_values:\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    model.fit(data)\n",
    "    wcss.append(model.inertia_)\n",
    "\n",
    "plt.plot(n_clusters_values, wcss, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "_ = plt.title(\"Elbow method using cluster inertia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so called elbow method can be subtile here, but it seems to match our\n",
    "visual intuition from the 3D plots: having 6 clusters seems to be the best\n",
    "choice for correctly identifying groups.\n",
    "\n",
    "Another useful metric is the Silhouette Score. A high silhouette score means\n",
    "that the data points are not only well-grouped within their own clusters but\n",
    "also well-separated from other clusters. A value of 0 indicates that the the\n",
    "decision boundary between two neighboring clusters may overlap, whereas\n",
    "negative values indicate that some samples might have been assigned to the\n",
    "wrong cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "for n_clusters in n_clusters_values:\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = model.fit_predict(data)\n",
    "    score = silhouette_score(data, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.plot(n_clusters_values, silhouette_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "_ = plt.title(\"Silhouette scores for different n_clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score reaches a maximum when `n_clusters=6`, which confirms\n",
    "both the visual intuition and the optimal number of clusters found using the\n",
    "elbow method."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}