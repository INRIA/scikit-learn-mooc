{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcdd Exercise M4.01\n",
    "\n",
    "In this exercise we investigate the stability of the k-means algorithm. For\n",
    "such purpose, we use the RFM Dataset. RFM is a method used for analyzing\n",
    "customer value and the acronym RFM stands for the three dimensions:\n",
    "\n",
    "- Recency: How recently did the customer purchase;\n",
    "- Frequency: How often do they purchase;\n",
    "- Monetary Value: How much do they spend.\n",
    "\n",
    "It is commonly used in marketing and has received particular attention in\n",
    "retail and professional services industries as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/rfm_segmentation.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As k-means clustering relies on computing distances between samples, in\n",
    "general we need to scale our data before training the clustering model. That\n",
    "was not the case in our previous notebook, as the features in the Mall\n",
    "customers dataset already have the same scale.\n",
    "\n",
    "Show that scaling is important or else \"monetary\" have a dominant impact when\n",
    "forming clusters. You can adapt the helper function `plot_clusters` from the\n",
    "previous notebook to make plots using `KMeans` for `n_clusters_values = [2, 4,\n",
    "6, 8]` without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline composed by a `StandardScaler` followed by a `KMeans` step\n",
    "# as the final predictor. Set the `random_state` for reproducibility. Then, make\n",
    "# a plot of the WCSS or inertia for `n_clusters` varying from 2 to 10. You can\n",
    "# use the following helper function for such purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def plot_n_clusters_scores(\n",
    "    model,\n",
    "    data,\n",
    "    score_type=\"inertia\",\n",
    "    n_clusters_values=range(2, 11),\n",
    "    alpha=1.0,\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots clustering scores (inertia or silhouette) for a range of n_clusters.\n",
    "\n",
    "    Parameters:\n",
    "        model: A pipeline whose last step has a `n_clusters` hyperparameter.\n",
    "        data: The input data to cluster.\n",
    "        score_type: \"inertia\" or \"silhouette\" to decide which score to compute.\n",
    "        n_clusters_values: Iterable of integers representing `n_clusters` to try.\n",
    "        alpha: Transparency of the plot line, useful when several plots overlap.\n",
    "        title: Optional title to set; default title used if None.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for n_clusters in n_clusters_values:\n",
    "        model[-1].set_params(n_clusters=n_clusters)\n",
    "\n",
    "        if score_type == \"inertia\":\n",
    "            ylabel = \"Inertia\"\n",
    "            model.fit(data)\n",
    "            scores.append(model[-1].inertia_)\n",
    "        elif score_type == \"silhouette\":\n",
    "            ylabel = \"Silhouette score\"\n",
    "            cluster_labels = model.fit_predict(data)\n",
    "            data_transformed = model[:-1].transform(data)\n",
    "            score = silhouette_score(data_transformed, cluster_labels)\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"score_type must be either 'inertia' or 'silhouette'\"\n",
    "            )\n",
    "\n",
    "    plt.plot(n_clusters_values, scores, color=\"tab:blue\", alpha=alpha)\n",
    "    plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    _ = plt.title(title or f\"{ylabel} for varying n_clusters\", y=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the best choice of n_clusters remains stable when resampling\n",
    "the dataset. For such purpose:\n",
    "- Keep a fixed `random_state` for the `KMeans` step to isolate the\n",
    "  effect of data resampling.\n",
    "- Generate resamplings consisting of 90% of the data by using\n",
    "  `train_test_split` with `train_size=0.9`\n",
    "- Use the `plot_n_clusters_scores` function inside a `for` loop to make\n",
    "  multiple overlapping plots of the inertia, each time using a different\n",
    "  resamplings.\n",
    "\n",
    "Is the elbow (optimal number of clusters) stable across all different\n",
    "resamplings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `KMeans` uses a smart selection of the initial centroids called\n",
    "\"k-means++\". Instead of picking points completly at random, it tries several\n",
    "candidate centroids at each step and picks the best ones based on an\n",
    "estimation of how much they would help reduce the overall inertia. This method\n",
    "improves the chances of finding better cluster centroids and speeds up\n",
    "convergence compared to random initialization.\n",
    "\n",
    "Because \"k-means++\" already does a good job of finding suitable centroids, a\n",
    "single initialization is typically sufficient for most cases. That is why the\n",
    "parameter `n_init` in scikit-learn (which controls the number of times the\n",
    "algorithm is run with different centroid initializations) is set to 1 by\n",
    "default when `init=\"k-means++\"`. Nevertheless, there may be cases (as when\n",
    "data is unevenly distributed) where increasing `n_init` may help ensuring a\n",
    "global minimal inertia.\n",
    "\n",
    "Repeat the previous example but setting `n_init=5`. Remeber to fix the\n",
    "`random_state` for the `KMeans` initialization to only estimate the\n",
    "variability related to resamplings of the data. Are the resulting inertia\n",
    "curves more stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment, but this time determine if the optimal number of\n",
    "clusters is stable across subsamplings when using the `silhouette_score`. Be\n",
    "aware that computing the silhouette score is more computationally costly than\n",
    "computing the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again repeat the experiment to determine the stability of the optimal\n",
    "number of clusters. This time, instead of using a `StandardScaler`, use a\n",
    "`QuantileTransformer` with default parameters as the preprocessing step in the\n",
    "pipeline. For the `KMeans` step, keep `n_init=5` and a fixed `random_state`.\n",
    "What happens in terms of silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "model = make_pipeline(QuantileTransformer(), KMeans(n_init=5, random_state=0))\n",
    "for random_state in range(1, 11):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data, train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    plot_n_clusters_scores(\n",
    "        model,\n",
    "        data_subsample,\n",
    "        score_type=\"silhouette\",\n",
    "        alpha=0.2,\n",
    "        title=\"Stability of silhouette score\\nwith n_init=5 and QuantileTransformer\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}