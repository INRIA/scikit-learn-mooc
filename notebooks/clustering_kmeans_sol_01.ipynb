{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcc3 Solution for Exercise M4.01\n",
    "\n",
    "In this exercise we investigate the stability of the k-means algorithm. For\n",
    "such purpose, we use the RFM Dataset. RFM is a method used for analyzing\n",
    "customer value and the acronym RFM stands for the three dimensions:\n",
    "\n",
    "- Recency: How recently did the customer purchase;\n",
    "- Frequency: How often do they purchase;\n",
    "- Monetary Value: How much do they spend.\n",
    "\n",
    "It is commonly used in marketing and has received particular attention in\n",
    "retail and professional services industries as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/rfm_segmentation.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As k-means clustering relies on computing distances between samples, in\n",
    "general we need to scale our data before training the clustering model. That\n",
    "was not the case in our previous notebook, as the features in the Mall\n",
    "customers dataset already have the same scale.\n",
    "\n",
    "Show that scaling is important or else \"monetary\" have a dominant impact when\n",
    "forming clusters. You can adapt the helper function `plot_clusters` from the\n",
    "previous notebook to make plots using `KMeans` for `n_clusters_values = [2, 4,\n",
    "6, 8]` without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def plot_clusters(model, ax):\n",
    "\n",
    "    cluster_labels = model.fit_predict(data)\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "\n",
    "    ax.scatter(\n",
    "        data[\"monetary\"],\n",
    "        data[\"frequency\"],\n",
    "        data[\"recency\"],\n",
    "        c=cluster_labels,\n",
    "        s=50,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.set_box_aspect(None, zoom=0.84)\n",
    "    ax.set_xlabel(\"Monetary\", labelpad=15)\n",
    "    ax.set_ylabel(\"Frequency\", labelpad=15)\n",
    "    ax.set_zlabel(\"Recency\", labelpad=15)\n",
    "    ax.set_title(f\"n_clusters={n_clusters}\", y=0.99)\n",
    "    _ = plt.tight_layout()\n",
    "\n",
    "\n",
    "n_clusters_values = [2, 4, 6, 8]\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=2, figsize=(17, 15), subplot_kw={\"projection\": \"3d\"}\n",
    ")\n",
    "\n",
    "for ax, n_clusters in zip(axes.flatten(), n_clusters_values):\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    plot_clusters(model, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline composed by a `StandardScaler` followed by a `KMeans` step\n",
    "# as the final predictor. Set the `random_state` for reproducibility. Then, make\n",
    "# a plot of the WCSS or inertia for `n_clusters` varying from 2 to 10. You can\n",
    "# use the following helper function for such purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def plot_n_clusters_scores(\n",
    "    model,\n",
    "    data,\n",
    "    score_type=\"inertia\",\n",
    "    n_clusters_values=range(2, 11),\n",
    "    alpha=1.0,\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots clustering scores (inertia or silhouette) for a range of n_clusters.\n",
    "\n",
    "    Parameters:\n",
    "        model: A pipeline whose last step has a `n_clusters` hyperparameter.\n",
    "        data: The input data to cluster.\n",
    "        score_type: \"inertia\" or \"silhouette\" to decide which score to compute.\n",
    "        n_clusters_values: Iterable of integers representing `n_clusters` to try.\n",
    "        alpha: Transparency of the plot line, useful when several plots overlap.\n",
    "        title: Optional title to set; default title used if None.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for n_clusters in n_clusters_values:\n",
    "        model[-1].set_params(n_clusters=n_clusters)\n",
    "\n",
    "        if score_type == \"inertia\":\n",
    "            ylabel = \"Inertia\"\n",
    "            model.fit(data)\n",
    "            scores.append(model[-1].inertia_)\n",
    "        elif score_type == \"silhouette\":\n",
    "            ylabel = \"Silhouette score\"\n",
    "            cluster_labels = model.fit_predict(data)\n",
    "            data_transformed = model[:-1].transform(data)\n",
    "            score = silhouette_score(data_transformed, cluster_labels)\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"score_type must be either 'inertia' or 'silhouette'\"\n",
    "            )\n",
    "\n",
    "    plt.plot(n_clusters_values, scores, color=\"tab:blue\", alpha=alpha)\n",
    "    plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    _ = plt.title(title or f\"{ylabel} for varying n_clusters\", y=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = make_pipeline(StandardScaler(), KMeans(random_state=0))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "plot_n_clusters_scores(model, data, score_type=\"inertia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the best choice of n_clusters remains stable when resampling\n",
    "the dataset. For such purpose:\n",
    "- Keep a fixed `random_state` for the `KMeans` step to isolate the\n",
    "  effect of data resampling.\n",
    "- Generate resamplings consisting of 90% of the data by using\n",
    "  `train_test_split` with `train_size=0.9`\n",
    "- Use the `plot_n_clusters_scores` function inside a `for` loop to make\n",
    "  multiple overlapping plots of the inertia, each time using a different\n",
    "  resamplings.\n",
    "\n",
    "Is the elbow (optimal number of clusters) stable across all different\n",
    "resamplings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for random_state in range(1, 11):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data, train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    plot_n_clusters_scores(\n",
    "        model,\n",
    "        data_subsample,\n",
    "        score_type=\"inertia\",\n",
    "        alpha=0.2,\n",
    "        title=\"Stability of inertia across resamplings\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The inertia changes drastically as a function of the subsamples, it is\n",
    "then not possible to systematically define an optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `KMeans` uses a smart selection of the initial centroids called\n",
    "\"k-means++\". Instead of picking points completly at random, it tries several\n",
    "candidate centroids at each step and picks the best ones based on an\n",
    "estimation of how much they would help reduce the overall inertia. This method\n",
    "improves the chances of finding better cluster centroids and speeds up\n",
    "convergence compared to random initialization.\n",
    "\n",
    "Because \"k-means++\" already does a good job of finding suitable centroids, a\n",
    "single initialization is typically sufficient for most cases. That is why the\n",
    "parameter `n_init` in scikit-learn (which controls the number of times the\n",
    "algorithm is run with different centroid initializations) is set to 1 by\n",
    "default when `init=\"k-means++\"`. Nevertheless, there may be cases (as when\n",
    "data is unevenly distributed) where increasing `n_init` may help ensuring a\n",
    "global minimal inertia.\n",
    "\n",
    "Repeat the previous example but setting `n_init=5`. Remeber to fix the\n",
    "`random_state` for the `KMeans` initialization to only estimate the\n",
    "variability related to resamplings of the data. Are the resulting inertia\n",
    "curves more stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "model = make_pipeline(StandardScaler(), KMeans(n_init=5, random_state=0))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "for random_state in range(1, 11):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data, train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    plot_n_clusters_scores(\n",
    "        model,\n",
    "        data_subsample,\n",
    "        score_type=\"inertia\",\n",
    "        alpha=0.2,\n",
    "        title=\"Stability of inertia with n_init=5\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The inertia is now stable, but an elbow is not clearly defined and then it is\n",
    "not possible to define an optimal number of clusters from this heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment, but this time determine if the optimal number of\n",
    "clusters is stable across subsamplings when using the `silhouette_score`. Be\n",
    "aware that computing the silhouette score is more computationally costly than\n",
    "computing the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "for random_state in range(1, 11):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data, train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    plot_n_clusters_scores(\n",
    "        model,\n",
    "        data_subsample,\n",
    "        score_type=\"silhouette\",\n",
    "        alpha=0.2,\n",
    "        title=\"Stability of silhouette score with n_init=5\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The silhouette score also varies as a function of the resampling, even after\n",
    "setting `n_init=5`. It may seem that the optimal number of clusters is\n",
    "sometimes 5, 6, or 9, depending on the sampling. We can conclude that in this\n",
    "case, what makes challenging to find the optimal number of clusters is not\n",
    "related to the metric, but possibly to the data itself, or to the modeling\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again repeat the experiment to determine the stability of the optimal\n",
    "number of clusters. This time, instead of using a `StandardScaler`, use a\n",
    "`QuantileTransformer` with default parameters as the preprocessing step in the\n",
    "pipeline. For the `KMeans` step, keep `n_init=5` and a fixed `random_state`.\n",
    "What happens in terms of silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "model = make_pipeline(QuantileTransformer(), KMeans(n_init=5, random_state=0))\n",
    "for random_state in range(1, 11):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data, train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    plot_n_clusters_scores(\n",
    "        model,\n",
    "        data_subsample,\n",
    "        score_type=\"silhouette\",\n",
    "        alpha=0.2,\n",
    "        title=\"Stability of silhouette score\\nwith n_init=5 and QuantileTransformer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The silhouette score is much more stable across resamplings. Moreover, the\n",
    "optimal number of clusters seems to be 2, as it provides the highest score,\n",
    "indicating that the data points are well-separated and correctly grouped with\n",
    "good cohesion. However, 4 or 6 clusters may still make sense if the clustering\n",
    "has specific use cases or domain relevance, but you should be cautious as the\n",
    "relatively low silhouette score suggests that some points may be misassigned.\n",
    "\n",
    "Indeed, we can plot the transformed space and observe some samples seem to be\n",
    "misassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    QuantileTransformer(), KMeans(n_init=5, n_clusters=6, random_state=0)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster_labels = model.fit_predict(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={\"projection\": \"3d\"})\n",
    "scatter = ax.scatter(\n",
    "    *model[:-1].transform(data).T,\n",
    "    c=cluster_labels,\n",
    "    cmap=\"viridis\",\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.set_box_aspect(None, zoom=0.84)\n",
    "ax.set_xlabel(\"Transformed Monetary\", labelpad=15)\n",
    "ax.set_ylabel(\"Transformed Frequency\", labelpad=15)\n",
    "ax.set_zlabel(\"Transformed Recency\", labelpad=15)\n",
    "ax.set_title(\"Clusters in quantile-transformed space\", y=0.99)\n",
    "_ = plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "From the plot above, it would feel more natural to cluster together all\n",
    "samples with transformed monetary equal to 0 instead of dividing it into 2\n",
    "different clusters. Moreover, those clusters seem to take samples with low but\n",
    "non-zero values of monetary, that would belong more naturally to other\n",
    "clusters.\n",
    "\n",
    "What happens here is that data points at transformed monetary equal to 0 are\n",
    "not isotropic in the 3 dimensions, i.e. they are spread on a plane of\n",
    "transformed recency and frequency ranging from 0 to 1. Remember that k-means\n",
    "consists of minimizing samples' euclidean distances to their assigned\n",
    "centroid. As a consequence, k-means is more appropriate for clusters that are\n",
    "isotropic and normally distributed.\n",
    "\n",
    "We will learn more about how to deal with anisotropic clusters in a future\n",
    "notebook."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}