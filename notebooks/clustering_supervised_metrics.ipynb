{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering performance metrics in the presence of labels\n",
    "\n",
    "In this notebook we briefly introduce how to deal with text data. Then we\n",
    "introduce the use of supervised metrics to evaluate a clustering model.\n",
    "\n",
    "Previously, we saw how categorical features can be converted into numbers\n",
    "using techniques like one-hot encoding, where each category is assigned a\n",
    "unique position in a vector. We can apply a similar idea to text: treat each\n",
    "unique word as a feature (a column), and represent each document as a vector\n",
    "(a row) indicating the word counts in it. This encoding process is known as\n",
    "\"vectorization\". Here is a quick example using `CountVectorizer` to turn a few\n",
    "short phrases into a numerical table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"This is a simple phrase\",\n",
    "    \"This phrase is shorter\",\n",
    "    \"The previous phrase is shorter than the first phrase\",\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe in particular that the words \"the\" and \"phrase\" are counted twice in\n",
    "the last document (the third row in the dataframe). Readers interested can\n",
    "visit the [scikit-learn example on comparing vectorization\n",
    "strategies](https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html).\n",
    "\n",
    "Now let's use the BBC News dataset to show how text documents can be cluster\n",
    "by topic. The dataset consists of 1,250 samples divided into 5 different\n",
    "categories: \"business\", \"entertainment\", \"sport\", \"politics\" and \"tech\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/bbc_news.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by trying a model consisting of `StringEncoder`, which vectorizes the\n",
    "text while keeping the feature space reasonably small, followed by `KMeans`.\n",
    "\n",
    "In a previous notebook we increased the default value of `n_init` as in that\n",
    "case data turned out to be unevenly distributed. In this case we also set\n",
    "`n_init=5` but for a different reason. When working with high-dimensional data\n",
    "(i.e. data with a large number of features) such as vectorized text, k-means\n",
    "can initialize centroids on extremely isolated data points that can stay their\n",
    "own centroids all along. You can see the [example on clustering sparse\n",
    "data](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#clustering-sparse-data-with-k-means)\n",
    "for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import StringEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StringEncoder(), KMeans(n_init=5, random_state=0))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though clustering is an unsupervised learning method, we have access to\n",
    "labels for the categories that were assigned by the BBC editors, allowing us\n",
    "to evaluate how well the clusters found using our models match these labels.\n",
    "While we could turn this into a multiclass classification problem and use\n",
    "metrics such as the accuracy, this approach has limitations. For example, an\n",
    "article on \"tech for entertainment\" might be assigned to a cluster that\n",
    "doesn't perfectly match its original label, even though it shares similarities\n",
    "with both \"tech\" and \"entertainment\" articles. This is why we focus on metrics\n",
    "that evaluate clustering beyond strict label matching.\n",
    "\n",
    "In this notebook, we'll use two metrics: V-measure and Adjusted Rand Index\n",
    "(ARI). The V-measure addresses overlaps by measuring both homogeneity (how\n",
    "pure the clusters are) and completeness (how well each category is grouped\n",
    "together). For instance, if an article on \"tech and entertainment\" is placed\n",
    "in a cluster mostly about \"entertainment,\" this can still be a reasonable\n",
    "result. V-measure ranges from 0 to 1, where 1 indicates perfect clustering\n",
    "(both pure and complete), and 0 means the clustering is ineffective.\n",
    "\n",
    "The Adjusted Rand Index (ARI) measures the similarity between the predicted\n",
    "clusters and human-assigned labels, adjusting for random labeling. For the BBC\n",
    "News dataset, it compares pairs of articles to see if they are in the same\n",
    "cluster in both the predicted and human-assigned labels. High ARI means that\n",
    "articles from the same category are grouped together, and articles from\n",
    "different categories are separated. ARI ranges from -1 (worse than random\n",
    "clustering) to 1 (perfect clustering), with 0 indicating a model that assigns\n",
    "cluster labels at random. A high ARI value shows good alignment between the\n",
    "predicted clusters and the human-assigned labels, while a low ARI suggests\n",
    "poor clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ValidationCurveDisplay\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, train_size=0.9, random_state=0)\n",
    "\n",
    "n_clusters_values = range(2, 11)\n",
    "scoring_names = {\n",
    "    \"V-measure\": \"v_measure_score\",\n",
    "    \"ARI\": \"adjusted_rand_score\",\n",
    "}\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for scoring in scoring_names.values():\n",
    "    ValidationCurveDisplay.from_estimator(\n",
    "        model,\n",
    "        data[\"text\"],\n",
    "        data[\"category\"],\n",
    "        param_name=\"kmeans__n_clusters\",\n",
    "        param_range=n_clusters_values,\n",
    "        score_type=\"train\",\n",
    "        scoring=scoring,\n",
    "        std_display_style=\"errorbar\",\n",
    "        cv=cv,\n",
    "        n_jobs=4,\n",
    "        ax=ax,\n",
    "    )\n",
    "ax.set(\n",
    "    ylim=(-0.1, 1.1),\n",
    "    xlabel=\"Number of components\",\n",
    "    ylabel=\"Score\",\n",
    "    title=\"Validation curves for\\nStringEncoder + KMeans\",\n",
    ")\n",
    "handles, _ = ax.get_legend_handles_labels()\n",
    "_ = ax.legend(handles=handles, labels=scoring_names.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that both V-measure and ARI reach their maximum when\n",
    "`n_clusters=5`, which matches the number of human-assigned categories in the\n",
    "dataset. This alignment reflects both good editorial labels (as the categories\n",
    "are well-defined and internally consistent) as well as a clustering pipeline\n",
    "that can reasonably extract the structure in the data that matches the human\n",
    "intuition.\n",
    "\n",
    "But the question may arise, if we didn't have access to labels at all, would\n",
    "the silhouette score also lead us to chose `n_clusters=5`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_clusters_values = range(2, 11)\n",
    "for random_state in range(1, 6):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data[\"text\"], train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    scores = []\n",
    "    for n_clusters in n_clusters_values:\n",
    "        model[-1].set_params(n_clusters=n_clusters)\n",
    "        cluster_labels = model.fit_predict(data_subsample)\n",
    "        data_transformed = model[:-1].transform(data_subsample)\n",
    "        score = silhouette_score(data_transformed, cluster_labels)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.plot(n_clusters_values, scores, color=\"tab:blue\", alpha=0.2)\n",
    "    plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "    plt.ylabel(\"Silhouette score\")\n",
    "    _ = plt.title(\"Silhouette score for varying n_clusters\", y=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the silhouette score favors larger values for `n_clusters` than\n",
    "the supervised metrics we saw before suggests that, in terms of cluster\n",
    "tightness and separation, having more clusters leads to better-defined\n",
    "clusters. This could be a case where the data contains subclusters or finer\n",
    "distinctions within the categories that the algorithm can capture with more\n",
    "clusters. Essentially, while there may be 5 high-level categories, the\n",
    "subcategories within those 5 groups (such as articles that are very niche\n",
    "within \"tech\" or \"politics\") could benefit from additional clusters.\n",
    "\n",
    "To reduce the variability across resamplings, we can normalize the results\n",
    "coming from the `StringEncoder`. Here `Normalizer` scales each sample\n",
    "individually to have unit length. This ensures that clustering is driven by\n",
    "the relative angle of the features coming from the vectorizer, rather than the\n",
    "overall size of the vector. If two documents use the same set of dominant\n",
    "words, in similar proportions, their vectors end up pointing in roughly the\n",
    "same direction, resulting in a small relative angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "model = make_pipeline(\n",
    "    StringEncoder(), Normalizer(copy=False), KMeans(n_init=5, random_state=0)\n",
    ")\n",
    "\n",
    "for random_state in range(1, 6):\n",
    "    data_subsample, _ = train_test_split(\n",
    "        data[\"text\"], train_size=0.9, random_state=random_state\n",
    "    )\n",
    "    scores = []\n",
    "    for n_clusters in n_clusters_values:\n",
    "        model[-1].set_params(n_clusters=n_clusters)\n",
    "        cluster_labels = model.fit_predict(data_subsample)\n",
    "        data_transformed = model[:-1].transform(data_subsample)\n",
    "        score = silhouette_score(data_transformed, cluster_labels)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.plot(n_clusters_values, scores, color=\"tab:blue\", alpha=0.2)\n",
    "    plt.xlabel(\"Number of clusters (n_clusters)\")\n",
    "    plt.ylabel(\"Silhouette score\")\n",
    "    _ = plt.title(\"Silhouette score for varying n_clusters\", y=1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work as intended."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}