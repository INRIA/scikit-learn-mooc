{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making clusters part of a supervised pipeline\n",
    "\n",
    "This notebook explores how K-means clustering can be used as a feature\n",
    "engineering step to improve the performance of a regression model.\n",
    "\n",
    "Here we use the California Housing dataset, which includes information about\n",
    "the geographic location (latitude and longitude).\n",
    "\n",
    "Our goal is to predict the median house value (MedHouseVal) using a ridge\n",
    "regression model, to investigate whether adding features derived from applying\n",
    "K-means to geographic coordinates can improve the pipeline's predictive\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first design a predictive pipeline that completly ignores the\n",
    "coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=0\n",
    ")\n",
    "geo_columns = [\"Latitude\", \"Longitude\"]\n",
    "model_drop_geo = make_pipeline(\n",
    "    make_column_transformer((\"drop\", geo_columns), remainder=\"passthrough\"),\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha=1e-12),\n",
    ")\n",
    "test_error_drop_geo = -cross_val_score(\n",
    "    model_drop_geo, data_train, target_train, scoring=\"neg_mean_absolute_error\"\n",
    ")\n",
    "print(\n",
    "    \"The test MAE without geographical features is: \"\n",
    "    f\"{test_error_drop_geo.mean():.2f} \u00b1 {test_error_drop_geo.std():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a Mean Absolute Error of approximately 57k$ when dropping the\n",
    "geographical features.\n",
    "\n",
    "As seen in the previous notebook, we suspect that the price information may be\n",
    "linked to the distance to the nearest urban center, and proximity to the\n",
    "coast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_map(\n",
    "    data,\n",
    "    lat=\"Latitude\",\n",
    "    lon=\"Longitude\",\n",
    "    color=target,\n",
    "    zoom=5,\n",
    "    height=600,\n",
    "    labels={\"color\": \"price (k$)\"},\n",
    ")\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    mapbox_center={\n",
    "        \"lat\": data[\"Latitude\"].mean(),\n",
    "        \"lon\": data[\"Longitude\"].mean(),\n",
    "    },\n",
    "    margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    ")\n",
    "fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first feed the coordinates directly to the linear model without\n",
    "transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive_geo = make_pipeline(StandardScaler(), Ridge(alpha=1e-12))\n",
    "test_error_naive_geo = -cross_val_score(\n",
    "    model_naive_geo,\n",
    "    data_train,\n",
    "    target_train,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    ")\n",
    "print(\n",
    "    \"The test MAE with raw geographical features is: \"\n",
    "    f\"{test_error_naive_geo.mean():.2f} \u00b1 {test_error_naive_geo.std():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the geospatial data naively improves the performance a bit, however,\n",
    "we suspect that we can do better by introducing features that represent\n",
    "proximity to points of interests (urban centers, the coast, parks, etc.).\n",
    "\n",
    "We could look for a dataset containing all the coordinates of the city\n",
    "centers, the coast line and other points of interest in California, then\n",
    "manually engineer such features. However this would require a non-trivial\n",
    "amount of code. Instead we can rely on K-means to achieve something similar\n",
    "implicitly: we use it to find a large number of centroids from the housing\n",
    "data directly and consider each centroid a potential point of interest.\n",
    "\n",
    "The `KMeans` class implements a `transform` method that, given a set of data\n",
    "points as an argument, computes the distance to the nearest centroid for each\n",
    "of them. As a result, `KMeans` can be used as a preprocessing step in a\n",
    "feature engineering pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_cluster_geo = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (KMeans(n_clusters=100), geo_columns),\n",
    "        remainder=\"passthrough\",\n",
    "        force_int_remainder_cols=False,  # Silence a warning in scikit-learn v1.6\n",
    "    ),\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha=1e-12),\n",
    ")\n",
    "test_error_cluster_geo = -cross_val_score(\n",
    "    model_cluster_geo,\n",
    "    data_train,\n",
    "    target_train,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    ")\n",
    "print(\n",
    "    \"The test MAE with clustered geographical features is:\"\n",
    "    f\" {test_error_cluster_geo.mean():.2f} \u00b1\"\n",
    "    f\" {test_error_cluster_geo.std():.2f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting mean test error is much lower. Furthermore, `KMeans` is now part\n",
    "of a supervised pipeline, which means we can use a grid-search to tune\n",
    "`n_clusters` as we have learned in previous modules of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_name = \"columntransformer__kmeans__n_clusters\"\n",
    "param_grid = {param_name: [10, 30, 100, 300, 1_000, 3_000]}\n",
    "grid_search = GridSearchCV(\n",
    "    model_cluster_geo, param_grid=param_grid, scoring=\"neg_mean_absolute_error\"\n",
    ")\n",
    "grid_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_columns = [\n",
    "    \"mean_test_score\",\n",
    "    \"std_test_score\",\n",
    "    \"mean_fit_time\",\n",
    "    \"std_fit_time\",\n",
    "    \"mean_score_time\",\n",
    "    \"std_score_time\",\n",
    "    \"param_\" + param_name,\n",
    "]\n",
    "grid_search_results = pd.DataFrame(grid_search.cv_results_)[results_columns]\n",
    "grid_search_results[\"mean_test_error\"] = -grid_search_results[\n",
    "    \"mean_test_score\"\n",
    "]\n",
    "grid_search_results = (\n",
    "    grid_search_results.drop(columns=[\"mean_test_score\"])\n",
    "    .rename(columns={\"param_\" + param_name: \"n_clusters\"})\n",
    "    .round(3)\n",
    ")\n",
    "grid_search_results.sort_values(\"mean_test_error\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger number of clusters increases the predictive performance at the cost\n",
    "of larger fitting and prediction times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"mean_fit_time\": \"CV fit time (s)\",\n",
    "    \"mean_test_error\": \"CV score (MAE)\",\n",
    "}\n",
    "fig = px.scatter(\n",
    "    grid_search_results,\n",
    "    x=\"mean_fit_time\",\n",
    "    y=\"mean_test_error\",\n",
    "    error_x=\"std_fit_time\",\n",
    "    error_y=\"std_test_score\",\n",
    "    hover_data=grid_search_results.columns,\n",
    "    labels=labels,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"Trade-off between fit time and mean test score\",\n",
    "        \"y\": 0.95,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally evaluate the best model found by our analysis and see how well\n",
    "it can generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_clusters = grid_search.best_params_[param_name]\n",
    "print(\n",
    "    f\"The test MAE with {best_n_clusters} clusters is: \"\n",
    "    f\"{-grid_search.score(data_test, target_test):.3f} k$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates one way to leverage clustering for non-linear\n",
    "feature engineering, but there are many ways to compose unsupervised models in\n",
    "a supervised-learning pipeline.\n",
    "\n",
    "We can finally observe that even if K-means was not the best clustering\n",
    "algorithm from a qualitatively point of view (as presented in the previous\n",
    "notebook), it is still helpful at crafting predictive features."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}