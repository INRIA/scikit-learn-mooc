{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making clusters part of a supervised pipeline\n",
    "\n",
    "This notebook explores how K-means clustering can be used as a feature\n",
    "engineering step to improve the performance of a regression model.\n",
    "\n",
    "Here we use the California Housing dataset, which includes information about\n",
    "the geographic location (latitude and longitude).\n",
    "\n",
    "Our goal is to predict the median house value (MedHouseVal) using a ridge\n",
    "regression model, to investigate whether adding features derived from applying\n",
    "K-means to geographic coordinates can improve the pipeline's predictive\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first design a predictive pipeline that completly ignores the\n",
    "coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=0\n",
    ")\n",
    "geo_columns = [\"Latitude\", \"Longitude\"]\n",
    "model_drop_geo = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [\n",
    "            (\"geo\", \"drop\", geo_columns),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha=1e-12),\n",
    ")\n",
    "cv_results_drop_geo = cross_validate(\n",
    "    model_drop_geo, data_train, target_train, scoring=\"r2\"\n",
    ")\n",
    "pd.DataFrame(cv_results_drop_geo).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a score of approximately 54% of the variance is explained by the\n",
    "non-geographical features.\n",
    "\n",
    "As seen in the previous notebook, we suspect that the price information may be\n",
    "linked to the distance to the nearest urban center, and proximity to the\n",
    "coast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def plot_map(df, color_feature):\n",
    "    fig = px.scatter_mapbox(\n",
    "        df,\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        color=color_feature,\n",
    "        zoom=5,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        mapbox_center={\n",
    "            \"lat\": df[\"Latitude\"].mean(),\n",
    "            \"lon\": df[\"Longitude\"].mean(),\n",
    "        },\n",
    "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_map(data, target)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first feed the coordinates directly to the linear model without\n",
    "transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naive_geo = make_pipeline(StandardScaler(), Ridge(alpha=1e-12))\n",
    "cv_results_naive_geo = cross_validate(\n",
    "    model_naive_geo, data_train, target_train, scoring=\"r2\"\n",
    ")\n",
    "pd.DataFrame(cv_results_naive_geo).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the geospatial data naively improves the performance a bit, however,\n",
    "we suspect that we can do better by introducing features that represent\n",
    "proximity to points of interests (urban centers, the coast, parks, etc.).\n",
    "\n",
    "We could look for a dataset containing all the coordinates of the city\n",
    "centers, the coast line and other points of interest in California, then\n",
    "manually engineer such features. However this would require a non-tricial\n",
    "amount of code. Instead we can rely on the K-means class to achieve something\n",
    "similar implicitly: we will configure K-means to find a large number of\n",
    "centroids from our housing data directly and consider each centroid a\n",
    "potential point of interest.\n",
    "\n",
    "The `KMeans` class implements a `transform` method that, given a set of data\n",
    "points as an argument, computes the distance to the nearest centroid for each\n",
    "of them. As a result, `KMeans` can be used as a preprocessing step in a\n",
    "feature engineering pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_cluster_geo = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [\n",
    "            (\"geo\", KMeans(n_clusters=100), geo_columns),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha=1e-12),\n",
    ")\n",
    "cv_results_cluster_geo = cross_validate(\n",
    "    model_cluster_geo, data_train, target_train, scoring=\"r2\"\n",
    ")\n",
    "pd.DataFrame(cv_results_cluster_geo).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a grid-search to tune `n_clusters` in this supervised\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_name = \"columntransformer__geo__n_clusters\"\n",
    "param_grid = {param_name: [10, 30, 100, 300, 1_000, 3_000]}\n",
    "grid_search = GridSearchCV(\n",
    "    model_cluster_geo, param_grid=param_grid, scoring=\"r2\"\n",
    ")\n",
    "grid_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_columns = [\n",
    "    \"mean_test_score\",\n",
    "    \"std_test_score\",\n",
    "    \"mean_fit_time\",\n",
    "    \"std_fit_time\",\n",
    "    \"mean_score_time\",\n",
    "    \"std_score_time\",\n",
    "    \"param_\" + param_name,\n",
    "]\n",
    "grid_search_results = pd.DataFrame(grid_search.cv_results_)[results_columns]\n",
    "grid_search_results = grid_search_results.rename(\n",
    "    columns={\"param_\" + param_name: \"n_clusters\"}\n",
    ").round(3)\n",
    "grid_search_results.sort_values(\"mean_test_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger number of clusters increases the predictive performance at the cost\n",
    "of larger fitting and prediction times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"mean_fit_time\": \"CV fit time (s)\",\n",
    "    \"mean_test_score\": \"CV score (R2)\",\n",
    "}\n",
    "fig = px.scatter(\n",
    "    grid_search_results,\n",
    "    x=\"mean_fit_time\",\n",
    "    y=\"mean_test_score\",\n",
    "    error_x=\"std_fit_time\",\n",
    "    error_y=\"std_test_score\",\n",
    "    hover_data=grid_search_results.columns,\n",
    "    labels=labels,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"Trade-off between fit time and mean test score\",\n",
    "        \"y\": 0.95,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally evaluate the best model found by our analysis and see how well\n",
    "it can generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Final model test R2 score: {grid_search.score(data_test, target_test):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates one way to leverage clustering for non-linear\n",
    "feature engineering, but there are many ways to compose unsupervised models in\n",
    "a supervised-learning pipeline.\n",
    "\n",
    "We can finally observe that even if K-means was not the best clustering\n",
    "algorithm from a qualitatively point of view (as presented in the previous\n",
    "notebook), it is still helpful at crafting predictive features."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}