{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-married",
   "metadata": {},
   "source": [
    "# Effect of the sample size in cross-validation\n",
    "\n",
    "In the previous notebook, we presented the general cross-validation framework\n",
    "and how to assess if a predictive model is underfiting, overfitting, or\n",
    "generalizing. Besides these aspects, it is also important to understand how\n",
    "the different errors are influenced by the number of samples available.\n",
    "\n",
    "In this notebook, we will show this aspect by looking a the\n",
    "variability of the different errors.\n",
    "\n",
    "Let's first load the data and create the same model as in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "data, target = housing.data, housing.target\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-african",
   "metadata": {},
   "source": [
    "\n",
    "## Ability of a model to learn depending of the sample size\n",
    "\n",
    "We recall that the size of the dataset is given by the number\n",
    "of rows in `data` / the length of the vector `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-hobby",
   "metadata": {},
   "source": [
    "Let's do an experiment and reduce the number of samples and repeat the\n",
    "previous experiment. We will create a function that define a `ShuffleSplit`\n",
    "and given a regressor and the data `data` and `target` will run a\n",
    "cross-validation. The function will finally return the testing error\n",
    "as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "\n",
    "def make_cv_analysis(regressor, data, target):\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "    cv_results = cross_validate(regressor, data, target,\n",
    "                                cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "                                return_train_score=True)\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    return (cv_results[\"test_score\"] * -1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-clock",
   "metadata": {},
   "source": [
    "Now that we have a function to run each experiment, we will create an array\n",
    "defining the number of samples for which we want to run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 500, 1000, 5000, 10000, 15000, target.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# to make our results reproducible\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# create a dictionary where we will store the result of each run\n",
    "scores_sample_sizes = {\"# samples\": [], \"test error\": []}\n",
    "for n_samples in sample_sizes:\n",
    "    # select a subset of the data with a specific number of samples\n",
    "    sample_idx = rng.choice(\n",
    "        np.arange(target.size), size=n_samples, replace=False)\n",
    "    data_sampled, target_sampled = data.iloc[sample_idx], target[sample_idx]\n",
    "    # run the experiment\n",
    "    score = make_cv_analysis(regressor, data_sampled, target_sampled)\n",
    "    # store the results\n",
    "    scores_sample_sizes[\"# samples\"].append(n_samples)\n",
    "    scores_sample_sizes[\"test error\"].append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-underwear",
   "metadata": {},
   "source": [
    "Now, we collected all our results and we will create a pandas dataframe to\n",
    "easily make some plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sample_sizes = pd.DataFrame(\n",
    "    np.array(scores_sample_sizes[\"test error\"]).T,\n",
    "    columns=scores_sample_sizes[\"# samples\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.displot(scores_sample_sizes, kind=\"kde\")\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Testing errors distribution \\n\"\n",
    "              \"by varying the sample size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-moscow",
   "metadata": {},
   "source": [
    "For the different sample sizes, we plotted the distribution of the\n",
    "testing error. We observe that the smaller the number of samples is,\n",
    "the larger the variance of the testing errors is. Thus, having a small\n",
    "number of samples might put us in a situation where it is impossible to get a\n",
    "reliable evaluation.\n",
    "\n",
    "## Learning curve\n",
    "\n",
    "Here, we plotted the different curves to highlight the issue of small sample\n",
    "size. However, this experiment is also used to draw the so-called **learning\n",
    "curve**. This curve gives some additional indication regarding the benefit of\n",
    "adding new training samples to improve a model's statistical performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "results = learning_curve(\n",
    "    regressor, data, target, train_sizes=sample_sizes[:-1], cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-commerce",
   "metadata": {},
   "source": [
    "Now, we can plot the curve curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "error_type = [\"training error\", \"Testing error\"]\n",
    "errors = [train_errors, test_errors]\n",
    "\n",
    "for name, err in zip(error_type, errors):\n",
    "    ax.plot(train_size, err.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "            alpha=0.8)\n",
    "    ax.fill_between(train_size, err.mean(axis=1) - err.std(axis=1),\n",
    "                    err.mean(axis=1) + err.std(axis=1),\n",
    "                    alpha=0.5, label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(train_size)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Number of samples in the training set\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Learning curve for decision tree\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-quick",
   "metadata": {},
   "source": [
    "We see that the more samples we add to the training set on this learning\n",
    "curve, the lower the error becomes. With this curve, we are searching for the\n",
    "plateau for which there is no benefit to adding samples anymore or assessing\n",
    "the potential gain of adding more samples into the training set.\n",
    "\n",
    "For this dataset we notice that our decision tree model would really benefit\n",
    "from additional datapoints to reduce the amount of over-fitting and hopefully\n",
    "reduce the testing error even further.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In the notebook, we learnt:\n",
    "\n",
    "* the influence of the number of samples in a dataset, especially on the\n",
    "  variability of the errors reported when running the cross-validation;\n",
    "* about the learning curve that is a visual representation of the capacity\n",
    "  of a model to improve by adding new samples."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
