{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of the sample size in cross-validation\n",
    "\n",
    "In the previous notebook, we presented the general cross-validation framework\n",
    "and how to assess if a predictive model is underfiting, overfitting or\n",
    "generalizing.\n",
    "\n",
    "Besides these aspects, it is also important to understand the influence\n",
    "of the number of samples available on the different errors. In this notebook,\n",
    "we will show this aspect by looking a the variability of the different\n",
    "errors.\n",
    "\n",
    "Let's first load the data and create the identical model as in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ability of a model to learn depending of the sample size\n",
    "\n",
    "We recall that the size of the dataset is given as the number of row in `X`\n",
    "or the length of the vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an experiment and reduce the number of samples and repeat the\n",
    "previous experiment. We will create a function that define a `ShuffleSplit`\n",
    "and given a regressor and the data `X` and `y` will run a cross-validation.\n",
    "The function will finally return the generalization error as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "\n",
    "def make_cv_analysis(regressor, X, y):\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "    cv_results = cross_validate(regressor, X, y,\n",
    "                                cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "                                return_train_score=True)\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    return (cv_results[\"test_score\"] * -1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to run each experiment, we will create an array\n",
    "defining the number of samples for which we want to run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 500, 1000, 5000, 10000, 15000, y.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# to make our results reproducible\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# create a dictionary where we will store the result of each run\n",
    "scores_sample_sizes = {\"# samples\": [], \"test error\": []}\n",
    "for n_samples in sample_sizes:\n",
    "    # select a subset of the data with a specific number of samples\n",
    "    sample_idx = rng.choice(np.arange(y.size), size=n_samples, replace=False)\n",
    "    X_sampled, y_sampled = X.iloc[sample_idx], y[sample_idx]\n",
    "    # run the experiment\n",
    "    score = make_cv_analysis(regressor, X_sampled, y_sampled)\n",
    "    # store the results\n",
    "    scores_sample_sizes[\"# samples\"].append(n_samples)\n",
    "    scores_sample_sizes[\"test error\"].append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we collected all our results and we will create a pandas dataframe to\n",
    "easily make some plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sample_sizes = pd.DataFrame(\n",
    "    np.array(scores_sample_sizes[\"test error\"]).T,\n",
    "    columns=scores_sample_sizes[\"# samples\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.displot(scores_sample_sizes, kind=\"kde\")\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Generalization errors distribution \\n\"\n",
    "              \"by varying the sample size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the different sample sizes, we plotted the distribution of the\n",
    "generalization error. We observe that smaller is the sample size; larger is\n",
    "the variance of the generalization errors. Thus, having a small number of\n",
    "samples might put us in a situation where it is impossible to get a reliable\n",
    "evaluation.\n",
    "\n",
    "## Learning curve\n",
    "\n",
    "Here, we plotted the different curves to highlight the issue of small sample\n",
    "size. However, this experiment is also used to draw the so-called **learning\n",
    "curve**. This curve gives some additional indication regarding the benefit of\n",
    "adding new training samples to improve a model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "results = learning_curve(\n",
    "    regressor, X, y, train_sizes=sample_sizes[:-1], cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the curve curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "error_type = [\"Empirical error\", \"Generalization error\"]\n",
    "errors = [train_errors, test_errors]\n",
    "\n",
    "for name, err in zip(error_type, errors):\n",
    "    ax.plot(train_size, err.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "            alpha=0.8)\n",
    "    ax.fill_between(train_size, err.mean(axis=1) - err.std(axis=1),\n",
    "                    err.mean(axis=1) + err.std(axis=1),\n",
    "                    alpha=0.5, label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(train_size)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Number of samples in the training set\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Learning curve for decision tree\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the more samples we add to the training set on this learning\n",
    "curve, the lower the error becomes. With this curve, we are searching for the\n",
    "plateau for which there is no benefit to adding samples anymore or assessing\n",
    "the potential gain of adding more samples into the training set.\n",
    "\n",
    "For this dataset we notice that our decision tree model would really benefit\n",
    "from additional datapoints to reduce the amount of over-fitting and hopefully\n",
    "reduce the generalization error even further.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In the notebook, we learnt:\n",
    "\n",
    "* the influence of the number of samples in a dataset, specifically on the\n",
    "  variability of the errors reported when running the cross-validation;\n",
    "* about the learning curve that is a visual representation of the capacity\n",
    "  of a model to improve by adding new samples."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
