{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested cross-validation\n",
    "\n",
    "In this notebook, we show a pattern called **nested cross-validation** which\n",
    "should be used when you want to both evaluate a model and tune the\n",
    "model's hyperparameters.\n",
    "\n",
    "Cross-validation is a powerful tool to evaluate the generalization performance\n",
    "of a model. It is also used to select the best model from a pool of models.\n",
    "This pool of models can be the same family of predictor but with different\n",
    "parameters. In this case, we call this procedure **hyperparameter tuning**.\n",
    "\n",
    "We could also imagine that we would like to choose among heterogeneous models\n",
    "that will similarly use the cross-validation.\n",
    "\n",
    "Before we go into details regarding the nested cross-validation, we will\n",
    "first recall the pattern used to fine tune a model's hyperparameters.\n",
    "\n",
    "Let's load the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data, target = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll make a minimal example using the utility `GridSearchCV` to find\n",
    "the best parameters via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [.01, .1]}\n",
    "model_to_tune = SVC()\n",
    "\n",
    "search = GridSearchCV(estimator=model_to_tune, param_grid=param_grid,\n",
    "                      n_jobs=2)\n",
    "search.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that `GridSearchCV` will train a model with some specific parameter\n",
    "on a training set and evaluate it on testing. However, this evaluation is\n",
    "done via cross-validation using the `cv` parameter. This procedure is\n",
    "repeated for all possible combinations of parameters given in `param_grid`.\n",
    "\n",
    "The attribute `best_params_` will give us the best set of parameters that\n",
    "maximize the mean score on the internal test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameter found are: {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show the mean score obtained using the parameter `best_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean score in CV is: {search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, one should be extremely careful using this score. The\n",
    "misinterpretation would be the following: since the score was computed on a\n",
    "test set, it could be considered our model's testing score.\n",
    "\n",
    "However, we should not forget that we used this score to pick-up the best\n",
    "model. It means that we used knowledge from the test set (i.e. test score) to\n",
    "decide our model's training parameter.\n",
    "\n",
    "Thus, this score is not a reasonable estimate of our testing error.\n",
    "Indeed, we can show that it will be too optimistic in practice. The good way\n",
    "is to use a \"nested\" cross-validation. We will use an inner cross-validation\n",
    "corresponding to the previous procedure shown to optimize the\n",
    "hyperparameters. We will also include this procedure within an outer\n",
    "cross-validation, which will be used to estimate the testing error of\n",
    "our tuned model.\n",
    "\n",
    "In this case, our inner cross-validation will always get the training set of\n",
    "the outer cross-validation, making it possible to compute the testing\n",
    "score on a completely independent set.\n",
    "\n",
    "We will show below how we can create such nested cross-validation and obtain\n",
    "the testing score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Declare the inner and outer cross-validation\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model = GridSearchCV(\n",
    "    estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=2)\n",
    "\n",
    "# Outer cross-validation to compute the testing score\n",
    "test_score = cross_val_score(model, data, target, cv=outer_cv, n_jobs=2)\n",
    "print(f\"The mean score using nested cross-validation is: \"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the reported score is more trustful and should be close\n",
    "to production's expected generalization performance.\n",
    "\n",
    "We will illustrate the difference between the nested and non-nested\n",
    "cross-validation scores to show that the latter one will be too optimistic in\n",
    "practice. In this regard, we will repeat several time the experiment and\n",
    "shuffle the data differently. Besides, we will store the score obtain with\n",
    "and without the nested cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_not_nested = []\n",
    "test_score_nested = []\n",
    "\n",
    "N_TRIALS = 20\n",
    "for i in range(N_TRIALS):\n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid,\n",
    "                         cv=inner_cv, n_jobs=2)\n",
    "    model.fit(data, target)\n",
    "    test_score_not_nested.append(model.best_score_)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    test_score = cross_val_score(model, data, target, cv=outer_cv, n_jobs=2)\n",
    "    test_score_nested.append(test_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can merge the data together and make a box plot of the two strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_scores = {\n",
    "    \"Not nested CV\": test_score_not_nested,\n",
    "    \"Nested CV\": test_score_nested,\n",
    "}\n",
    "all_scores = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
    "all_scores.plot.box(color=color, vert=False)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "_ = plt.title(\"Comparison of mean accuracy obtained on the test sets with\\n\"\n",
    "              \"and without nested cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model's generalization performance with the nested\n",
    "cross-validation is not as good as the non-nested cross-validation.\n",
    "\n",
    "As a conclusion, when optimizing parts of the machine learning pipeline (e.g.\n",
    "hyperparameter, transform, etc.), one needs to use nested cross-validation to\n",
    "evaluate the generalization performance of the predictive model. Otherwise, the\n",
    "results obtained without nested cross-validation are over-optimistic."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}