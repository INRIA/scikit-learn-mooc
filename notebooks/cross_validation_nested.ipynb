{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested cross-validation\n",
    "\n",
    "In this notebook, we show a pattern called **nested cross-validation** which\n",
    "is should be used when you want to both evaluate a model and tune the\n",
    "hyperparameters' model.\n",
    "\n",
    "Cross-validation is a powerful tool to evaluate the performance of a model.\n",
    "It is also used to select the best model from a pool of models. This pool of\n",
    "models can be the same family of predictor but with different parameters. In\n",
    "this case, we call this procedure **fine-tuning** of the model\n",
    "hyperparameters.\n",
    "\n",
    "We could also imagine that we would like to choose among heterogeneous models\n",
    "that will similarly use the cross-validation.\n",
    "\n",
    "Before to go into details regarding the nested cross-validation, we will\n",
    "first recall the patter used to fine tune hyperparameters' model.\n",
    "\n",
    "First, we will load the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we show a minimal example of using the utility `GridSearchCV` to find\n",
    "the best parameters via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [.01, .1]}\n",
    "model_to_tune = SVC()\n",
    "\n",
    "search = GridSearchCV(estimator=model_to_tune, param_grid=param_grid,\n",
    "                      n_jobs=-1)\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that `GridSearchCV` will train a model with some specific parameter\n",
    "on a training set and evaluate it on testing. However, this evaluation is\n",
    "done via cross-validation using the `cv` parameter. This procedure is\n",
    "repeated for all possible combinations of parameters given in `param_grid`.\n",
    "\n",
    "The attribute `best_params_` will give us the best set of parameters that\n",
    "maximize the mean score on the internal test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameter found are: {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now show the mean score obtained using the parameter `best_score_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean score in CV is: {search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, one should be extremely careful using this score. The\n",
    "misinterpretation would be the following: since the score was computed on a\n",
    "test set, it could be considered our model's generalization score.\n",
    "\n",
    "However, we should not forget that we used this score to pick-up the best\n",
    "model. It means that we used knowledge from the test set (i.e. test score) to\n",
    "decide our model's training parameter.\n",
    "\n",
    "Thus, this score is not a reasonable estimate of our generalization error.\n",
    "Indeed, we can show that it will be too optimistic in practice. The good way\n",
    "is to use a \"nested\" cross-validation. We will use an inner cross-validation\n",
    "corresponding to the previous procedure shown to optimize the\n",
    "hyperparameters. We will also include this procedure within an outer\n",
    "cross-validation, which will be used to estimate the generalization error of\n",
    "our fine-tuned model.\n",
    "\n",
    "In this case, our inner cross-validation will always get the training set of\n",
    "the outer cross-validation, making it possible to compute the generalization\n",
    "score on a completely independent set.\n",
    "\n",
    "We will show below how we can create such nested cross-validation and obtain\n",
    "the generalization score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Declare the inner and outer cross-validation\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model = GridSearchCV(\n",
    "    estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# Outer cross-validation to compute the generalization score\n",
    "test_score = cross_val_score(model, X, y, cv=outer_cv, n_jobs=-1)\n",
    "print(f\"The mean score using nested cross-validation is: \"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the reported score is more trustful and should be close\n",
    "to production's expected performance.\n",
    "\n",
    "We will illustrate the difference between the nested and non-nested\n",
    "cross-validation scores to show that the latter one will be too optimistic in\n",
    "practice. In this regard, we will repeat several time the experiment and\n",
    "shuffle the data differently. Besides, we will store the score obtain with\n",
    "and without the nested cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_not_nested = []\n",
    "test_score_nested = []\n",
    "\n",
    "N_TRIALS = 20\n",
    "for i in range(N_TRIALS):\n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "    # Non_nested parameter search and scoring\n",
    "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid,\n",
    "                         cv=inner_cv, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    test_score_not_nested.append(model.best_score_)\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    test_score = cross_val_score(model, X, y, cv=outer_cv, n_jobs=-1)\n",
    "    test_score_nested.append(test_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can merge the dta together and make a box plot of the two strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_scores = {\n",
    "    \"Not nested CV\": test_score_not_nested,\n",
    "    \"Nested CV\": test_score_nested,\n",
    "}\n",
    "all_scores = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "# Define the style of the box style\n",
    "boxplot_property = {\n",
    "    \"vert\": False, \"whis\": 100, \"patch_artist\": True, \"widths\": 0.3,\n",
    "    \"boxprops\": dict(linewidth=3, color='black', alpha=0.9),\n",
    "    \"medianprops\": dict(linewidth=2.5, color='black', alpha=0.9),\n",
    "    \"whiskerprops\": dict(linewidth=3, color='black', alpha=0.9),\n",
    "    \"capprops\": dict(linewidth=3, color='black', alpha=0.9),\n",
    "}\n",
    "\n",
    "\n",
    "ax = all_scores.plot.box(**boxplot_property)\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "_ = ax.set_title(\"Comparison of mean accuracy obtained on the test sets with\\n\"\n",
    "                 \"and without nested cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model's performance with the nested cross-validation is\n",
    "not as good as the non-nested cross-validation.\n",
    "\n",
    "As a conclusion, when optimizing part of the machine learning pipeline\n",
    "(e.g. hyperparameter, transform, etc.), one needs to use nested\n",
    "cross-validation to evaluate the performance of the predictive model.\n",
    "Otherwise, the results obtained without nested cross-validation are too\n",
    "over-optimistic."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
