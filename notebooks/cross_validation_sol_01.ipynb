{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ordinary-brunei",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 01\n",
    "\n",
    "The aim of this exercise is to make the following experiments:\n",
    "\n",
    "* train and test a support vector machine classifier through\n",
    "  cross-validation;\n",
    "* study the effect of the parameter gamma of this classifier using a\n",
    "  validation curve;\n",
    "* study if it would be useful in term of classification if we could add new\n",
    "  samples in the dataset using a learning curve.\n",
    "\n",
    "To make these experiments we will first load the blood transfusion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "blood_transfusion = pd.read_csv(\"../datasets/blood_transfusion.csv\")\n",
    "data = blood_transfusion.drop(columns=\"Class\")\n",
    "target = blood_transfusion[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-charger",
   "metadata": {},
   "source": [
    "We will use a support vector machine classifier (SVM). In its most simple\n",
    "form, a SVM classifier is a linear classifier behaving similarly to a\n",
    "logistic regression. Indeed, the optimization used to find the optimal\n",
    "weights of the linear model are different but we don't need to know these\n",
    "details for the exercise.\n",
    "\n",
    "Also, this classifier can become more flexible/expressive by using a\n",
    "so-called kernel. The model becomes non-linear. Again, no requirement\n",
    "regarding the mathematics is required to accomplish this exercise.\n",
    "\n",
    "We will use an RBF kernel where a parameter `gamma` allows to tune the\n",
    "flexibility of the model.\n",
    "\n",
    "First let's create a predictive pipeline made of:\n",
    "\n",
    "* a [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "  with default parameter;\n",
    "* a [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "  where the parameter `kernel` could be set to `\"rbf\"`. Note that this is the\n",
    "  default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = make_pipeline(StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-daughter",
   "metadata": {},
   "source": [
    "Evaluate the statistical performance of your model by cross-validation with a\n",
    "`ShuffleSplit` scheme. Thus, you can use\n",
    "[`sklearn.model_selection.cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n",
    "and pass a [`sklearn.model_selection.ShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)\n",
    "to the `cv` parameter. Only fix the `random_state=0` in the `ShuffleSplit`\n",
    "and let the other parameters to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(random_state=0)\n",
    "cv_results = cross_validate(model, data, target, cv=cv, n_jobs=-1)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy score of our model:\\n\"\n",
    "    f\"{cv_results['test_score'].mean():.3f} +/- \"\n",
    "    f\"{cv_results['test_score'].std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-victor",
   "metadata": {},
   "source": [
    "As previously mentioned, the parameter `gamma` is one of the parameter\n",
    "controlling under/over-fitting in support vector machine with an RBF kernel.\n",
    "\n",
    "Compute the validation curve to evaluate the effect of the parameter `gamma`.\n",
    "You can vary its value between `10e-3` and `10e2` by generating samples on a\n",
    "logarithmic scale. Thus, you can use `np.logspace(-3, 2, num=30)`.\n",
    "\n",
    "Since we are manipulating a `Pipeline` the parameter name will be set to\n",
    "`svc__gamma` instead of only `gamma`. You can retrieve the parameter name\n",
    "using `model.get_params().keys()`. We will go more into details regarding\n",
    "accessing and setting hyperparameter in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "gammas = np.logspace(-3, 2, num=30)\n",
    "param_name = \"svc__gamma\"\n",
    "train_scores, test_scores = validation_curve(\n",
    "    model, data, target, param_name=param_name, param_range=gammas, cv=cv,\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-theory",
   "metadata": {},
   "source": [
    "Plot the validation curve for the train and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "for name, scores in zip(\n",
    "    [\"Training score\", \"Testing score\"], [train_scores, test_scores]\n",
    "):\n",
    "    ax.plot(\n",
    "        gammas, scores.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "        alpha=0.8)\n",
    "    ax.fill_between(\n",
    "        gammas, scores.mean(axis=1) - scores.std(axis=1),\n",
    "        scores.mean(axis=1) + scores.std(axis=1),\n",
    "        alpha=0.5, label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(gammas)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(r\"Value of hyperparameter $\\gamma$\")\n",
    "ax.set_ylabel(\"Accuracy score\")\n",
    "ax.set_title(\"Validation score of support vector machine\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-behalf",
   "metadata": {},
   "source": [
    "Looking at the curve, we can clearly identify the over-fitting regime of\n",
    "the SVC classifier when `gamma > 1`.\n",
    "The best setting is around `gamma = 1` while for `gamma < 1`,\n",
    "it is not very clear if the classifier is under-fitting but the\n",
    "testing score is worse than for `gamma = 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-syndication",
   "metadata": {},
   "source": [
    "Now, you can perform an analysis to check whether adding new samples to the\n",
    "dataset could help our model to better generalize. Compute the learning curve\n",
    "(using [`sklearn.model_selection.learning_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html))\n",
    "by computing the train and test scores for different training dataset size.\n",
    "Plot the train and test scores with respect to the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1, num=10)\n",
    "results = learning_curve(\n",
    "    model, data, target, train_sizes=train_sizes, cv=cv, n_jobs=-1)\n",
    "train_size, train_scores, test_scores = results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "for name, scores in zip(\n",
    "    [\"Training score\", \"Testing score\"], [train_scores, test_scores]\n",
    "):\n",
    "    ax.plot(\n",
    "        train_sizes, scores.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "        alpha=0.8)\n",
    "    ax.fill_between(\n",
    "        train_sizes, scores.mean(axis=1) - scores.std(axis=1),\n",
    "        scores.mean(axis=1) + scores.std(axis=1),\n",
    "        alpha=0.5, label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(train_sizes)\n",
    "ax.set_xlabel(\"Number of samples in the training set\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Learning curve for support vector machine\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-blame",
   "metadata": {},
   "source": [
    "We observe that adding new samples in the dataset does not improve the\n",
    "testing score. We can only conclude that the standard deviation of\n",
    "the training error is decreasing when adding more samples which is not a\n",
    "surprise."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
