{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The framework and why do we need it\n",
    "\n",
    "In this notebook, we present the general cross-validation framework. Before\n",
    "to go into details, we will linger on the reasons for always having training\n",
    "and testing sets. Let's first look at the limitation of using a dataset\n",
    "without keeping any samples out.\n",
    "\n",
    "To illustrate the different concepts, we will use the california housing\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that in this dataset, the aim is to predict the median value of\n",
    "houses in an area in California. The feature collected are based on general\n",
    "real-estate and geographical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify future visualization, we transform the target in k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y *= 100\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical error vs generalization error\n",
    "\n",
    "As mentioned previously, we start by fitting a decision tree regressor on the\n",
    "entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the regressor, we would like to know the regressor's potential\n",
    "performance once deployed in production. For this purpose, we use the mean\n",
    "absolute error, which gives us an error in the native unit, i.e. k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = regressor.predict(X)\n",
    "score = mean_absolute_error(y_pred, y)\n",
    "print(f\"In average, our regressor make an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We get perfect prediction with no error. It is too optimistic and almost\n",
    "always revealing a methodological problem when doing machine learning.\n",
    "\n",
    "Indeed, we trained and predicted on the same dataset. Since our decision tree\n",
    "was fully grown, every sample in the dataset is stored in a leaf node.\n",
    "Therefore, our decision tree fully memorized the dataset given during `fit`\n",
    "and make no single error when predicting on the same data.\n",
    "\n",
    "This error computed above is called the **empirical error** or **training\n",
    "error**.\n",
    "\n",
    "We trained a predictive model to minimize the empirical error but our aim is\n",
    "to minimize the error on data that has not been seen during training.\n",
    "\n",
    "This error is also called the **generalization error** or the \"true\"\n",
    "**testing error**. Thus, the most basic evaluation involves:\n",
    "\n",
    "* splitting our dataset into two subsets: a training set and a testing set;\n",
    "* fitting the model on the training set;\n",
    "* estimating the empirical error on the training set;\n",
    "* estimating the generalization error on the testing set.\n",
    "\n",
    "So let's split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our model only on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can estimate the different type of errors. Let's start by\n",
    "computing the empirical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_train)\n",
    "score = mean_absolute_error(y_pred, y_train)\n",
    "print(f\"The empirical error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the same phenomena than in the previous experiment. Our model\n",
    "memorized the training set. However, we can now compute the generalization\n",
    "error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "score = mean_absolute_error(y_pred, y_test)\n",
    "print(f\"The generalization error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization error is not minimum and equal to zero. Indeed, this\n",
    "error is closer to the performance of our model if it was deployed in\n",
    "production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of the cross-validation estimates\n",
    "\n",
    "When doing a single train-test split we don't give any indication\n",
    "regarding the robustness of the evaluation of our predictive model: in\n",
    "particular, if the test set is small, this estimate of the generalization\n",
    "error can be unstable and do not reflect the \"true error rate\" we would have\n",
    "observed with the same model on an unlimited amount of test data.\n",
    "\n",
    "For instance, we could have been lucky when we did our random split of our\n",
    "limited dataset and isolated some of the easiest cases to predict in the\n",
    "testing set just by chance: the estimation of the generalization error would\n",
    "be overly optimistic, in this case.\n",
    "\n",
    "**Cross-validation** allows estimating the robustness of a predictive model\n",
    "by repeating the splitting procedure. It will give several empirical and\n",
    "generalization errors and thus some **estimate of the variability of the\n",
    "model performance**.\n",
    "\n",
    "There are different cross-validation strategies, for now we are going to\n",
    "focus on one called \"shuffle-split\". At each iteration of this strategy we:\n",
    "\n",
    "- shuffle the order of the samples of a copy of the full data at random;\n",
    "- split the shuffled dataset into a train and a test set;\n",
    "- train a new model on the train set;\n",
    "- evaluate the generalization error on the test set.\n",
    "\n",
    "We repeat this procedure `n_splits` times. Using `n_splits=30` means that we\n",
    "will train 30 models in total and all of them will be discarded: we just\n",
    "record their performance on each variant of the test set.\n",
    "\n",
    "To evaluate the performance of our regressor, we can use `cross_validate`\n",
    "with a `ShuffleSplit` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "cv_results = cross_validate(\n",
    "    regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results `cv_results` are stored into a Python dictionary. We will convert\n",
    "it into a pandas dataframe to ease visualization and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, scikit-learn model evaluation tools always use a convention\n",
    "where \"higher is better\", this explains we used\n",
    "`scoring=\"neg_mean_absolute_error\"` (meaning \"negative mean absolute error\").\n",
    "\n",
    "Let us revert the negation to get the actual error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results reported by the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get timing information to fit and predict at each round of\n",
    "cross-validation. Also, we get the test score, which corresponds to the\n",
    "generalization error on each of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 30 entries in our resulting dataframe because we performed 30 splits.\n",
    "Therefore, we can show the generalization error distribution and thus, have\n",
    "an estimate of its variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.displot(cv_results[\"test_error\"], kde=True, bins=10)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the generalization error is clustered around 45.5 k\\$ and\n",
    "ranges from 43 k\\$ to 49 k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validated generalization error is: \"\n",
    "      f\"{cv_results['test_error'].mean():.2f} k$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The standard deviation of the generalization error is: \"\n",
    "      f\"{cv_results['test_error'].std():.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standard deviation is much smaller than the mean: we could\n",
    "summarize that our cross-validation estimate of the generalization error is\n",
    "45.7 +/- 1.1 k\\$.\n",
    "\n",
    "If we were to train a single model on the full dataset (without\n",
    "cross-validation) and then had later access to an unlimited amount of test\n",
    "data, we would expect its true generalization error to fall close to that\n",
    "region.\n",
    "\n",
    "While this information is interesting in it-self, this should be contrasted\n",
    "to the scale of the natural variability of the target `y` in our dataset.\n",
    "\n",
    "Let us plot the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(y, kde=True, bins=20)\n",
    "_ = plt.xlabel(\"Median House Value (k$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The standard deviation of the target is: {y.std():.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable ranges from close to 0 k\\$ up to 500 k\\$ and, with a\n",
    "standard deviation around 115 k\\$.\n",
    "\n",
    "We notice that the mean estimate of the generalization error obtained by\n",
    "cross-validation is a bit than the natural scale of variation of the target\n",
    "variable. Furthermore the standard deviation of the cross validation estimate\n",
    "of the generalization error is even much smaller.\n",
    "\n",
    "This is a good start, but not necessarily enough to decide whether the\n",
    "generalization performance is good enough to make our prediction useful in\n",
    "practice.\n",
    "\n",
    "We recall that our model makes, on average, an error around 45 k\\$. With this\n",
    "information and looking at the target distribution, such an error might be\n",
    "acceptable when predicting houses with a 500 k\\$. However, it would be an\n",
    "issue with a house with a value of 50 k\\$. Thus, this indicates that our\n",
    "metric (Mean Absolute Error) is not ideal.\n",
    "\n",
    "We might instead choose a metric relative to the target value to predict: the\n",
    "mean absolute percentage error would have been a much better choice.\n",
    "\n",
    "But in all cases, an error of 45 k\\$ might be too large to automatically use\n",
    "our model to tag house value without expert supervision.\n",
    "\n",
    "To better understand the performance of our model and maybe find insights on\n",
    "how to improve it we will compare the generalization error with the empirical\n",
    "error. Thus, we need to compute the error on the training set, which is\n",
    "possible using the `cross_validate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(regressor, X, y,\n",
    "                            cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "                            return_train_score=True, n_jobs=2)\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the train and test score and take the error instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "scores[[\"train error\", \"test error\"]] = -cv_results[\n",
    "    [\"train_score\", \"test_score\"]]\n",
    "sns.histplot(scores, bins=50)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the distribution of the empirical and generalization errors, we\n",
    "get information about whether our model is over-fitting, under-fitting (or\n",
    "both at the same time).\n",
    "\n",
    "Here, we observe a **small empirical error** (actually zero), meaning that\n",
    "the model is **not under-fitting**: it is flexible enough to capture any\n",
    "variations present in the training set.\n",
    "\n",
    "However the **significantly larger generalization error** tells us that the\n",
    "model is **over-fitting**: the model has memorized many variations of the\n",
    "training set that could be considered \"noisy\" because they do not generalize\n",
    "to help us make good prediction on the test set.\n",
    "\n",
    "Some model hyperparameters are usually the key to go from a model that\n",
    "underfits to a model that overfits, hopefully going through a region were we\n",
    "can get a good balance between the two. We can acquire knowledge by plotting\n",
    "a curve called the validation curve. This curve applies the above experiment\n",
    "and varies the value of a hyperparameter.\n",
    "\n",
    "For the decision tree, the `max_depth` the main parameter to control the\n",
    "trade-off between under-fitting and over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth = [1, 5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    regressor, X, y, param_name=\"max_depth\", param_range=max_depth,\n",
    "    cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we collected the results, we will show the validation curve by\n",
    "plotting the empirical and generalization errors (as well as their\n",
    "deviations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "error_type = [\"Empirical error\", \"Generalization error\"]\n",
    "errors = [train_errors, test_errors]\n",
    "\n",
    "for name, err in zip(error_type, errors):\n",
    "    ax.plot(max_depth, err.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "            alpha=0.8)\n",
    "    ax.fill_between(max_depth, err.mean(axis=1) - err.std(axis=1),\n",
    "                    err.mean(axis=1) + err.std(axis=1), alpha=0.5,\n",
    "                    label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(max_depth)\n",
    "ax.set_xlabel(\"Maximum depth of decision tree\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Validation curve for decision tree\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation curve can be divided into three areas:\n",
    "\n",
    "- For `max_depth < 10`, the decision tree underfits. The empirical error and\n",
    "  therefore also the generalization error are both high. The model is too\n",
    "  constrained and cannot capture much of the variability of the target\n",
    "  variable.\n",
    "\n",
    "- The region around `max_depth = 10` corresponds to the parameter for which\n",
    "  the decision tree generalizes the best. It is flexible enough to capture a\n",
    "  fraction of the variability of the target that generalizes, while not\n",
    "  memorizing all of the noise in the target.\n",
    "\n",
    "- For `max_depth > 10`, the decision tree overfits. The empirical error\n",
    "  becomes very small, while the generalization error increases. In this\n",
    "  region, the models captures too much of the noisy part of the variations of\n",
    "  the target and this harms its ability to generalize well to test data.\n",
    "\n",
    "Note that for `max_depth = 10`, the model overfits a bit as there is a gap\n",
    "between the empirical error and the generalization error. It can also\n",
    "potentially underfit also a bit at the same time, because the empirical error\n",
    "is still far from zero (more than 30 k\\$), meaning that the model might\n",
    "still be too constrained to model interesting parts of the data. However the\n",
    "generalization error is minimal, and this is what really matters. This is the\n",
    "best compromise we could reach by just tuning this parameter.\n",
    "\n",
    "We were lucky that the variance of the errors was small compared to their\n",
    "respective values, and therefore the conclusions above are quite clear. This\n",
    "is not necessarily always the case.\n",
    "\n",
    "We will now focus on one factor that can affect this variance, namely, the\n",
    "size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Let's do an experiment and reduce the number of samples and repeat the\n",
    "previous experiment. We will create a function that define a `ShuffleSplit`\n",
    "and given a regressor and the data `X` and `y` will run a cross-validation.\n",
    "The function will finally return the generalization error as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cv_analysis(regressor, X, y):\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "    cv_results = cross_validate(regressor, X, y,\n",
    "                                cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "                                return_train_score=True)\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    return (cv_results[\"test_score\"] * -1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to run each experiment, we will create an array\n",
    "defining the number of samples for which we want to run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 500, 1000, 5000, 10000, 15000, y.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# to make our results reproducible\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# create a dictionary where we will store the result of each run\n",
    "scores_sample_sizes = {\"# samples\": [], \"test error\": []}\n",
    "for n_samples in sample_sizes:\n",
    "    # select a subset of the data with a specific number of samples\n",
    "    sample_idx = rng.choice(np.arange(y.size), size=n_samples, replace=False)\n",
    "    X_sampled, y_sampled = X.iloc[sample_idx], y[sample_idx]\n",
    "    # run the experiment\n",
    "    score = make_cv_analysis(regressor, X_sampled, y_sampled)\n",
    "    # store the results\n",
    "    scores_sample_sizes[\"# samples\"].append(n_samples)\n",
    "    scores_sample_sizes[\"test error\"].append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we collected all our results and we will create a pandas dataframe to\n",
    "easily make some plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sample_sizes = pd.DataFrame(\n",
    "    np.array(scores_sample_sizes[\"test error\"]).T,\n",
    "    columns=scores_sample_sizes[\"# samples\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(scores_sample_sizes, kind=\"kde\")\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Generalization errors distribution \\n\"\n",
    "              \"by varying the sample size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the different sample sizes, we plotted the distribution of the\n",
    "generalization error. We observe that smaller is the sample size; larger is\n",
    "the variance of the generalization errors. Thus, having a small number of\n",
    "samples might put us in a situation where it is impossible to get a reliable\n",
    "evaluation.\n",
    "\n",
    "Here, we plotted the different curve to highlight the issue of small sample\n",
    "size. However, this experiment is also used to draw the so-called **learning\n",
    "curve**. This curve gives some additional indication regarding the benefit of\n",
    "adding new training samples to improve a model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "results = learning_curve(\n",
    "    regressor, X, y, train_sizes=sample_sizes[:-1], cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the curve curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "error_type = [\"Empirical error\", \"Generalization error\"]\n",
    "errors = [train_errors, test_errors]\n",
    "\n",
    "for name, err in zip(error_type, errors):\n",
    "    ax.plot(train_size, err.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "            alpha=0.8)\n",
    "    ax.fill_between(train_size, err.mean(axis=1) - err.std(axis=1),\n",
    "                    err.mean(axis=1) + err.std(axis=1),\n",
    "                    alpha=0.5, label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(train_size)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Number of samples in the training set\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Learning curve for decision tree\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the more samples we add to the training set on this learning\n",
    "curve, the lower the error becomes. With this curve, we are searching for the\n",
    "plateau for which there is no benefit to adding samples anymore or assessing\n",
    "the potential gain of adding more samples into the training set.\n",
    "\n",
    "For this dataset we notice that our decision tree model would really benefit\n",
    "from additional datapoints to reduce the amount of over-fitting and hopefully\n",
    "reduce the generalization error even further.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we saw:\n",
    "\n",
    "* the necessity of splitting the data into a train and test set;\n",
    "* the meaning of the empirical and generalization errors;\n",
    "* the overall cross-validation framework with the possibility to study\n",
    "  performance variations;\n",
    "* the effect of hyperparameter tuning using the validation curve;\n",
    "* the effect of sample size using the learning curve."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
