{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The framework and why do we need it\n",
    "\n",
    "In this notebook, we present the general cross-validation framework. Before\n",
    "to go into details, we will linger on the reasons for always having training\n",
    "and testing sets. Let's first look at the limitation of using a dataset\n",
    "without keeping any samples out.\n",
    "\n",
    "To illustrate the different concepts, we will use the california housing\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that in this dataset, the aim is to predict the median value of\n",
    "houses in an area in California. The feature collected are based on general\n",
    "real-estate and geographical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify future visualization, we transform the target in k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y *= 100\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical error vs generalization error\n",
    "\n",
    "As mentioned previously, we start by fitting a decision tree regressor on the\n",
    "entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the regressor, we would like to know the regressor's potential\n",
    "performance once deployed in production. For this purpose, we use the mean\n",
    "absolute error, which gives us an error in the native unit, i.e. k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = regressor.predict(X)\n",
    "score = mean_absolute_error(y_pred, y)\n",
    "print(f\"In average, our regressor make an error of {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We get perfect prediction with no error. It is too optimistic and almost\n",
    "always revealing a methodological problem when doing machine learning.\n",
    "\n",
    "Indeed, we trained and predicted on the same dataset. Since our decision tree\n",
    "was fully grown, every sample in the dataset is stored in a leaf node.\n",
    "Therefore, our decision tree fully memorized the dataset given during `fit`\n",
    "and make no single error when predicting on the same data.\n",
    "\n",
    "This error computed above is called the **empirical error** or **training\n",
    "error**.\n",
    "\n",
    "We trained a predictive model to minimize the empirical error but our aim is\n",
    "to minimize the error on data that has not been seen during training.\n",
    "\n",
    "This error is also called the **generalization error** or the \"true\"\n",
    "**testing error**. Thus, the most basic evaluation involves:\n",
    "\n",
    "* splitting our dataset into two subsets: a training set and a testing set;\n",
    "* fitting the model on the training set;\n",
    "* estimating the empirical error on the training set;\n",
    "* estimating the generalization error on the testing set.\n",
    "\n",
    "So let's split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our model only on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can estimate the different type of errors. Let's start by\n",
    "computing the empirical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_train)\n",
    "score = mean_absolute_error(y_pred, y_train)\n",
    "print(f\"The empirical error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the same phenomena than in the previous experiment. Our model\n",
    "memorized the training set. However, we can now compute the generalization\n",
    "error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "score = mean_absolute_error(y_pred, y_test)\n",
    "print(f\"The generalization error of our model is {score:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization error is not minimum and equal to zero. Indeed, this\n",
    "error is closer to the performance of our model if it was deployed in\n",
    "production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of the cross-validation estimates\n",
    "\n",
    "When doing a single train-test split we don't give any indication\n",
    "regarding the robustness of the evaluation of our predictive model: in\n",
    "particular, if the test set is small, this estimate of the generalization\n",
    "error can be unstable and do not reflect the \"true error rate\" we would have\n",
    "observed with the same model on an unlimited amount of test data.\n",
    "\n",
    "For instance, we could have been lucky when we did our random split of our\n",
    "limited dataset and isolated some of the easiest cases to predict in the\n",
    "testing set just by chance: the estimation of the generalization error would\n",
    "be overly optimistic, in this case.\n",
    "\n",
    "**Cross-validation** allows estimating the robustness of a predictive model\n",
    "by repeating the splitting procedure. It will give several empirical and\n",
    "generalization errors and thus some **estimate of the variability of the\n",
    "model performance**.\n",
    "\n",
    "There are different cross-validation strategies, for now we are going to\n",
    "focus on one called \"shuffle-split\". At each iteration of this strategy we:\n",
    "\n",
    "- shuffle the order of the samples of a copy of the full data at random;\n",
    "- split the shuffled dataset into a train and a test set;\n",
    "- train a new model on the train set;\n",
    "- evaluate the generalization error on the test set.\n",
    "\n",
    "We repeat this procedure `n_splits` times. Using `n_splits=30` means that we\n",
    "will train 30 models in total and all of them will be discarded: we just\n",
    "record their performance on each variant of the test set.\n",
    "\n",
    "To evaluate the performance of our regressor, we can use `cross_validate`\n",
    "with a `ShuffleSplit` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "cv_results = cross_validate(\n",
    "    regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results `cv_results` are stored into a Python dictionary. We will convert\n",
    "it into a pandas dataframe to ease visualization and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "By convention, scikit-learn model evaluation tools always use a convention\n",
    "where \"higher is better\", this explains we used\n",
    "`scoring=\"neg_mean_absolute_error\"` (meaning \"negative mean absolute error\").\n",
    "```\n",
    "\n",
    "Let us revert the negation to get the actual error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_error\"] = -cv_results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results reported by the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get timing information to fit and predict at each round of\n",
    "cross-validation. Also, we get the test score, which corresponds to the\n",
    "generalization error on each of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 30 entries in our resulting dataframe because we performed 30 splits.\n",
    "Therefore, we can show the generalization error distribution and thus, have\n",
    "an estimate of its variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sns.displot(cv_results[\"test_error\"], kde=True, bins=10)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the generalization error is clustered around 45.5 k\\$ and\n",
    "ranges from 43 k\\$ to 49 k\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validated generalization error is: \"\n",
    "      f\"{cv_results['test_error'].mean():.2f} k$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The standard deviation of the generalization error is: \"\n",
    "      f\"{cv_results['test_error'].std():.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standard deviation is much smaller than the mean: we could\n",
    "summarize that our cross-validation estimate of the generalization error is\n",
    "45.7 +/- 1.1 k\\$.\n",
    "\n",
    "If we were to train a single model on the full dataset (without\n",
    "cross-validation) and then had later access to an unlimited amount of test\n",
    "data, we would expect its true generalization error to fall close to that\n",
    "region.\n",
    "\n",
    "While this information is interesting in it-self, this should be contrasted\n",
    "to the scale of the natural variability of the target `y` in our dataset.\n",
    "\n",
    "Let us plot the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(y, kde=True, bins=20)\n",
    "_ = plt.xlabel(\"Median House Value (k$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The standard deviation of the target is: {y.std():.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable ranges from close to 0 k\\$ up to 500 k\\$ and, with a\n",
    "standard deviation around 115 k\\$.\n",
    "\n",
    "We notice that the mean estimate of the generalization error obtained by\n",
    "cross-validation is a bit than the natural scale of variation of the target\n",
    "variable. Furthermore the standard deviation of the cross validation estimate\n",
    "of the generalization error is even much smaller.\n",
    "\n",
    "This is a good start, but not necessarily enough to decide whether the\n",
    "generalization performance is good enough to make our prediction useful in\n",
    "practice.\n",
    "\n",
    "We recall that our model makes, on average, an error around 45 k\\$. With this\n",
    "information and looking at the target distribution, such an error might be\n",
    "acceptable when predicting houses with a 500 k\\$. However, it would be an\n",
    "issue with a house with a value of 50 k\\$. Thus, this indicates that our\n",
    "metric (Mean Absolute Error) is not ideal.\n",
    "\n",
    "We might instead choose a metric relative to the target value to predict: the\n",
    "mean absolute percentage error would have been a much better choice.\n",
    "\n",
    "But in all cases, an error of 45 k\\$ might be too large to automatically use\n",
    "our model to tag house value without expert supervision.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we saw:\n",
    "\n",
    "* the necessity of splitting the data into a train and test set;\n",
    "* the meaning of the empirical and generalization errors;\n",
    "* the overall cross-validation framework with the possibility to study\n",
    "  performance variations;"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
