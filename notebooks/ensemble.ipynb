{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning: when many are better that the one\n",
    "\n",
    "In this notebook, we will go in depth into algorithms which combine several\n",
    "simple learners (e.g. decision tree, linear model, etc.). We will\n",
    "see that combining simple learners will result in a more powerful and robust\n",
    "learner.\n",
    "We will focus on two families of ensemble methods:\n",
    "\n",
    "* ensemble using bootstrap (e.g. bagging and random-forest);\n",
    "* ensemble using boosting (e.g. adaptive boosting and gradient-boosting\n",
    "  decision tree).\n",
    "\n",
    "## Benefit of ensemble method at a first glance\n",
    "\n",
    "In this section, we will give a quick demonstration on the power of combining\n",
    "several learners instead of fine-tuning a single learner.\n",
    "\n",
    "We will start by loading the \"California Housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "df = california_housing.frame\n",
    "X, y = california_housing.data, california_housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we want to predict the median house value in some district\n",
    "in California based on demographic and geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by learning a single decision tree regressor. As we previously\n",
    "presented in the \"tree in depth\" notebook, this learner needs to be tuned to\n",
    "overcome over- or under-fitting. Indeed, the default parameters will not\n",
    "necessarily lead to an optimal decision tree. Instead of using the default\n",
    "value, we should search via cross-validation the optimal value of the\n",
    "important parameters such as `max_depth`, `min_samples_split`, or\n",
    "`min_samples_leaf`.\n",
    "\n",
    "We recall that we need to tune these parameters, as decision trees\n",
    "tend to overfit the training data if we grow deep trees, but there are \n",
    "no rules on what each parameter should be set to. \n",
    "Thus, not making a search could lead us\n",
    "to have an underfitted or overfitted model.\n",
    "\n",
    "\n",
    "First, let's keep a set of data to test our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first make a grid-search to fine-tune the parameters that we\n",
    "mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 8, None],\n",
    "    \"min_samples_split\": [2, 10, 30, 50],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 1],\n",
    "}\n",
    "cv = 3\n",
    "tree = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dataframe storing the important information collected during\n",
    "the tuning of the parameters and investigate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(tree.cv_results_)\n",
    "interesting_columns = [\n",
    "    \"param_max_depth\",\n",
    "    \"param_min_samples_split\",\n",
    "    \"param_min_samples_leaf\",\n",
    "    \"mean_test_score\",\n",
    "    \"rank_test_score\",\n",
    "    \"mean_fit_time\",\n",
    "]\n",
    "cv_results = cv_results[interesting_columns].sort_values(by=\"rank_test_score\")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From theses results, we can see that the best parameters is the combination\n",
    "where the depth of the tree is not limited and the minimum number of samples\n",
    "to create a leaf is also equal to 1 (the default values) and the\n",
    "minimum number of samples to make a split of 50 (much higher than the default\n",
    "value.\n",
    "\n",
    "It is interesting to look at the total amount of time it took to fit all\n",
    "these different models. In addition, we can check the performance of the\n",
    "optimal decision tree on the left-out testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fitting_time = (cv_results[\"mean_fit_time\"] * cv).sum()\n",
    "print(\n",
    "    f\"Required training time of the GridSearchCV: \"\n",
    "    f\"{total_fitting_time:.2f} seconds\"\n",
    ")\n",
    "print(\n",
    "    f\"Best R2 score of a single tree: {tree.best_score_:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have a model that has an $R^2$ score below 0.7. The amount of time\n",
    "to find the best learner depends on the number of folds used during the\n",
    "cross-validation in the grid-search multiplied by the number of parameters.\n",
    "Therefore, the computational cost is quite high.\n",
    "\n",
    "Now we will use an ensemble method called bagging. More details about this\n",
    "method will be discussed in the next section. In short, this method will use\n",
    "a base regressor (i.e. decision tree regressors) and will train several of\n",
    "them on a slightly modified version of the training set. Then, the\n",
    "predictions of all these base regressors will be combined by averaging.\n",
    "\n",
    "Here, we will use 50 decision trees and check the fitting time as well as\n",
    "the performance on the left-out testing data. It is important to note that\n",
    "we are not going to tune any parameter of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=base_estimator, n_estimators=50, random_state=0,\n",
    ")\n",
    "\n",
    "start_fitting_time = time()\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "elapsed_fitting_time = time() - start_fitting_time\n",
    "\n",
    "print(f\"Elapsed fitting time: {elapsed_fitting_time:.2f} seconds\")\n",
    "print(f\"R2 score: {bagging_regressor.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the computation time is much shorter for training the full\n",
    "ensemble than for the parameter search of a single tree. In addition, the\n",
    "score is significantly improved with a $R^2$ close to 0.8. Furthermore, note\n",
    "that this result is obtained before any parameter tuning. This shows the\n",
    "motivation behind the use of an ensemble learner: it gives a relatively good\n",
    "baseline with decent performance without any parameter tuning.\n",
    "\n",
    "Now, we will discuss in detail two ensemble families: bagging and\n",
    "boosting.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Bagging stands for Bootstrap AGGregatING. It uses bootstrap \n",
    "(random sampling \n",
    "with replacement) to learn several models.\n",
    "At predict time, the predictions of each learner\n",
    "are aggregated to give the final predictions.\n",
    "\n",
    "Let's define a simple dataset (which we have used before in a previous\n",
    "notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=50, sorted=False):\n",
    "    x_max, x_min = 1.4, -1.4\n",
    "    len_x = x_max - x_min\n",
    "    x = rng.rand(n_samples) * len_x - len_x / 2\n",
    "    noise = rng.randn(n_samples) * 0.3\n",
    "    y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "    if sorted:\n",
    "        sorted_idx = np.argsort(x)\n",
    "        x, y = x[sorted_idx], y[sorted_idx]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = generate_data(n_samples=50)\n",
    "\n",
    "plt.scatter(x, y,  color='k', s=9)\n",
    "plt.xlabel(\"Feature\")\n",
    "_ = plt.ylabel(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link between our feature and the target to predict is non-linear.\n",
    "However, a decision tree is capable of fitting such data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "grid = np.linspace(np.min(x), np.max(x), num=300)\n",
    "y_pred = tree.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.plot(grid, y_pred, label=\"Tree fitting\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Let's see how we can use bootstraping to learn several trees.\n",
    "\n",
    "### Bootstrap sample\n",
    "\n",
    "A bootstrap sample corresponds to a resampling, with replacement, of the\n",
    "original dataset, a sample that is the same size as the\n",
    "original dataset. Thus, the bootstrap sample will contain some\n",
    "data points several times while some of the original data points will\n",
    "not be present.\n",
    "\n",
    "We will create a function that given `x` and `y` will return a bootstrap\n",
    "sample `x_bootstrap` and `y_bootstrap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(x, y):\n",
    "    bootstrap_indices = rng.choice(\n",
    "        np.arange(y.shape[0]), size=y.shape[0], replace=True,\n",
    "    )\n",
    "    x_bootstrap_sample = x[bootstrap_indices]\n",
    "    y_bootstrap_sample = y[bootstrap_indices]\n",
    "    return x_bootstrap_sample, y_bootstrap_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate 3 bootstrap samples and qualitatively check the difference\n",
    "with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 3\n",
    "_, axs = plt.subplots(\n",
    "    ncols=n_bootstrap, figsize=(16, 6), sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "for idx, (ax, _) in enumerate(zip(axs, range(n_bootstrap))):\n",
    "    x_bootstrap_sample, y_bootstrap_sample = bootstrap_sample(x, y)\n",
    "    ax.scatter(\n",
    "        x_bootstrap_sample, y_bootstrap_sample,\n",
    "    )\n",
    "    ax.set_title(f\"Bootstrap sample #{idx}\")\n",
    "    ax.set_ylabel(\"Target\")\n",
    "    ax.set_xlabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the 3 generated bootstrap samples are all different. To\n",
    "confirm this intuition, we can check the number of unique samples in the\n",
    "bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to generate a larger set to have a good estimate\n",
    "x_huge, y_huge = generate_data(n_samples=10000)\n",
    "x_bootstrap_sample, y_bootstrap_sample = bootstrap_sample(x_huge, y_huge)\n",
    "\n",
    "print(\n",
    "    f\"Percentage of samples present in the original dataset: \"\n",
    "    f\"{np.unique(x_bootstrap_sample).size / x_bootstrap_sample.size * 100:.1f}\"\n",
    "    f\"%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, 63.2% of the original data points of the original dataset will\n",
    "be present in the bootstrap sample. The other 36.8% are just repeated\n",
    "samples.\n",
    "\n",
    "So, we are able to generate many datasets, all slightly different. Now, we\n",
    "can fit a decision tree to each of these datasets and each decision\n",
    "tree shall be slightly different as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(\n",
    "    ncols=n_bootstrap, figsize=(16, 6), sharex=True, sharey=True,\n",
    ")\n",
    "\n",
    "forest = []\n",
    "for idx, (ax, _) in enumerate(zip(axs, range(n_bootstrap))):\n",
    "    x_bootstrap_sample, y_bootstrap_sample = bootstrap_sample(x, y)\n",
    "    ax.scatter(x_bootstrap_sample, y_bootstrap_sample)\n",
    "\n",
    "    forest.append(\n",
    "        DecisionTreeRegressor(max_depth=3, random_state=0).fit(\n",
    "            x_bootstrap_sample.reshape(-1, 1), y_bootstrap_sample\n",
    "        )\n",
    "    )\n",
    "\n",
    "    grid = np.linspace(np.min(x), np.max(x), num=300)\n",
    "    y_pred = forest[-1].predict(grid.reshape(-1, 1))\n",
    "    ax.plot(grid, y_pred, linewidth=3, label=\"Fitted tree\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"Target\")\n",
    "    ax.set_xlabel(\"Features\")\n",
    "    ax.set_title(f\"Bootstrap #{idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these decision functions on the same plot to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(x, y, color=\"k\", alpha=0.4)\n",
    "y_pred_forest = []\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    y_pred = tree.predict(grid.reshape(-1, 1))\n",
    "    ax.plot(\n",
    "        grid,\n",
    "        y_pred,\n",
    "        \"--\",\n",
    "        label=f\"Tree #{tree_idx} predictions\",\n",
    "        linewidth=3,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    y_pred_forest.append(y_pred)\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating\n",
    "\n",
    "Once our trees are fitted and we are able to get predictions for each of\n",
    "them, we also need to combine them. In regression, the most straightforward\n",
    "approach is to average the different predictions from all learners. We can\n",
    "plot the averaged predictions from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(x, y, color=\"k\", alpha=0.4)\n",
    "y_pred_forest = []\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    y_pred = tree.predict(grid.reshape(-1, 1))\n",
    "    ax.plot(\n",
    "        grid,\n",
    "        y_pred,\n",
    "        \"--\",\n",
    "        label=f\"Tree #{tree_idx} predictions\",\n",
    "        linewidth=3,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    y_pred_forest.append(y_pred)\n",
    "\n",
    "# Averaging the predictions\n",
    "y_pred_forest = np.mean(y_pred_forest, axis=0)\n",
    "ax.plot(\n",
    "    grid,\n",
    "    y_pred_forest,\n",
    "    \"-\",\n",
    "    label=\"Averaged predictions\",\n",
    "    linewidth=3,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unbroken red line shows the averaged predictions, which would be the\n",
    "final preditions given by our 'bag' of decision tree regressors.\n",
    "\n",
    "## Random forest\n",
    "\n",
    "A popular machine-learning algorithm is the random forest. A Random forest\n",
    "is a modification of the bagging algorithm. In bagging, any classifier or\n",
    "regressor can be used. In a random forest, the base classifier or regressor\n",
    "must be a decision tree. In our previous example, we used a decision\n",
    "tree but we could have used a linear model as the regressor for our\n",
    "bagging algorithm.\n",
    "\n",
    "In addition, random forest is different from bagging when used with\n",
    "classifiers: when searching for the best split, only a subset of the original\n",
    "features are used. By default, this subset of feature is equal to the square\n",
    "root of the total number of features. In regression, the total number of\n",
    "available features will be used.\n",
    "\n",
    "We will illustrate the usage of a random forest and compare it with the\n",
    "bagging regressor on the \"California housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = california_housing.data, california_housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "    n_estimators=100, random_state=0, n_jobs=-1\n",
    ")\n",
    "bagging = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(random_state=0),\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    f\"Performance of random forest: {random_forest.score(X_test, y_test):.3f}\"\n",
    ")\n",
    "print(f\"Performance of bagging: {bagging.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we don't need to provide a `base_estimator` parameter to\n",
    "`RandomForestRegressor`, it is always a tree classifier. Also note that the\n",
    "scores are almost identical. This is because our\n",
    "problem is a regression problem and therefore, the number of features used\n",
    "in random forest and bagging is the same.\n",
    "\n",
    "For classification problems, we would need to pass a tree model instance\n",
    "with the parameter `max_features=\"sqrt\"` to `BaggingRegressor` if we wanted\n",
    "it to have the same behaviour as the random forest classifier.\n",
    "\n",
    "### Classifiers details\n",
    "\n",
    "Up to now, we have only focused on regression problems. There is a little\n",
    "difference between regression and classification.\n",
    "\n",
    "First, the `base_estimator` should be chosen in line with the problem that\n",
    "is solved: use a classifier with a classification problem and a regressor\n",
    "with a regression problem.\n",
    "\n",
    "Then, the aggregation method is different in regression and classification:\n",
    "- in regression, the average prediction is computed. For instance, if \n",
    "three learners predict 0.4, 0.3 and 0.31, the aggregation will output 0.33,\n",
    "- while in classification, the class which highest probability\n",
    "(after averaging the predicted probabilities) is predicted. For instance, if\n",
    "three learners predict (for two classes) the probability (0.4, 0.6),\n",
    "(0.3, 0.7) and (0.31, 0.69), the aggregation probability is (0.33, 0.67)\n",
    "and the second class would be predicted.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We saw in this section two algorithms that use bootstrap samples to create\n",
    "an ensemble of classifiers or regressors. These algorithms train several\n",
    "learners on different bootstrap samples. The predictions are then\n",
    "aggregated. This operation can be done in a very efficient manner since the\n",
    "training of each learner can be done in parallel.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "We recall that bagging builds an ensemble in a parallel manner: each learner\n",
    "is trained independently from each other. The idea behind boosting is\n",
    "different. The ensemble is a sequence of learners where the\n",
    "`Nth` learner requires all previous learners, from 1 to `N-1`.\n",
    "\n",
    "Intuitively, bagging adds learners to the ensemble to correct the\n",
    "mistakes of the previous learners. We will start with an\n",
    "algorithm named Adaptive Boosting (AdaBoost) to get some intuition about the\n",
    "main ideas behind boosting.\n",
    "\n",
    "### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "We will first focus on AdaBoost, which we will use for a classification\n",
    "problem.\n",
    "We will load the \"penguin\" dataset used in the \"tree in depth\" notebook.\n",
    "We will predict penguin species from the features culmen length and depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "# select the features of interest\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data = data[culmen_columns + [target_column]]\n",
    "data[target_column] = data[target_column].str.split().str[0]\n",
    "data = data.dropna()\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we are also using on the function used the previous\n",
    "\"tree in depth\" notebook\n",
    "to plot the decision function of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_decision_function(X, y, clf, sample_weight=None, fit=True, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    if fit:\n",
    "        clf.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    feature_0_min, feature_0_max = (\n",
    "        X.iloc[:, 0].min() - 1,\n",
    "        X.iloc[:, 0].max() + 1,\n",
    "    )\n",
    "    feature_1_min, feature_1_max = (\n",
    "        X.iloc[:, 1].min() - 1,\n",
    "        X.iloc[:, 1].max() + 1,\n",
    "    )\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(feature_0_min, feature_0_max, plot_step),\n",
    "        np.arange(feature_1_min, feature_1_max, plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    sns.scatterplot(\n",
    "        data=pd.concat([X, y], axis=1),\n",
    "        x=X.columns[0],\n",
    "        y=X.columns[1],\n",
    "        hue=y.name,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We will purposely train a shallow decision tree. Since the tree is shallow,\n",
    "it is unlikely to overfit and some of the training examples will even be\n",
    "misclassified on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "plot_decision_function(X, y, tree, ax=ax)\n",
    "\n",
    "# find the misclassified samples\n",
    "y_pred = tree.predict(X)\n",
    "misclassified_samples_idx = np.flatnonzero(y != y_pred)\n",
    "\n",
    "ax.plot(\n",
    "    X.iloc[misclassified_samples_idx, 0],\n",
    "    X.iloc[misclassified_samples_idx, 1],\n",
    "    \"+k\",\n",
    "    label=\"Misclassified samples\",\n",
    ")\n",
    "ax.legend(fontsize = 'small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that several samples have been misclassified by the\n",
    "classifier.\n",
    "\n",
    "We mentioned that boosting relies on creating a new classifier which tries to\n",
    "correct these misclassifications. In scikit-learn, learners support a\n",
    "parameter `sample_weight` which forces the learner to pay more attention to\n",
    "samples with higher weights, during the training.\n",
    "\n",
    "This parameter is set when calling\n",
    "`classifier.fit(X, y, sample_weight=weights)`.\n",
    "We will use this trick to create a new classifier by 'discarding' all\n",
    "correctly classified samples and only considering the misclassified samples.\n",
    "Thus, mosclassified samples will be assigned a weight of 1 while well\n",
    "classified samples will assigned to a weight of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.zeros_like(y, dtype=int)\n",
    "sample_weight[misclassified_samples_idx] = 1\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "plot_decision_function(X, y, tree, sample_weight=sample_weight, ax=ax)\n",
    "\n",
    "ax.plot(\n",
    "    X.iloc[misclassified_samples_idx, 0],\n",
    "    X.iloc[misclassified_samples_idx, 1],\n",
    "    \"+k\",\n",
    "    label=\"Previous misclassified samples\",\n",
    ")\n",
    "ax.legend(fontsize = 'small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the decision function drastically changed. Qualitatively,\n",
    "we see that the previously misclassified samples are now correctly\n",
    "classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X)\n",
    "newly_misclassified_samples_idx = np.flatnonzero(y != y_pred)\n",
    "remaining_misclassified_samples_idx = np.intersect1d(\n",
    "    misclassified_samples_idx, newly_misclassified_samples_idx\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of samples previously misclassified and still misclassified: \"\n",
    "    f\"{len(remaining_misclassified_samples_idx)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are making mistakes on previously well classified samples. Thus,\n",
    "we get the intuition that we should weight the predictions of each classifier\n",
    "differently, most probably by using the number of mistakes each classifier\n",
    "is making.\n",
    "\n",
    "So we could use the classification error to combine both trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_weight = [\n",
    "    (y.shape[0] - len(misclassified_samples_idx)) / y.shape[0],\n",
    "    (y.shape[0] - len(newly_misclassified_samples_idx)) / y.shape[0],\n",
    "]\n",
    "ensemble_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier was 94% accurate and the second one 69% accurate.\n",
    "Therefore, when predicting a class, we should trust the first classifier\n",
    "slightly more than the second one. We could use these accuracy values to\n",
    "weight the predictions of each learner.\n",
    "\n",
    "To summarize, boosting learns several classifiers, each of which will\n",
    "focus more or less on specific samples of the dataset. Boosting is thus\n",
    "different from bagging: here we never resample our dataset, we just assign\n",
    "different weights to the original dataset.\n",
    "\n",
    "Boosting requires some strategy to combine the learners together:\n",
    "\n",
    "* one needs to define a way to compute the weights to be assigned\n",
    "  to samples;\n",
    "* one needs to assign a weight to each learner when making predictions.\n",
    "\n",
    "Indeed, we defined a really simple scheme to assign sample weights and\n",
    "learner weights. However, there are statistical theories (like in AdaBoost)\n",
    "for how these sample and learner weights can be optimally calculated.\n",
    "FIXME: I think we should add a reference to ESL here.\n",
    "\n",
    "We will use the AdaBoost classifier implemented in scikit-learn and\n",
    "look at the underlying decision tree classifiers trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3, random_state=0),\n",
    "    n_estimators=3,\n",
    "    algorithm=\"SAMME\",\n",
    "    random_state=0,\n",
    ")\n",
    "adaboost.fit(X, y)\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, figsize=(16, 6))\n",
    "\n",
    "for ax, tree in zip(axs, adaboost.estimators_):\n",
    "    plot_decision_function(X, y, tree, fit=False, ax=ax)\n",
    "\n",
    "print(f\"Weight of each classifier: {adaboost.estimator_weights_}\")\n",
    "print(f\"Error of each classifier: {adaboost.estimator_errors_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that AdaBoost has learnt three different classifiers each of which\n",
    "focuses on different samples. Looking at the weights of each learner, we see\n",
    "that the ensemble gives the highest weight to the first classifier. This\n",
    "indeed makes sense when we look at the errors of each classifier. The first\n",
    "classifier also has the highest classification performance.\n",
    "\n",
    "While AdaBoost is a nice algorithm to demonsrate the internal machinery of\n",
    "boosting\n",
    "algorithms, it is not the most efficient machine-learning algorithm.\n",
    "The most efficient algorithm based on boosting is the gradient-boosting\n",
    "decision tree (GBDT) algorithm which we will discuss now.\n",
    "\n",
    "### Gradient-boosting decision tree (GBDT)\n",
    "\n",
    "Gradient-boosting differs from AdaBoost due to the following reason: instead\n",
    "of assigning weights to specific samples, GBDT will fit a decision\n",
    "tree on the residuals error (hence the name \"gradient\") of the previous tree.\n",
    "Therefore, each new added tree in the ensemble predicts the error made by the\n",
    "previous learner instead of predicting the target directly.\n",
    "\n",
    "In this section, we will provide some intuition about the way learners\n",
    "are combined to give the final prediction. In this regard, let's go back\n",
    "to our regression problem which is more intuitive for demonstrating the\n",
    "underlying machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data(sorted=True)\n",
    "\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"Feature\")\n",
    "_ = plt.ylabel(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously discussed, boosting will be based on assembling a sequence\n",
    "of learners. We will start by creating a decision tree regressor. We will fix\n",
    "the depth of the tree so that the resulting learner will underfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "grid = np.linspace(np.min(x), np.max(x), num=200)\n",
    "y_pred_grid_raw = tree.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "line_predictions = plt.plot(grid, y_pred_grid_raw, \"--\")\n",
    "\n",
    "y_pred_raw = tree.predict(x.reshape(-1, 1))\n",
    "for idx in range(len(y)):\n",
    "    lines_residuals = plt.plot(\n",
    "        [x[idx], x[idx]], [y[idx], y_pred_raw[idx]], color=\"red\",\n",
    "    )\n",
    "\n",
    "_ = plt.legend(\n",
    "    [line_predictions[0], lines_residuals[0]], [\"Fitted tree\", \"Residuals\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tree underfits the data, its accuracy is far\n",
    "from perfect on the training data. We can observe this in the figure by\n",
    "looking at the difference between the predictions and the ground-truth data.\n",
    "We represent these errors, called \"Residuals\", by unbroken red lines.\n",
    "\n",
    "Indeed, our initial tree was not expressive enough to handle the complexity\n",
    "of the data, as shown by the residuals.\n",
    "In a gradient-boosting algorithm, the idea is to create a second tree\n",
    "which, given the same data `x`, will try to predict the residuals instead of\n",
    "the target `y`. We would therefore have a tree that is able to predict the\n",
    "errors made by the initial tree.\n",
    "\n",
    "Let's train such a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - y_pred_raw\n",
    "\n",
    "tree_residuals = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "tree_residuals.fit(x.reshape(-1, 1), residuals)\n",
    "\n",
    "y_pred_grid_residuals = tree_residuals.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(x, residuals, color=\"k\", s=9)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "line_predictions = plt.plot(grid, y_pred_grid_residuals, \"--\")\n",
    "\n",
    "y_pred_residuals = tree_residuals.predict(x.reshape(-1, 1))\n",
    "for idx in range(len(y)):\n",
    "    lines_residuals = plt.plot(\n",
    "        [x[idx], x[idx]], [residuals[idx], y_pred_residuals[idx]], color=\"red\",\n",
    "    )\n",
    "\n",
    "_ = plt.legend(\n",
    "    [line_predictions[0], lines_residuals[0]], [\"Fitted tree\", \"Residuals\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this new tree only manages to fit some of the residuals.\n",
    "We will focus on the last sample in `x` and\n",
    "explain how the predictions of both trees are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "axs[0].scatter(x, y, color=\"k\", s=9)\n",
    "axs[0].set_xlabel(\"Feature\")\n",
    "axs[0].set_ylabel(\"Target\")\n",
    "axs[0].plot(grid, y_pred_grid_raw, \"--\")\n",
    "\n",
    "axs[1].scatter(x, residuals, color=\"k\", s=9)\n",
    "axs[1].set_xlabel(\"Feature\")\n",
    "axs[1].set_ylabel(\"Residuals\")\n",
    "plt.plot(grid, y_pred_grid_residuals, \"--\")\n",
    "\n",
    "for idx in range(len(y)):\n",
    "    axs[0].plot(\n",
    "        [x[idx], x[idx]], [y[idx], y_pred_raw[idx]], color=\"red\",\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        [x[idx], x[idx]], [residuals[idx], y_pred_residuals[idx]], color=\"red\",\n",
    "    )\n",
    "\n",
    "axs[0].scatter(x[-1], y[-1], color=\"tab:orange\", label=\"Sample of interest\")\n",
    "axs[1].scatter(\n",
    "    x[-1], residuals[-1], color=\"tab:orange\", label=\"Sample of interest\"\n",
    ")\n",
    "\n",
    "axs[0].set_xlim([1.1, 1.4])\n",
    "axs[1].set_xlim([1.1, 1.4])\n",
    "axs[0].set_ylim([0, 2])\n",
    "axs[1].set_ylim([-0.7, 0.7])\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sample of interest, our initial tree is making an error (small\n",
    "residual). When\n",
    "fitting the second tree, the residual in this case is perfectly fitted and\n",
    "predicted. We will quantitatively check this prediction using the fitted\n",
    "tree. First, let's check the prediction of the initial tree and compare it\n",
    "with the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = x[-1]\n",
    "y_true = y[-1]\n",
    "\n",
    "print(f\"True value to predict for f(x={x_max:.3f}) = {y_true:.3f}\")\n",
    "\n",
    "y_pred_first_tree = tree.predict([[x_max]])[0]\n",
    "print(\n",
    "    f\"Prediction of the first decision tree for x={x_max:.3f}: \"\n",
    "    f\"y={y_pred_first_tree:.3f}\"\n",
    ")\n",
    "print(f\"Error of the tree: {y_true - y_pred_first_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we visually observed, we have a small error. Now, we can use the second\n",
    "tree to try to predict this residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Prediction of the residual for x={x_max:.3f}: \"\n",
    "    f\"{tree_residuals.predict([[x_max]])[0]:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our second tree is capable of prediting the exact residual\n",
    "(error) of our first tree. Therefore, we can predict the value of\n",
    "`x` by summing the prediction of the all trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_first_and_second_tree = (\n",
    "    y_pred_first_tree + tree_residuals.predict([[x_max]])[0]\n",
    ")\n",
    "print(\n",
    "    f\"Prediction of the first and second decision trees combined for \"\n",
    "    f\"x={x_max:.3f}: y={y_pred_first_and_second_tree:.3f}\"\n",
    ")\n",
    "print(f\"Error of the tree: {y_true - y_pred_first_and_second_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose a sample for which only two trees were enough to make the perfect\n",
    "prediction. However, we saw in the previous plot that two trees were not\n",
    "enough to correct the residuals of all samples. Therefore, one needs to\n",
    "add several trees to the ensemble to successfully correct the error.\n",
    "(i.e. the second tree corrects the first tree's error, while the third tree\n",
    "corrects the second tree's error and so on.)\n",
    "\n",
    "We will compare the performance of random-forest and gradient boosting on\n",
    "the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = california_housing.data, california_housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,)\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "start_time = time()\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "fit_time_gradient_boosting = time() - start_time\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "start_time = time()\n",
    "random_forest.fit(X_train, y_train)\n",
    "fit_time_random_forest = time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"The performance of gradient-boosting are:\"\n",
    "    f\"{gradient_boosting.score(X_test, y_test):.3f}\"\n",
    ")\n",
    "print(f\"Fitting time took: {fit_time_gradient_boosting:.2f} seconds\")\n",
    "\n",
    "print(\n",
    "    f\"The performance of random-forest are:\"\n",
    "    f\"{random_forest.score(X_test, y_test):.3f}\"\n",
    ")\n",
    "print(f\"Fitting time took: {fit_time_random_forest:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In term of computation performance, the forest can be parallelized and will\n",
    "benefit from the having multiple CPUs. In terms of scoring performance, both\n",
    "algorithms lead to very close results.\n",
    "\n",
    "## Parameter consideration with random forest and gradient-boosting\n",
    "\n",
    "In the previous section, we did not discuss the parameters of random forest\n",
    "and gradient-boosting. However, there are a couple of things to keep in mind\n",
    "when setting these parameters.\n",
    "\n",
    "### Random forest\n",
    "\n",
    "The main parameter to tune with random forest is the `n_estimators`\n",
    "parameter. In general, the more trees in the forest, the better the\n",
    "performance will be. However, it will slow down the fitting and prediction\n",
    "time. So one has to balance compute time and performance when setting the\n",
    "number of estimators when putting such learner in production.\n",
    "\n",
    "The `max_depth` parameter could also be tuned. Sometimes, there is no need\n",
    "to have fully grown trees. However, be aware that with random forest, trees\n",
    "are generally deep since we are seeking to overfit the learners on the\n",
    "bootstrap samples because this will be mitigated by combining them.\n",
    "Assembling underfitted trees (i.e. shallow trees) might also lead to an\n",
    "underfitted forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [10, 20, 30],\n",
    "    \"max_depth\": [3, 5, None],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(n_jobs=-1), param_grid=param_grid, n_jobs=-1,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "columns = [\"params\", \"mean_test_score\", \"rank_test_score\"]\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results[columns].sort_values(by=\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in our grid-search, the largest `max_depth` together with\n",
    "largest `n_estimators` led to the best performance.\n",
    "\n",
    "### Gradient-boosting decision tree\n",
    "\n",
    "For gradient-boosting, parameters are coupled, so we can not anymore\n",
    "set the parameters one after the other. The\n",
    "important parameters are `n_estimators`, `max_depth`, and `learning_rate`.\n",
    "\n",
    "Let's first discuss the `max_depth` parameter. We saw in the section on\n",
    "gradient-boosting that the algorithm fits the error of the previous tree\n",
    "in the ensemble. Thus, fitting fully grown trees will be detrimental. Indeed,\n",
    "the first tree of the ensemble would perfectly fit (overfit) the data and\n",
    "thus no subsequent tree would be required, since there would be no residuals.\n",
    "Therefore, the tree used in gradient-boosting should have a low depth,\n",
    "typically between 3 to 8 levels.\n",
    "Having very weak learners at each step will help reducing overfitting.\n",
    "\n",
    "\n",
    "With this consideration in mind, the deeper the trees, the faster the\n",
    "residuals will be corrected and less learners are required. So `n_estimators`\n",
    "should be increased if `max_depth` is lower.\n",
    "\n",
    "Finally, we have overlooked the impact of the `learning_rate` parameter up\n",
    "till now. When\n",
    "fitting the residuals one could choose if the tree should try to correct all\n",
    "possible errors or only a fraction of them. The learning-rate allows you to\n",
    "control this behaviour. A small learning-rate value would only correct the\n",
    "residuals of very few samples. If a large learning-rate is set (e.g., 1),\n",
    "we would fit the residuals of all samples. So, with a very low\n",
    "learning-rate, we will need more estimators to correct the overall error.\n",
    "However, a too large learning-rate tends to obtain an overfitted ensemble,\n",
    "similar to having a too large tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [10, 30, 50],\n",
    "    \"max_depth\": [3, 5, None],\n",
    "    \"learning_rate\": [0.1, 1],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(), param_grid=param_grid, n_jobs=-1,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results[columns].sort_values(by=\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerating gradient-boosting\n",
    "\n",
    "We previously mentioned that random-forest is an efficient algorithm since\n",
    "each tree of the ensemble can be fitted at the same time independently.\n",
    "Therefore, the algorithm scales efficiently with both the number of CPUs and\n",
    "the number of samples.\n",
    "\n",
    "In gradient-boosting, the algorithm is a sequential algorithm. It requires\n",
    "the `N-1` trees to have been fit to be able to fit the tree at stage `N`.\n",
    "Therefore, the algorithm is\n",
    "quite computationally expensive. The most expensive part in this algorithm is\n",
    "the search for the best split in the tree which is a brute-force\n",
    "approach: all possible split are evaluated and the best one is picked. We\n",
    "explained this process in the notebook \"tree in depth\", which\n",
    "you can refer to.\n",
    "\n",
    "To accelerate the gradient-boosting algorithm, one could reduce the number of\n",
    "splits to be evaluated. As a consequence, the performance of such a\n",
    "tree would be reduced. However, since we are combining several trees in a\n",
    "gradient-boosting, we can add more estimators to overcome\n",
    "this issue.\n",
    "\n",
    "This algorithm is called `HistGradientBoostingClassifier` and\n",
    "`HistGradientBoostingRegressor`. Each feature in the dataset `X` is first\n",
    "binned by computing histograms which are later used to evaluate the potential\n",
    "splits. The number\n",
    "of splits to evaluate is then much smaller. This algorithm becomes much more\n",
    "efficient than gradient boosting when the dataset has 10,000+ samples.\n",
    "\n",
    "Below we will give an example of a large dataset and we can compare\n",
    "computation time with the earlier experiment in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "histogram_gradient_boosting = HistGradientBoostingRegressor(\n",
    "    max_iter=200, random_state=0,\n",
    ")\n",
    "start_time = time()\n",
    "histogram_gradient_boosting.fit(X_train, y_train)\n",
    "fit_time_histogram_gradient_boosting = time() - start_time\n",
    "\n",
    "print(\n",
    "    f\"The performance of histogram gradient-boosting are:\"\n",
    "    f\"{histogram_gradient_boosting.score(X_test, y_test):.3f}\"\n",
    ")\n",
    "print(f\"Fitting time took: {fit_time_histogram_gradient_boosting:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram gradient-boosting is the best algorithm in terms of score.\n",
    "It will also scale when the number of samples increases, while the normal\n",
    "gradient-boosting will not.\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "So in this notebook we discussed ensemble learners which are a type of\n",
    "learner that combines simpler learners together. We saw two strategies:\n",
    "one based on bootstrap samples allowing learners to be fit in a parallel\n",
    "manner and the other called boosting which fit learners in a sequential\n",
    "manner.\n",
    "\n",
    "From these two families, we mainly focused on giving intuitions regarding the\n",
    "internal machinery of the random forest and gradient-boosting algorithms\n",
    "which are state-of-the-art methods."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
