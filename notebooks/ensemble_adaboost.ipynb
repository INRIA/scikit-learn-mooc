{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Boosting (AdaBoost)\n",
    "\n",
    "In this notebook, we present the Adaptive Boosting (AdaBoost) algorithm. The\n",
    "aim is to intuitions regarding the internal machinery of AdaBoost and\n",
    "boosting more in general.\n",
    "\n",
    " We will load the \"penguin\" dataset used in the \"tree in depth\" notebook. We\n",
    "will predict penguin species from the features culmen length and depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "range_features = {\n",
    "    feature_name: (X[feature_name].min() - 1, X[feature_name].max() + 1)\n",
    "    for feature_name in X.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we are also using on the function used the previous \"tree in\n",
    "depth\" notebook to plot the decision function of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(fitted_classifier, range_features, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will purposely train a shallow decision tree. Since the tree is shallow,\n",
    "it is unlikely to overfit and some of the training examples will even be\n",
    "misclassified on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "palette = [\"tab:red\", \"tab:blue\", \"black\"]\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X, y)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1], hue=target_column,\n",
    "                data=data, palette=palette, ax=ax)\n",
    "_ = plot_decision_function(tree, range_features, ax=ax)\n",
    "\n",
    "# find the misclassified samples\n",
    "y_pred = tree.predict(X)\n",
    "misclassified_samples_idx = np.flatnonzero(y != y_pred)\n",
    "\n",
    "ax.plot(X.iloc[misclassified_samples_idx, 0],\n",
    "        X.iloc[misclassified_samples_idx, 1],\n",
    "        \"+k\", label=\"Misclassified samples\")\n",
    "_ = ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that several samples have been misclassified by the classifier.\n",
    "\n",
    "We mentioned that boosting relies on creating a new classifier which tries to\n",
    "correct these misclassifications. In scikit-learn, learners support a\n",
    "parameter `sample_weight` which forces the learner to pay more attention to\n",
    "samples with higher weights, during the training.\n",
    "\n",
    "This parameter is set when calling\n",
    "`classifier.fit(X, y, sample_weight=weights)`.\n",
    "We will use this trick to create a new classifier by 'discarding' all\n",
    "correctly classified samples and only considering the misclassified samples.\n",
    "Thus, misclassified samples will be assigned a weight of 1 while well\n",
    "classified samples will assigned to a weight of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.zeros_like(y, dtype=int)\n",
    "sample_weight[misclassified_samples_idx] = 1\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1], hue=target_column,\n",
    "                data=data, palette=palette, ax=ax)\n",
    "plot_decision_function(tree, range_features, ax=ax)\n",
    "\n",
    "ax.plot(X.iloc[misclassified_samples_idx, 0],\n",
    "        X.iloc[misclassified_samples_idx, 1],\n",
    "        \"+k\", label=\"Previous misclassified samples\")\n",
    "_ = ax.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the decision function drastically changed. Qualitatively, we see\n",
    "that the previously misclassified samples are now correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X)\n",
    "newly_misclassified_samples_idx = np.flatnonzero(y != y_pred)\n",
    "remaining_misclassified_samples_idx = np.intersect1d(\n",
    "    misclassified_samples_idx, newly_misclassified_samples_idx\n",
    ")\n",
    "\n",
    "print(f\"Number of samples previously misclassified and \"\n",
    "      f\"still misclassified: {len(remaining_misclassified_samples_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are making mistakes on previously well classified samples. Thus,\n",
    "we get the intuition that we should weight the predictions of each classifier\n",
    "differently, most probably by using the number of mistakes each classifier\n",
    "is making.\n",
    "\n",
    "So we could use the classification error to combine both trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_weight = [\n",
    "    (y.shape[0] - len(misclassified_samples_idx)) / y.shape[0],\n",
    "    (y.shape[0] - len(newly_misclassified_samples_idx)) / y.shape[0],\n",
    "]\n",
    "ensemble_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier was 94% accurate and the second one 69% accurate.\n",
    "Therefore, when predicting a class, we should trust the first classifier\n",
    "slightly more than the second one. We could use these accuracy values to\n",
    "weight the predictions of each learner.\n",
    "\n",
    "To summarize, boosting learns several classifiers, each of which will\n",
    "focus more or less on specific samples of the dataset. Boosting is thus\n",
    "different from bagging: here we never resample our dataset, we just assign\n",
    "different weights to the original dataset.\n",
    "\n",
    "Boosting requires some strategy to combine the learners together:\n",
    "\n",
    "* one needs to define a way to compute the weights to be assigned\n",
    "  to samples;\n",
    "* one needs to assign a weight to each learner when making predictions.\n",
    "\n",
    "Indeed, we defined a really simple scheme to assign sample weights and\n",
    "learner weights. However, there are statistical theories (like in AdaBoost)\n",
    "for how these sample and learner weights can be optimally calculated.\n",
    "\n",
    "**FIXME: I think we should add a reference to ESL here.**\n",
    "\n",
    "We will use the AdaBoost classifier implemented in scikit-learn and\n",
    "look at the underlying decision tree classifiers trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "adaboost = AdaBoostClassifier(base_estimator=base_estimator,\n",
    "                              n_estimators=3, algorithm=\"SAMME\",\n",
    "                              random_state=0)\n",
    "adaboost.fit(X, y)\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, figsize=(18, 6))\n",
    "\n",
    "for ax, tree in zip(axs, adaboost.estimators_):\n",
    "    sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1],\n",
    "                    hue=target_column, data=data,\n",
    "                    palette=palette, ax=ax)\n",
    "    plot_decision_function(tree, range_features, ax=ax)\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "\n",
    "print(f\"Weight of each classifier: {adaboost.estimator_weights_}\")\n",
    "print(f\"Error of each classifier: {adaboost.estimator_errors_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that AdaBoost has learnt three different classifiers each of which\n",
    "focuses on different samples. Looking at the weights of each learner, we see\n",
    "that the ensemble gives the highest weight to the first classifier. This\n",
    "indeed makes sense when we look at the errors of each classifier. The first\n",
    "classifier also has the highest classification performance.\n",
    "\n",
    "While AdaBoost is a nice algorithm to demonstrate the internal machinery of\n",
    "boosting algorithms, it is not the most efficient machine-learning algorithm.\n",
    "The most efficient algorithm based on boosting is the gradient-boosting\n",
    "decision tree (GBDT) algorithm which we will discuss after a short exercise."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
