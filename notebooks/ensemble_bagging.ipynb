{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "In this notebook, we will present the first ensemble using bootstrap samples\n",
    "called bagging.\n",
    "\n",
    "Bagging stands for Bootstrap AGGregatING. It uses bootstrap (random sampling\n",
    "with replacement) to learn several models. At predict time, the predictions\n",
    "of each learner are aggregated to give the final predictions.\n",
    "\n",
    "First, we will generate a simple synthetic dataset to get insights regarding\n",
    "bootstraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a random number generator that will be used to set the randomness\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=50):\n",
    "    \"\"\"Generate synthetic dataset. Returns `data_train`, `data_test`,\n",
    "    `target_train`.\"\"\"\n",
    "    x_max, x_min = 1.4, -1.4\n",
    "    len_x = x_max - x_min\n",
    "    x = rng.rand(n_samples) * len_x - len_x / 2\n",
    "    noise = rng.randn(n_samples) * 0.3\n",
    "    y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "    data_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "    data_test = pd.DataFrame(\n",
    "        np.linspace(x_max, x_min, num=300), columns=[\"Feature\"])\n",
    "    target_train = pd.Series(y, name=\"Target\")\n",
    "\n",
    "    return data_train, data_test, target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_train, data_test, target_train = generate_data(n_samples=50)\n",
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "_ = plt.title(\"Synthetic regression dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link between our feature and the target to predict is non-linear.\n",
    "However, a decision tree is capable of fitting such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(data_train, target_train)\n",
    "y_pred = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "plt.plot(data_test, y_pred, label=\"Fitted tree\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Predictions by a single decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Let's see how we can use bootstraping to learn several trees.\n",
    "\n",
    "## Bootstrap sample\n",
    "\n",
    "A bootstrap sample corresponds to a resampling, with replacement, of the\n",
    "original dataset, a sample that is the same size as the original dataset.\n",
    "Thus, the bootstrap sample will contain some data points several times while\n",
    "some of the original data points will not be present.\n",
    "\n",
    "We will create a function that given `data` and `target` will return a\n",
    "bootstrap sample `data_bootstrap` and `target_bootstrap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data, target):\n",
    "    # indices corresponding to a sampling with replacement of the same sample\n",
    "    # size than the original data\n",
    "    bootstrap_indices = rng.choice(\n",
    "        np.arange(target.shape[0]), size=target.shape[0], replace=True,\n",
    "    )\n",
    "    data_bootstrap_sample = data.iloc[bootstrap_indices]\n",
    "    target_bootstrap_sample = target.iloc[bootstrap_indices]\n",
    "    return data_bootstrap_sample, target_bootstrap_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate 3 bootstrap samples and qualitatively check the difference\n",
    "with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraps_illustration = pd.DataFrame()\n",
    "bootstraps_illustration[\"Original\"] = data_train[\"Feature\"]\n",
    "\n",
    "n_bootstrap = 3\n",
    "for bootstrap_idx in range(n_bootstrap):\n",
    "    # draw a bootstrap from the original data\n",
    "    bootstrap_data, target_data = bootstrap_sample(data_train, target_train)\n",
    "    # store only the bootstrap sample\n",
    "    bootstraps_illustration[f\"Boostrap sample #{bootstrap_idx + 1}\"] = \\\n",
    "        bootstrap_data[\"Feature\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we generated three bootstrap samples and we stored only\n",
    "the feature values. In this manner, we will plot the features value from each\n",
    "set and check the how different they are.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">In the next cell, we transform the dataframe from wide to long format. The\n",
    "column name become a by row information. <tt class=\"docutils literal\">pd.melt</tt> is in charge of doing this\n",
    "transformation. We make this transformation because we will use the seaborn\n",
    "function <tt class=\"docutils literal\">sns.swarmplot</tt> that expect long format dataframe.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraps_illustration = bootstraps_illustration.melt(\n",
    "    var_name=\"Type of data\", value_name=\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x=bootstraps_illustration[\"Feature\"],\n",
    "              y=bootstraps_illustration[\"Type of data\"])\n",
    "_ = plt.title(\"Feature values for the different sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the 3 generated bootstrap samples are all different from the\n",
    "original dataset. The sampling with replacement is the cause of this\n",
    "fluctuation. To confirm this intuition, we can check the number of unique\n",
    "samples in the bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_huge, data_test_huge, target_train_huge = generate_data(\n",
    "    n_samples=100_000)\n",
    "data_bootstrap_sample, target_bootstrap_sample = bootstrap_sample(\n",
    "    data_train_huge, target_train_huge)\n",
    "\n",
    "ratio_unique_sample = (np.unique(data_bootstrap_sample).size /\n",
    "                       data_bootstrap_sample.size)\n",
    "print(\n",
    "    f\"Percentage of samples present in the original dataset: \"\n",
    "    f\"{ratio_unique_sample * 100:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, ~63.2% of the original data points of the original dataset will\n",
    "be present in the bootstrap sample. The other ~36.8% are just repeated\n",
    "samples.\n",
    "\n",
    "So, we are able to generate many datasets, all slightly different. Now, we\n",
    "can fit a decision tree for each of these datasets and they all\n",
    "shall be slightly different as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = []\n",
    "for bootstrap_idx in range(n_bootstrap):\n",
    "    tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "\n",
    "    data_bootstrap_sample, target_bootstrap_sample = bootstrap_sample(\n",
    "        data_train, target_train)\n",
    "    tree.fit(data_bootstrap_sample, target_bootstrap_sample)\n",
    "    forest.append(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created a forest with many different trees, we can use each of\n",
    "the tree to predict on the testing data. They shall give slightly different\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    target_predicted = tree.predict(data_test)\n",
    "    plt.plot(data_test, target_predicted, linestyle=\"--\", alpha=0.8,\n",
    "             label=f\"Tree #{tree_idx} predictions\")\n",
    "\n",
    "plt.legend()\n",
    "_ = plt.title(\"Predictions of trees trained on different bootstraps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating\n",
    "\n",
    "Once our trees are fitted and we are able to get predictions for each of\n",
    "them, we need to combine them. In regression, the most straightforward\n",
    "approach is to average the different predictions from all learners. We can\n",
    "plot the averaged predictions from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "\n",
    "target_predicted_forest = []\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    target_predicted = tree.predict(data_test)\n",
    "    plt.plot(data_test, target_predicted, linestyle=\"--\", alpha=0.8,\n",
    "             label=f\"Tree #{tree_idx} predictions\")\n",
    "    target_predicted_forest.append(target_predicted)\n",
    "\n",
    "target_predicted_forest = np.mean(target_predicted_forest, axis=0)\n",
    "plt.plot(data_test, target_predicted_forest, label=\"Averaged predictions\",\n",
    "         linestyle=\"-\")\n",
    "plt.legend()\n",
    "plt.title(\"Predictions of individual and combined tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unbroken red line shows the averaged predictions, which would be the\n",
    "final preditions given by our 'bag' of decision tree regressors.\n",
    "\n",
    "## Bagging in scikit-learn\n",
    "\n",
    "Scikit-learn implements bagging estimators. It takes a base model that is the\n",
    "model trained on each bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
    "                           n_estimators=3)\n",
    "bagging.fit(data_train, target_train)\n",
    "target_predicted_forest = bagging.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "plt.plot(data_test, target_predicted_forest, label=\"Bag of decision trees\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Predictions from a bagging classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we used a decision tree as a base model, nothing prevent us of using\n",
    "any other type of model. We will give an example that will use a linear\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=LinearRegression(),\n",
    "                           n_estimators=3)\n",
    "bagging.fit(data_train, target_train)\n",
    "target_predicted_linear = bagging.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data_train[\"Feature\"], y=target_train, color=\"black\",\n",
    "                alpha=0.5)\n",
    "plt.plot(data_test, target_predicted_forest, label=\"Bag of decision trees\")\n",
    "plt.plot(data_test, target_predicted_linear, label=\"Bag of linear regression\")\n",
    "plt.legend()\n",
    "_ = plt.title(\"Bagging classifiers using \\ndecision trees and linear models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we see that using a bag of linear models is not helpful here because\n",
    "we still obtain a linear model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}