{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "In this notebook, we will present the first ensemble using bootstrap samples\n",
    "called bagging.\n",
    "\n",
    "Bagging stands for Bootstrap AGGregatING. It uses bootstrap (random sampling\n",
    "with replacement) to learn several models. At predict time, the predictions\n",
    "of each learner are aggregated to give the final predictions.\n",
    "\n",
    "First, we will generate a simple synthetic dataset to get insights regarding\n",
    "bootstraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a random number generator that\n",
    "# will be used to set the randomness\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=50):\n",
    "    \"\"\"Generate synthetic dataset. Returns `X_train`, `X_test`, `y_train`.\"\"\"\n",
    "    x_max, x_min = 1.4, -1.4\n",
    "    len_x = x_max - x_min\n",
    "    x = rng.rand(n_samples) * len_x - len_x / 2\n",
    "    noise = rng.randn(n_samples) * 0.3\n",
    "    y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "    X_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "    X_test = pd.DataFrame(\n",
    "        np.linspace(x_max, x_min, num=300), columns=[\"Feature\"])\n",
    "    y_train = pd.Series(y, name=\"Target\")\n",
    "\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "X_train, X_test, y_train = generate_data(n_samples=50)\n",
    "_ = sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\",\n",
    "                    alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link between our feature and the target to predict is non-linear.\n",
    "However, a decision tree is capable of fitting such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\",\n",
    "                     alpha=0.5)\n",
    "ax.plot(X_test, y_pred, label=\"Fitted tree\")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Let's see how we can use bootstraping to learn several trees.\n",
    "\n",
    "## Bootstrap sample\n",
    "\n",
    "A bootstrap sample corresponds to a resampling, with replacement, of the\n",
    "original dataset, a sample that is the same size as the original dataset.\n",
    "Thus, the bootstrap sample will contain some data points several times while\n",
    "some of the original data points will not be present.\n",
    "\n",
    "We will create a function that given `X` and `y` will return a bootstrap\n",
    "sample `X_bootstrap` and `y_bootstrap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    bootstrap_indices = rng.choice(\n",
    "        np.arange(y.shape[0]), size=y.shape[0], replace=True,\n",
    "    )\n",
    "    X_bootstrap_sample = X.iloc[bootstrap_indices]\n",
    "    y_bootstrap_sample = y.iloc[bootstrap_indices]\n",
    "    return X_bootstrap_sample, y_bootstrap_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate 3 bootstrap samples and qualitatively check the difference\n",
    "with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_bootstrap = 3\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for marker, bootstrap_idx in zip([\"o\", \"^\", \"x\"], range(n_bootstrap)):\n",
    "    X_bootstrap_sample, y_bootstrap_sample = bootstrap_sample(\n",
    "        X_train, y_train)\n",
    "\n",
    "    sns.scatterplot(x=X_bootstrap_sample[\"Feature\"], y=y_bootstrap_sample,\n",
    "                    label=f\"Bootstrap sample #{bootstrap_idx}\",\n",
    "                    marker=marker, alpha=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the 3 generated bootstrap samples are all different. To\n",
    "confirm this intuition, we can check the number of unique samples in the\n",
    "bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to generate a larger set to have a good estimate\n",
    "X_huge_train, y_huge_train, X_test_huge = generate_data(n_samples=10000)\n",
    "X_bootstrap_sample, y_bootstrap_sample = bootstrap_sample(\n",
    "    X_huge_train, y_huge_train)\n",
    "\n",
    "print(\n",
    "    f\"Percentage of samples present in the original dataset: \"\n",
    "    f\"{np.unique(X_bootstrap_sample).size / X_bootstrap_sample.size * 100:.1f}\"\n",
    "    f\"%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, 63.2% of the original data points of the original dataset will\n",
    "be present in the bootstrap sample. The other 36.8% are just repeated\n",
    "samples.\n",
    "\n",
    "So, we are able to generate many datasets, all slightly different. Now, we\n",
    "can fit a decision tree to each of these datasets and each decision\n",
    "tree shall be slightly different as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=n_bootstrap, figsize=(16, 4),\n",
    "                      sharex=True, sharey=True)\n",
    "\n",
    "forest = []\n",
    "for idx, (ax, _) in enumerate(zip(axs, range(n_bootstrap))):\n",
    "    X_bootstrap_sample, y_bootstrap_sample = bootstrap_sample(\n",
    "        X_train, y_train)\n",
    "    tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "    tree.fit(X_bootstrap_sample, y_bootstrap_sample)\n",
    "    forest.append(tree)\n",
    "\n",
    "    y_pred = forest[-1].predict(X_test)\n",
    "\n",
    "    sns.scatterplot(x=X_bootstrap_sample[\"Feature\"], y=y_bootstrap_sample,\n",
    "                    ax=ax, color=\"black\", alpha=0.5)\n",
    "    ax.plot(X_test, y_pred, linewidth=3, label=\"Fitted tree\")\n",
    "    ax.set_title(f\"Bootstrap sample #{idx}\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these decision functions on the same plot to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5)\n",
    "y_pred_forest = []\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    y_pred = tree.predict(X_test)\n",
    "    ax.plot(X_test, y_pred, label=f\"Tree #{tree_idx} predictions\",\n",
    "            linestyle=\"--\", linewidth=3, alpha=0.8)\n",
    "    y_pred_forest.append(y_pred)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating\n",
    "\n",
    "Once our trees are fitted and we are able to get predictions for each of\n",
    "them, we also need to combine them. In regression, the most straightforward\n",
    "approach is to average the different predictions from all learners. We can\n",
    "plot the averaged predictions from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\",\n",
    "                     alpha=0.5)\n",
    "y_pred_forest = []\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    y_pred = tree.predict(X_test)\n",
    "    ax.plot(X_test, y_pred, label=f\"Tree #{tree_idx} predictions\",\n",
    "            linestyle=\"--\", linewidth=3, alpha=0.8)\n",
    "    y_pred_forest.append(y_pred)\n",
    "\n",
    "y_pred_forest = np.mean(y_pred_forest, axis=0)\n",
    "ax.plot(X_test, y_pred_forest, label=\"Averaged predictions\",\n",
    "        linestyle=\"-\", linewidth=3, alpha=0.8)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unbroken red line shows the averaged predictions, which would be the\n",
    "final preditions given by our 'bag' of decision tree regressors.\n",
    "\n",
    "## Bagging in scikit-learn\n",
    "\n",
    "Scikit-learn implements bagging estimators. It takes a base model that is the\n",
    "model trained on each bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
    "                           n_estimators=3)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_forest = bagging.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\",\n",
    "                     alpha=0.5)\n",
    "ax.plot(X_test, y_pred_forest, label=\"Bag of decision trees\",\n",
    "        linestyle=\"-\", linewidth=3, alpha=0.8)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we used a decision tree as a base model, nothing prevent us of using\n",
    "any other type of model. We will give an example that will use a linear\n",
    "regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=LinearRegression(),\n",
    "                           n_estimators=3)\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_linear = bagging.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\",\n",
    "                     alpha=0.5)\n",
    "ax.plot(X_test, y_pred_forest, label=\"Bag of decision trees\",\n",
    "        linestyle=\"-\", linewidth=3, alpha=0.8)\n",
    "ax.plot(X_test, y_pred_linear, label=\"Bag of linear regression\",\n",
    "        linestyle=\"-\", linewidth=3, alpha=0.8)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we see that using a bag of linear models is not helpful here because\n",
    "we still obtain a linear model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
