{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosting decision tree (GBDT)\n",
    "\n",
    "In this notebook, we present gradient boosting decision tree algorithm and\n",
    "the difference with AdaBoost.\n",
    "\n",
    "Gradient-boosting differs from AdaBoost due to the following reason: instead\n",
    "of assigning weights to specific samples, GBDT will fit a decision tree on\n",
    "the residuals error (hence the name \"gradient\") of the previous tree.\n",
    "Therefore, each new added tree in the ensemble predicts the error made by the\n",
    "previous learner instead of predicting the target directly.\n",
    "\n",
    "In this section, we will provide some intuition about the way learners are\n",
    "combined to give the final prediction. In this regard, let's go back to our\n",
    "regression problem which is more intuitive for demonstrating the underlying\n",
    "machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a random number generator that\n",
    "# will be used to set the randomness\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "def generate_data(n_samples=50):\n",
    "    \"\"\"Generate synthetic dataset. Returns `X_train`, `X_test`, `y_train`.\"\"\"\n",
    "    x_max, x_min = 1.4, -1.4\n",
    "    len_x = x_max - x_min\n",
    "    x = rng.rand(n_samples) * len_x - len_x / 2\n",
    "    noise = rng.randn(n_samples) * 0.3\n",
    "    y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "    X_train = pd.DataFrame(x, columns=[\"Feature\"])\n",
    "    X_test = pd.DataFrame(np.linspace(x_max, x_min, num=300),\n",
    "                          columns=[\"Feature\"])\n",
    "    y_train = pd.Series(y, name=\"Target\")\n",
    "\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "X_train, X_test, y_train = generate_data()\n",
    "\n",
    "_ = sns.scatterplot(x=X_train[\"Feature\"], y=y_train,\n",
    "                    color=\"black\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously discussed, boosting will be based on assembling a sequence\n",
    "of learners. We will start by creating a decision tree regressor. We will fix\n",
    "the depth of the tree so that the resulting learner will underfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = tree.predict(X_train)\n",
    "y_pred_test = tree.predict(X_test)\n",
    "\n",
    "# plot the data\n",
    "_ = sns.scatterplot(x=X_train[\"Feature\"], y=y_train,\n",
    "                    color=\"black\", alpha=0.5)\n",
    "# plot the predictions\n",
    "line_predictions = plt.plot(X_test, y_pred_test, \"--\")\n",
    "\n",
    "for idx in range(len(y_train)):\n",
    "    # plot the residuals\n",
    "    lines_residuals = plt.plot([X_train.iloc[idx], X_train.iloc[idx]],\n",
    "                               [y_train.iloc[idx], y_pred_train[idx]],\n",
    "                               color=\"red\")\n",
    "\n",
    "_ = plt.legend([line_predictions[0], lines_residuals[0]],\n",
    "               [\"Fitted tree\", \"Residuals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "In the cell above, we manually edit the legend to get only a single label\n",
    "for all residual lines.\n",
    "```\n",
    "Since the tree underfits the data, its accuracy is far from perfect on the\n",
    "training data. We can observe this in the figure by looking at the difference\n",
    "between the predictions and the ground-truth data. We represent these errors,\n",
    "called \"Residuals\", by unbroken red lines.\n",
    "\n",
    "Indeed, our initial tree was not expressive enough to handle the complexity\n",
    "of the data, as shown by the residuals. In a gradient-boosting algorithm, the\n",
    "idea is to create a second tree which, given the same data `x`, will try to\n",
    "predict the residuals instead of the target `y`. We would therefore have a\n",
    "tree that is able to predict the errors made by the initial tree.\n",
    "\n",
    "Let's train such a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_train - y_pred_train\n",
    "\n",
    "tree_residuals = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "tree_residuals.fit(X_train, residuals)\n",
    "\n",
    "y_pred_train_residuals = tree_residuals.predict(X_train)\n",
    "y_pred_test_residuals = tree_residuals.predict(X_test)\n",
    "\n",
    "_ = sns.scatterplot(x=X_train[\"Feature\"], y=residuals,\n",
    "                    color=\"black\", alpha=0.5)\n",
    "line_predictions = plt.plot(X_test, y_pred_test_residuals, \"--\")\n",
    "\n",
    "for idx in range(len(y_train)):\n",
    "    lines_residuals = plt.plot([X_train.iloc[idx], X_train.iloc[idx]],\n",
    "                               [residuals[idx], y_pred_train_residuals[idx]],\n",
    "                               color=\"red\")\n",
    "\n",
    "_ = plt.legend([line_predictions[0], lines_residuals[0]],\n",
    "               [\"Fitted tree\", \"Residuals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this new tree only manages to fit some of the residuals. We will\n",
    "focus on the last sample in `X_train` and explain how the predictions of both\n",
    "trees are combined. Let's first select the last sample in `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = X_train.iloc[-1, 0]\n",
    "y_true = y_train.iloc[-1]\n",
    "y_true_residual = residuals.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the previous information and highlight our sample of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# plot all samples\n",
    "sns.scatterplot(x=X_train[\"Feature\"], y=y_train,\n",
    "                color=\"black\", alpha=0.5, ax=axs[0])\n",
    "axs[0].plot(X_test, y_pred_test, \"--\")\n",
    "sns.scatterplot(x=X_train[\"Feature\"], y=residuals,\n",
    "                color=\"black\", alpha=0.5, ax=axs[1])\n",
    "plt.plot(X_test, y_pred_test_residuals, \"--\")\n",
    "\n",
    "# plot the predictions of the trees\n",
    "for idx in range(len(y_train)):\n",
    "    axs[0].plot([X_train.iloc[idx], X_train.iloc[idx]],\n",
    "                [y_train.iloc[idx], y_pred_train[idx]],\n",
    "                color=\"red\")\n",
    "    axs[1].plot([X_train.iloc[idx], X_train.iloc[idx]],\n",
    "                [residuals[idx], y_pred_train_residuals[idx]],\n",
    "                color=\"red\")\n",
    "\n",
    "# plot the sample of interest\n",
    "axs[0].scatter(x_max, y_true, label=\"Sample of interest\",\n",
    "               color=\"tab:orange\", s=200)\n",
    "axs[1].scatter(x_max, y_true_residual, label=\"Sample of interest\",\n",
    "               color=\"tab:orange\", s=200)\n",
    "\n",
    "axs[0].set_xlim([-0.5, 0])\n",
    "axs[1].set_xlim([-0.5, 0])\n",
    "axs[0].set_title(\"Tree predictions\")\n",
    "axs[1].set_title(\"Prediction of the residuals\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.subplots_adjust(wspace=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sample of interest, our initial tree is making an error (small\n",
    "residual). When fitting the second tree, the residual in this case is\n",
    "perfectly fitted and predicted. We will quantitatively check this prediction\n",
    "using the fitted tree. First, let's check the prediction of the initial tree\n",
    "and compare it with the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True value to predict for f(x={x_max:.3f}) = {y_true:.3f}\")\n",
    "\n",
    "y_pred_first_tree = tree.predict([[x_max]])[0]\n",
    "print(f\"Prediction of the first decision tree for x={x_max:.3f}: \"\n",
    "      f\"y={y_pred_first_tree:.3f}\")\n",
    "print(f\"Error of the tree: {y_true - y_pred_first_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we visually observed, we have a small error. Now, we can use the second\n",
    "tree to try to predict this residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction of the residual for x={x_max:.3f}: \"\n",
    "      f\"{tree_residuals.predict([[x_max]])[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our second tree is capable of prediting the exact residual\n",
    "(error) of our first tree. Therefore, we can predict the value of `x` by\n",
    "summing the prediction of the all trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_first_and_second_tree = (\n",
    "    y_pred_first_tree + tree_residuals.predict([[x_max]])[0]\n",
    ")\n",
    "print(f\"Prediction of the first and second decision trees combined for \"\n",
    "      f\"x={x_max:.3f}: y={y_pred_first_and_second_tree:.3f}\")\n",
    "print(f\"Error of the tree: {y_true - y_pred_first_and_second_tree:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose a sample for which only two trees were enough to make the perfect\n",
    "prediction. However, we saw in the previous plot that two trees were not\n",
    "enough to correct the residuals of all samples. Therefore, one needs to\n",
    "add several trees to the ensemble to successfully correct the error.\n",
    "(i.e. the second tree corrects the first tree's error, while the third tree\n",
    "corrects the second tree's error and so on.)\n",
    "\n",
    "We will compare the performance of random-forest and gradient boosting on\n",
    "the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "\n",
    "start_time = time()\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "fit_time_gradient_boosting = time() - start_time\n",
    "\n",
    "start_time = time()\n",
    "score_gradient_boosting = gradient_boosting.score(X_test, y_test)\n",
    "score_time_gradient_boosting = time() - start_time\n",
    "\n",
    "print(\"Gradient boosting decision tree\")\n",
    "print(f\"R2 score: {score_gradient_boosting:.3f}\")\n",
    "print(f\"Fit time: {fit_time_gradient_boosting:.2f} s\")\n",
    "print(f\"Score time: {score_time_gradient_boosting:.5f} s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "\n",
    "start_time = time()\n",
    "random_forest.fit(X_train, y_train)\n",
    "fit_time_random_forest = time() - start_time\n",
    "\n",
    "start_time = time()\n",
    "score_random_forest = random_forest.score(X_test, y_test)\n",
    "score_time_random_forest = time() - start_time\n",
    "\n",
    "print(\"Random forest\")\n",
    "print(f\"R2 score: {score_random_forest:.3f}\")\n",
    "print(f\"Fit time: {fit_time_random_forest:.2f} s\")\n",
    "print(f\"Score time: {score_time_random_forest:.5f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In term of computation performance, the forest can be parallelized and will\n",
    "benefit from the having multiple CPUs. In terms of scoring performance, both\n",
    "algorithms lead to very close results.\n",
    "\n",
    "However, we can observe that the gradient boosting is a very fast algorithm\n",
    "to predict compared to random forest. This is due to the fact that gradient\n",
    "boosting uses shallow trees. We will go into details in the next notebook\n",
    "about the tree parametrization."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
