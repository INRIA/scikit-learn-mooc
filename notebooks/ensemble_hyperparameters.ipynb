{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "In the previous section, we did not discuss the parameters of random forest\n",
    "and gradient-boosting. However, there are a couple of things to keep in mind\n",
    "when setting these.\n",
    "\n",
    "This notebook gives crucial information regarding how to set the\n",
    "hyperparameters of both random forest and gradient boosting decision tree\n",
    "models.\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">For the sake of clarity, no cross-validation will be used to estimate the\n",
    "variability of the testing error. We are only showing the effect of the\n",
    "parameters on the validation set of what should be the inner loop of a nested\n",
    "cross-validation.</p>\n",
    "</div>\n",
    "\n",
    "We will start by loading the california housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "The main parameter to select in random forest is the `n_estimators` parameter.\n",
    "In general, the more trees in the forest, the better the generalization\n",
    "performance will be. However, it will slow down the fitting and prediction\n",
    "time. The goal is to balance computing time and generalization performance\n",
    "when setting the number of estimators. Here, we fix `n_estimators=100`, which\n",
    "is already the default value.\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">Tuning the <tt class=\"docutils literal\">n_estimators</tt> for random forests generally result in a waste of\n",
    "computer power. We just need to ensure that it is large enough so that doubling\n",
    "its value does not lead to a significant improvement of the validation error.</p>\n",
    "</div>\n",
    "\n",
    "Instead, we can tune the hyperparameter `max_features`, which controls the\n",
    "size of the random subset of features to consider when looking for the best\n",
    "split when growing the trees: smaller values for `max_features` will lead to\n",
    "more random trees with hopefully more uncorrelated prediction errors. However\n",
    "if `max_features` is too small, predictions can be too random, even after\n",
    "averaging with the trees in the ensemble.\n",
    "\n",
    "If `max_features` is set to `None`, then this is equivalent to setting\n",
    "`max_features=n_features` which means that the only source of randomness in\n",
    "the random forest is the bagging procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In this case, n_features={len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tune the different parameters that control the depth of each tree\n",
    "in the forest. Two parameters are important for this: `max_depth` and\n",
    "`max_leaf_nodes`. They differ in the way they control the tree structure.\n",
    "Indeed, `max_depth` will enforce to have a more symmetric tree, while\n",
    "`max_leaf_nodes` does not impose such constraint. If `max_leaf_nodes=None`\n",
    "then the number of leaf nodes is unlimited.\n",
    "\n",
    "The hyperparameter `min_samples_leaf` controls the minimum number of samples\n",
    "required to be at a leaf node. This means that a split point (at any depth) is\n",
    "only done if it leaves at least `min_samples_leaf` training samples in each of\n",
    "the left and right branches. A small value for `min_samples_leaf` means that\n",
    "some samples can become isolated when a tree is deep, promoting overfitting. A\n",
    "large value would prevent deep trees, which can lead to underfitting.\n",
    "\n",
    "Be aware that with random forest, trees are expected to be deep since we are\n",
    "seeking to overfit each tree on each bootstrap sample. Overfitting is\n",
    "mitigated when combining the trees altogether, whereas assembling underfitted\n",
    "trees (i.e. shallow trees) might also lead to an underfitted forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"max_features\": [1, 2, 3, 5, None],\n",
    "    \"max_leaf_nodes\": [10, 100, 1000, None],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10, 20, 50, 100],\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    RandomForestRegressor(n_jobs=2), param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\", n_iter=10, random_state=0, n_jobs=2,\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe in our search that we are required to have a large number of\n",
    "`max_leaf_nodes` and thus deep trees. This parameter seems particularly\n",
    "impactful with respect to the other tuning parameters, but large values of\n",
    "`min_samples_leaf` seem to reduce the performance of the model.\n",
    "\n",
    "In practice, more iterations of random search would be necessary to precisely\n",
    "assert the role of each parameters. Using `n_iter=10` is good enough to\n",
    "quickly inspect the hyperparameter combinations that yield models that work\n",
    "well enough without spending too much computational resources. Feel free to\n",
    "try more interations on your own.\n",
    "\n",
    "Once the `RandomizedSearchCV` has found the best set of hyperparameters, it\n",
    "uses them to refit the model using the full training set. To estimate the\n",
    "generalization performance of the best model it suffices to call `.score` on\n",
    "the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"On average, our random forest regressor makes an error of {error:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-boosting decision trees\n",
    "\n",
    "For gradient-boosting, parameters are coupled, so we cannot set the parameters\n",
    "one after the other anymore. The important parameters are `n_estimators`,\n",
    "`learning_rate`, and `max_depth` or `max_leaf_nodes` (as previously discussed\n",
    "random forest).\n",
    "\n",
    "Let's first discuss the `max_depth` (or `max_leaf_nodes`) parameter. We saw\n",
    "in the section on gradient-boosting that the algorithm fits the error of the\n",
    "previous tree in the ensemble. Thus, fitting fully grown trees would be\n",
    "detrimental. Indeed, the first tree of the ensemble would perfectly fit\n",
    "(overfit) the data and thus no subsequent tree would be required, since there\n",
    "would be no residuals. Therefore, the tree used in gradient-boosting should\n",
    "have a low depth, typically between 3 to 8 levels, or few leaves ($2^3=8$ to\n",
    "$2^8=256$). Having very weak learners at each step will help reducing\n",
    "overfitting.\n",
    "\n",
    "With this consideration in mind, the deeper the trees, the faster the\n",
    "residuals will be corrected and less learners are required. Therefore,\n",
    "`n_estimators` should be increased if `max_depth` is lower.\n",
    "\n",
    "Finally, we have overlooked the impact of the `learning_rate` parameter until\n",
    "now. When fitting the residuals, we would like the tree to try to correct all\n",
    "possible errors or only a fraction of them. The learning-rate allows you to\n",
    "control this behaviour. A small learning-rate value would only correct the\n",
    "residuals of very few samples. If a large learning-rate is set (e.g., 1), we\n",
    "would fit the residuals of all samples. So, with a very low learning-rate, we\n",
    "will need more estimators to correct the overall error. However, a too large\n",
    "learning-rate tends to obtain an overfitted ensemble, similar to having a too\n",
    "large tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": loguniform(0.01, 1),\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(), param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\", n_iter=20, random_state=0, n_jobs=2\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">Here, we tune the <tt class=\"docutils literal\">n_estimators</tt> but be aware that is better to use\n",
    "<tt class=\"docutils literal\">early_stopping</tt> as done in the Exercise M6.04.</p>\n",
    "</div>\n",
    "\n",
    "In this search, we see that the `learning_rate` is required to be large\n",
    "enough, i.e. > 0.1. We also observe that for the best ranked models, having a\n",
    "smaller `learning_rate`, will require more trees or a larger number of\n",
    "leaves for each tree. However, it is particularly difficult to draw\n",
    "more detailed conclusions since the best value of an hyperparameter depends\n",
    "on the other hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate the generalization performance of the best model using the\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"On average, our GBDT regressor makes an error of {error:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean test score in the held-out test set is slightly better than the score\n",
    "of the best model. The reason is that the final model is refitted on the whole\n",
    "training set and therefore, on more data than the cross-validated models of\n",
    "the grid search procedure."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}