{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "In the previous section, we did not discuss the parameters of random forest\n",
    "and gradient-boosting. However, there are a couple of things to keep in mind\n",
    "when setting these.\n",
    "\n",
    "This notebook gives crucial information regarding how to set the\n",
    "hyperparameters of both random forest and gradient boosting decision tree\n",
    "models.\n",
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">For the sake of clarity, no cross-validation will be used to estimate the\n",
    "testing error. We are only showing the effect of the parameters\n",
    "on the validation set of what should be the inner cross-validation.</p>\n",
    "</div>\n",
    "\n",
    "## Random forest\n",
    "\n",
    "The main parameter to tune for random forest is the `n_estimators` parameter.\n",
    "In general, the more trees in the forest, the better the generalization\n",
    "performance will be. However, it will slow down the fitting and prediction\n",
    "time. The goal is to balance computing time and generalization performance when\n",
    "setting the number of estimators when putting such learner in production.\n",
    "\n",
    "Then, we could also tune a parameter that controls the depth of each tree in\n",
    "the forest. Two parameters are important for this: `max_depth` and\n",
    "`max_leaf_nodes`. They differ in the way they control the tree structure.\n",
    "Indeed, `max_depth` will enforce to have a more symmetric tree, while\n",
    "`max_leaf_nodes` does not impose such constraint.\n",
    "\n",
    "Be aware that with random forest, trees are generally deep since we are\n",
    "seeking to overfit each tree on each bootstrap sample because this will be\n",
    "mitigated by combining them altogether. Assembling underfitted trees (i.e.\n",
    "shallow trees) might also lead to an underfitted forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    RandomForestRegressor(n_jobs=2), param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\", n_iter=10, random_state=0, n_jobs=2,\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe in our search that we are required to have a large\n",
    "number of leaves and thus deep trees. This parameter seems particularly\n",
    "impactful in comparison to the number of trees for this particular dataset:\n",
    "with at least 50 trees, the generalization performance will be driven by the\n",
    "number of leaves.\n",
    "\n",
    "Now we will estimate the generalization performance of the best model by\n",
    "refitting it with the full training set and using the test set for scoring on\n",
    "unseen data. This is done by default when calling the `.fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"On average, our random forest regressor makes an error of {error:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-boosting decision trees\n",
    "\n",
    "For gradient-boosting, parameters are coupled, so we cannot set the parameters\n",
    "one after the other anymore. The important parameters are `n_estimators`,\n",
    "`learning_rate`, and `max_depth` or `max_leaf_nodes` (as previously discused\n",
    "random forest).\n",
    "\n",
    "Let's first discuss the `max_depth` (or `max_leaf_nodes`) parameter. We saw\n",
    "in the section on gradient-boosting that the algorithm fits the error of the\n",
    "previous tree in the ensemble. Thus, fitting fully grown trees would be\n",
    "detrimental. Indeed, the first tree of the ensemble would perfectly fit\n",
    "(overfit) the data and thus no subsequent tree would be required, since there\n",
    "would be no residuals. Therefore, the tree used in gradient-boosting should\n",
    "have a low depth, typically between 3 to 8 levels, or few leaves ($2^3=8$ to\n",
    "$2^8=256$). Having very weak learners at each step will help reducing\n",
    "overfitting.\n",
    "\n",
    "With this consideration in mind, the deeper the trees, the faster the\n",
    "residuals will be corrected and less learners are required. Therefore,\n",
    "`n_estimators` should be increased if `max_depth` is lower.\n",
    "\n",
    "Finally, we have overlooked the impact of the `learning_rate` parameter until\n",
    "now. When fitting the residuals, we would like the tree to try to correct all\n",
    "possible errors or only a fraction of them. The learning-rate allows you to\n",
    "control this behaviour. A small learning-rate value would only correct the\n",
    "residuals of very few samples. If a large learning-rate is set (e.g., 1), we\n",
    "would fit the residuals of all samples. So, with a very low learning-rate, we\n",
    "will need more estimators to correct the overall error. However, a too large\n",
    "learning-rate tends to obtain an overfitted ensemble, similar to having a too\n",
    "large tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500],\n",
    "    \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100],\n",
    "    \"learning_rate\": loguniform(0.01, 1),\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(), param_distributions=param_distributions,\n",
    "    scoring=\"neg_mean_absolute_error\", n_iter=20, random_state=0, n_jobs=2\n",
    ")\n",
    "search_cv.fit(data_train, target_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"admonition caution alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Caution!</p>\n",
    "<p class=\"last\">Here, we tune the <tt class=\"docutils literal\">n_estimators</tt> but be aware that using early-stopping as\n",
    "in the previous exercise will be better.</p>\n",
    "</div>\n",
    "\n",
    "In this search, we see that the `learning_rate` is required to be large\n",
    "enough, i.e. > 0.1. We also observe that for the best ranked models, having a\n",
    "smaller `learning_rate`, will require more trees or a larger number of\n",
    "leaves for each tree. However, it is particularly difficult to draw\n",
    "more detailed conclusions since the best value of an hyperparameter depends\n",
    "on the other hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate the generalization performance of the best model\n",
    "using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = -search_cv.score(data_test, target_test)\n",
    "print(f\"On average, our GBDT regressor makes an error of {error:.2f} k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean test score in the held-out test set is slightly better than the score\n",
    "of the best model. The reason is that the final model is refitted on the whole\n",
    "training set and therefore, on more data than the inner cross-validated models\n",
    "of the grid search procedure."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}