{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning\n",
    "\n",
    "In the previous section, we did not discuss the parameters of random forest\n",
    "and gradient-boosting. However, there are a couple of things to keep in mind\n",
    "when setting these parameters.\n",
    "\n",
    "This notebook gives crucial information regarding how to set the\n",
    "hyperparameters of both random forest and gradient boostin decision tree\n",
    "models.\n",
    "\n",
    "## Random forest\n",
    "\n",
    "The main parameter to tune with random forest is the `n_estimators`\n",
    "parameter. In general, the more trees in the forest, the better the\n",
    "performance will be. However, it will slow down the fitting and prediction\n",
    "time. So one has to balance compute time and performance when setting the\n",
    "number of estimators when putting such learner in production.\n",
    "\n",
    "The `max_depth` parameter could also be tuned. Sometimes, there is no need to\n",
    "have fully grown trees. However, be aware that with random forest, trees are\n",
    "generally deep since we are seeking to overfit the learners on the bootstrap\n",
    "samples because this will be mitigated by combining them. Assembling\n",
    "underfitted trees (i.e. shallow trees) might also lead to an underfitted\n",
    "forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 20, 30],\n",
    "    \"max_depth\": [3, 5, None],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(n_jobs=-1), param_grid=param_grid, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "columns += [\"mean_test_score\", \"rank_test_score\"]\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results[columns].sort_values(by=\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in our grid-search, the largest `max_depth` together with\n",
    "largest `n_estimators` led to the best performance.\n",
    "\n",
    "## Gradient-boosting decision tree\n",
    "\n",
    "For gradient-boosting, parameters are coupled, so we can not anymore set the\n",
    "parameters one after the other. The important parameters are `n_estimators`,\n",
    "`max_depth`, and `learning_rate`.\n",
    "\n",
    "Let's first discuss the `max_depth` parameter. We saw in the section on\n",
    "gradient-boosting that the algorithm fits the error of the previous tree in\n",
    "the ensemble. Thus, fitting fully grown trees will be detrimental. Indeed,\n",
    "the first tree of the ensemble would perfectly fit (overfit) the data and\n",
    "thus no subsequent tree would be required, since there would be no residuals.\n",
    "Therefore, the tree used in gradient-boosting should have a low depth,\n",
    "typically between 3 to 8 levels. Having very weak learners at each step will\n",
    "help reducing overfitting.\n",
    "\n",
    "With this consideration in mind, the deeper the trees, the faster the\n",
    "residuals will be corrected and less learners are required. So `n_estimators`\n",
    "should be increased if `max_depth` is lower.\n",
    "\n",
    "Finally, we have overlooked the impact of the `learning_rate` parameter up\n",
    "till now. When fitting the residuals one could choose if the tree should try\n",
    "to correct all possible errors or only a fraction of them. The learning-rate\n",
    "allows you to control this behaviour. A small learning-rate value would only\n",
    "correct the residuals of very few samples. If a large learning-rate is set\n",
    "(e.g., 1), we would fit the residuals of all samples. So, with a very low\n",
    "learning-rate, we will need more estimators to correct the overall error.\n",
    "However, a too large learning-rate tends to obtain an overfitted ensemble,\n",
    "similar to having a too large tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 30, 50],\n",
    "    \"max_depth\": [3, 5, None],\n",
    "    \"learning_rate\": [0.1, 1],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(), param_grid=param_grid, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "columns += [\"mean_test_score\", \"rank_test_score\"]\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results[columns].sort_values(by=\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{cautious}\n",
    "Here, we tune the `n_estimators` but be aware that using early-stopping as\n",
    "in the previous exercise will be better.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
