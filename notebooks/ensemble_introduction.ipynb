{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory example to ensemble models\n",
    "\n",
    "This first notebook aims at emphasizing the benefit of ensemble methods over\n",
    "simple models (e.g. decision tree, linear model, etc.). Combining simple\n",
    "models result in more powerful and robust models with less hassle.\n",
    "\n",
    "We will start by loading the california housing dataset. We recall that the\n",
    "goal in this dataset is to predict the median house value in some district\n",
    "in California based on demographic and geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will divide the dataset into a traning and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a decision tree regressor and check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "print(f\"R2 score of the default tree:\\n\"\n",
    "      f\"{tree.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain fair results. However, as we previously presented in the \"tree in\n",
    "depth\" notebook, this model needs to be tuned to overcome over- or\n",
    "under-fitting. Indeed, the default parameters will not necessarily lead to an\n",
    "optimal decision tree. Instead of using the default value, we should search\n",
    "via cross-validation the optimal value of the important parameters such as\n",
    "`max_depth`, `min_samples_split`, or `min_samples_leaf`.\n",
    "\n",
    "We recall that we need to tune these parameters, as decision trees tend to\n",
    "overfit the training data if we grow deep trees, but there are no rules on\n",
    "what each parameter should be set to. Thus, not making a search could lead us\n",
    "to have an underfitted or overfitted model.\n",
    "\n",
    "Now, we make a grid-search to fine-tune the parameters that we mentioned\n",
    "earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 8, None],\n",
    "    \"min_samples_split\": [2, 10, 30, 50],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 1]}\n",
    "cv = 3\n",
    "\n",
    "tree = GridSearchCV(DecisionTreeRegressor(random_state=0),\n",
    "                    param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "_ = tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dataframe storing the important information collected during\n",
    "the tuning of the parameters and investigate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(tree.cv_results_)\n",
    "interesting_columns = [\n",
    "    \"param_max_depth\",\n",
    "    \"param_min_samples_split\",\n",
    "    \"param_min_samples_leaf\",\n",
    "    \"mean_test_score\",\n",
    "    \"rank_test_score\",\n",
    "    \"mean_fit_time\",\n",
    "]\n",
    "\n",
    "cv_results = cv_results[interesting_columns].sort_values(by=\"rank_test_score\")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From theses results, we can see that the best parameters is the combination\n",
    "where the depth of the tree is not limited and the minimum number of samples\n",
    "to create a leaf is also equal to 1 (the default values) and the\n",
    "minimum number of samples to make a split of 50 (much higher than the default\n",
    "value.\n",
    "\n",
    "It is interesting to look at the total amount of time it took to fit all\n",
    "these different models. In addition, we can check the performance of the\n",
    "optimal decision tree on the left-out testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fitting_time = (cv_results[\"mean_fit_time\"] * cv).sum()\n",
    "print(f\"Required training time of the GridSearchCV: \"\n",
    "      f\"{total_fitting_time:.2f} seconds\")\n",
    "print(f\"Best R2 score of a single tree: {tree.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have a model that has an $R^2$ score below 0.7. So this model is\n",
    "better than the previous default decision tree.\n",
    "\n",
    "However, the amount of time to find the best learner has an heavy\n",
    "computational cost. Indeed, it depends on the number of folds used during the\n",
    "cross-validation in the grid-search multiplied by the number of parameters.\n",
    "\n",
    "Now we will use an ensemble method called bagging. More details about this\n",
    "method will be discussed in the next section. In short, this method will use\n",
    "a base regressor (i.e. decision tree regressors) and will train several of\n",
    "them on a slightly modified version of the training set. Then, the\n",
    "predictions of all these base regressors will be combined by averaging.\n",
    "\n",
    "Here, we will use 50 decision trees and check the fitting time as well as\n",
    "the performance on the left-out testing data. It is important to note that\n",
    "we are not going to tune any parameter of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=base_estimator, n_estimators=50, random_state=0)\n",
    "\n",
    "start_fitting_time = time()\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "elapsed_fitting_time = time() - start_fitting_time\n",
    "\n",
    "print(f\"Elapsed fitting time: {elapsed_fitting_time:.2f} seconds\")\n",
    "print(f\"R2 score: {bagging_regressor.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the computation time is much shorter for training the full\n",
    "ensemble than for the parameter search of a single tree. In addition, the\n",
    "score is significantly improved with a $R^2$ close to 0.8. Furthermore, note\n",
    "that this result is obtained before any parameter tuning. This shows the\n",
    "motivation behind the use of an ensemble learner: it gives a relatively good\n",
    "baseline with decent performance without any parameter tuning.\n",
    "\n",
    "Now, we will discuss in detail two ensemble families: bagging and\n",
    "boosting:\n",
    "\n",
    "* ensemble using bootstrap (e.g. bagging and random-forest);\n",
    "* ensemble using boosting (e.g. adaptive boosting and gradient-boosting\n",
    "  decision tree)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
