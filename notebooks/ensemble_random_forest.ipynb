{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "In this notebook, we will present random forest models and show the\n",
    "differences with a bagging classifiers.\n",
    "\n",
    "A random forest, a popular model in machine learning, is a modification of\n",
    "the bagging algorithm. In bagging, any classifier or regressor can be used.\n",
    "In a random forest, the base classifier or regressor must be a decision tree.\n",
    "In our previous example, we used a decision tree but we could have used a\n",
    "linear model as the regressor for our bagging algorithm.\n",
    "\n",
    "In addition, random forest is different from bagging when used with\n",
    "classifiers: when searching for the best split, only a subset of the original\n",
    "features are used. By default, this subset of feature is equal to the square\n",
    "root of the total number of features. In regression, the total number of\n",
    "available features will be used.\n",
    "\n",
    "We will illustrate the usage of a random forest and compare it with the\n",
    "bagging regressor on the \"California housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=0,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "bagging = BaggingRegressor(base_estimator=tree, n_estimators=100,\n",
    "                           n_jobs=-1,)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Performance of random forest: \"\n",
    "      f\"{random_forest.score(X_test, y_test):.3f}\")\n",
    "print(f\"Performance of bagging: \"\n",
    "      f\"{bagging.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we don't need to provide a `base_estimator` parameter to\n",
    "`RandomForestRegressor`, it is always a tree classifier. Also note that the\n",
    "scores are almost identical. This is because our problem is a regression\n",
    "problem and therefore, the number of features used in random forest and\n",
    "bagging is the same.\n",
    "\n",
    "For classification problems, we would need to pass a tree model instance\n",
    "with the parameter `max_features=\"sqrt\"` to `BaggingRegressor` if we wanted\n",
    "it to have the same behaviour as the random forest classifier.\n",
    "\n",
    "## Classifiers details\n",
    "\n",
    "Up to now, we have only focused on regression problems. There is a little\n",
    "difference between regression and classification.\n",
    "\n",
    "First, the `base_estimator` should be chosen in line with the problem that\n",
    "is solved: use a classifier with a classification problem and a regressor\n",
    "with a regression problem.\n",
    "\n",
    "Then, the aggregation method is different in regression and classification:\n",
    "\n",
    "- in regression, the average prediction is computed. For instance, if\n",
    "  three learners predict 0.4, 0.3 and 0.31, the aggregation will output 0.33;\n",
    "- while in classification, the class which highest probability (after\n",
    "  averaging the predicted probabilities) is predicted. For instance, if three\n",
    "  learners predict (for two classes) the probability (0.4, 0.6), (0.3, 0.7)\n",
    "  and (0.31, 0.69), the aggregation probability is (0.33, 0.67) and the\n",
    "  second class would be predicted.\n",
    "\n",
    "# Midpoint summary\n",
    "\n",
    "We saw in this section two algorithms that use bootstrap samples to create\n",
    "an ensemble of classifiers or regressors. These algorithms train several\n",
    "learners on different bootstrap samples. The predictions are then\n",
    "aggregated. This operation can be done in a very efficient manner since the\n",
    "training of each learner can be done in parallel."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
