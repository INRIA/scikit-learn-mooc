{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "In this notebook, we will present the random forest models and\n",
    "show the differences with the bagging classifiers.\n",
    "\n",
    "Random forests are a popular model in machine learning. They are a\n",
    "modification of the bagging algorithm. In bagging, any classifier or\n",
    "regressor can be used. In random forests, the base classifier or regressor\n",
    "must be a decision tree. In our previous example, we used a decision tree but\n",
    "we could have used a linear model as the regressor for our bagging algorithm.\n",
    "\n",
    "In addition, random forests are different from bagging when used with\n",
    "classifiers: when searching for the best split, only a subset of the original\n",
    "features are used. By default, this subset of features is equal to the square\n",
    "root of the total number of features. In regression, the total number of\n",
    "available features will be used.\n",
    "\n",
    "We will illustrate the usage of a random forest and compare it with the\n",
    "bagging regressor on the \"California housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">If you want a deeper overview regarding this dataset, you can refer to the\n",
    "Appendix - Datasets description section at the end of this MOOC.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=0,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "bagging = BaggingRegressor(base_estimator=tree, n_estimators=100,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "scores_random_forest = cross_val_score(random_forest, data, target)\n",
    "scores_bagging = cross_val_score(bagging, data, target)\n",
    "\n",
    "print(f\"Statistical performance of random forest: \"\n",
    "      f\"{scores_random_forest.mean():.3f} +/- \"\n",
    "      f\"{scores_random_forest.std():.3f}\")\n",
    "print(f\"Statistical performance of bagging: \"\n",
    "      f\"{scores_bagging.mean():.3f} +/- {scores_bagging.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we don't need to provide a `base_estimator` parameter to\n",
    "`RandomForestRegressor`: it is always a tree classifier. Also note that the\n",
    "scores are almost identical. This is because our problem is a regression\n",
    "problem and therefore, the number of features used in random forest and\n",
    "bagging is the same.\n",
    "\n",
    "For classification problems, we would need to pass a tree model instance\n",
    "with the parameter `max_features=\"sqrt\"` to `BaggingRegressor` if we wanted\n",
    "it to have the same behaviour as the random forest classifier.\n",
    "\n",
    "## Classifiers details\n",
    "\n",
    "Until now, we have focused on regression problems. There are some\n",
    "differences between regression and classification.\n",
    "\n",
    "First, the `base_estimator` should be chosen depending on the problem that\n",
    "needs to be solved: use a classifier for a classification problem and a\n",
    "regressor for a regression problem.\n",
    "\n",
    "Secondly, the aggregation method is different:\n",
    "\n",
    "- in regression, the average prediction is computed. For instance, if\n",
    "  three learners predict 0.4, 0.3 and 0.31, the aggregation will output 0.33;\n",
    "- while in classification, the class which highest probability (after\n",
    "  averaging the predicted probabilities) is predicted. For instance, if three\n",
    "  learners predict (for two classes) the probability (0.4, 0.6), (0.3, 0.7)\n",
    "  and (0.31, 0.69), the aggregation probability is (0.33, 0.67) and the\n",
    "  second class would be predicted.\n",
    "\n",
    "# Midpoint summary\n",
    "\n",
    "We saw in this section two algorithms that use bootstrap samples to create\n",
    "an ensemble of classifiers or regressors. These algorithms train several\n",
    "learners on different bootstrap samples. The predictions are then\n",
    "aggregated. This operation can be done in a very efficient manner since the\n",
    "training of each learner can be done in parallel."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
