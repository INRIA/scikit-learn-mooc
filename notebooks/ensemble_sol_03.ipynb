{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 03\n",
    "\n",
    "This exercise aims at studying if AdaBoost is a model that is able to\n",
    "over-fit. We will make a grid-search and check the scores by varying the\n",
    "number of estimators.\n",
    "\n",
    "We first load the california housing dataset and split it into a training\n",
    "and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `AbaBoostRegressor`. Using the function\n",
    "`sklearn.model_selection.validation_curve` to get training and test scores\n",
    "by varying the value the number of estimators. *Hint: vary the number of\n",
    "estimators between 1 and 60.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "adaboost = AdaBoostRegressor()\n",
    "param_range = np.unique(np.logspace(0, 1.8, num=30).astype(int))\n",
    "train_scores, test_scores = validation_curve(\n",
    "    adaboost, X_train, y_train, param_name=\"n_estimators\",\n",
    "    param_range=param_range, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both the mean training and test scores. You can as well plot the\n",
    "standard deviation of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(param_range,\n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 alpha=0.3)\n",
    "plt.fill_between(param_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for AdaBoost regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the validation curve, we can see that AdaBoost is not immune against\n",
    "overfitting. Indeed, there is an optimal number of estimators to be found.\n",
    "Adding to much estimators are detrimental for the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment using a random forest instead of an AdaBoost regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "train_scores, test_scores = validation_curve(\n",
    "    adaboost, X_train, y_train, param_name=\"n_estimators\",\n",
    "    param_range=param_range, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(param_range,\n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 alpha=0.3)\n",
    "plt.fill_between(param_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for RandomForest regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrary to the AdaBoost regressor, we can see that increasing the number\n",
    "trees in the forest will increase the performance of the random forest.\n",
    "In fact, a random forest has less chance to suffer from overfitting than\n",
    "AdaBoost when increasing the number of estimators."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
