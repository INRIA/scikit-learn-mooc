{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcc3 Solution for Exercise M6.03\n",
    "\n",
    "This exercise aims at verifying if AdaBoost can over-fit.\n",
    "We will make a grid-search and check the scores by varying the\n",
    "number of estimators.\n",
    "\n",
    "We will first load the California housing dataset and split it into a\n",
    "training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">If you want a deeper overview regarding this dataset, you can refer to the\n",
    "Appendix - Datasets description section at the end of this MOOC.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create an `AbaBoostRegressor`. Use the function\n",
    "`sklearn.model_selection.validation_curve` to get training and test scores\n",
    "by varying the number of estimators. Use the mean absolute error as a metric\n",
    "by passing `scoring=\"neg_mean_absolute_error\"`.\n",
    "*Hint: vary the number of estimators between 1 and 60.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "adaboost = AdaBoostRegressor()\n",
    "param_range = np.unique(np.logspace(0, 1.8, num=30).astype(int))\n",
    "train_scores, test_scores = validation_curve(\n",
    "    adaboost, data_train, target_train,\n",
    "    param_name=\"n_estimators\", param_range=param_range,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both the mean training and test errors. You can also plot the\n",
    "standard deviation of the errors.\n",
    "*Hint: you can use `plt.errorbar`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.errorbar(param_range, train_errors.mean(axis=1),\n",
    "             yerr=train_errors.std(axis=1), label=\"Training score\",\n",
    "             alpha=0.7)\n",
    "plt.errorbar(param_range, test_errors.mean(axis=1),\n",
    "             yerr=test_errors.std(axis=1), label=\"Cross-validation score\",\n",
    "             alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Mean absolute error in k$\\n(smaller is better)\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for AdaBoost regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the validation curve, we can see that AdaBoost is not immune against\n",
    "overfitting. Indeed, there is an optimal number of estimators to be found.\n",
    "Adding too many estimators is detrimental for the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment using a random forest instead of an AdaBoost regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "train_scores, test_scores = validation_curve(\n",
    "    forest, data_train, target_train,\n",
    "    param_name=\"n_estimators\", param_range=param_range,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(param_range, train_errors.mean(axis=1),\n",
    "             yerr=train_errors.std(axis=1), label=\"Training score\",\n",
    "             alpha=0.7)\n",
    "plt.errorbar(param_range, test_errors.mean(axis=1),\n",
    "             yerr=test_errors.std(axis=1), label=\"Cross-validation score\",\n",
    "             alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Mean absolute error in k$\\n(smaller is better)\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for RandomForest regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrary to the AdaBoost regressor, we can see that increasing the number\n",
    "trees in the forest will increase the generalization performance (by decreasing\n",
    "the mean absolute error) of the random forest. In fact, a random forest has\n",
    "less chance to suffer from overfitting than AdaBoost when increasing the\n",
    "number of estimators."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}