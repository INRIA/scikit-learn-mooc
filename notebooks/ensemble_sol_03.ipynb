{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extraordinary-clause",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 03\n",
    "\n",
    "This exercise aims at verifying if AdaBoost can over-fit.\n",
    "We will make a grid-search and check the scores by varying the\n",
    "number of estimators.\n",
    "\n",
    "We will first load the California housing dataset and split it into a\n",
    "training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-ranch",
   "metadata": {},
   "source": [
    "Then, create an `AbaBoostRegressor`. Use the function\n",
    "`sklearn.model_selection.validation_curve` to get training and test scores\n",
    "by varying the number of estimators.\n",
    "*Hint: vary the number of estimators between 1 and 60.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "adaboost = AdaBoostRegressor()\n",
    "param_range = np.unique(np.logspace(0, 1.8, num=30).astype(int))\n",
    "train_scores, test_scores = validation_curve(\n",
    "    adaboost, data_train, target_train, param_name=\"n_estimators\",\n",
    "    param_range=param_range, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-player",
   "metadata": {},
   "source": [
    "Plot both the mean training and test scores. You can also plot the\n",
    "standard deviation of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(param_range,\n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 alpha=0.3)\n",
    "plt.fill_between(param_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for AdaBoost regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-patient",
   "metadata": {},
   "source": [
    "Plotting the validation curve, we can see that AdaBoost is not immune against\n",
    "overfitting. Indeed, there is an optimal number of estimators to be found.\n",
    "Adding too many estimators is detrimental for the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-reward",
   "metadata": {},
   "source": [
    "Repeat the experiment using a random forest instead of an AdaBoost regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "train_scores, test_scores = validation_curve(\n",
    "    adaboost, data_train, target_train, param_name=\"n_estimators\",\n",
    "    param_range=param_range, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\")\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(param_range,\n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 alpha=0.3)\n",
    "plt.fill_between(param_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"$R^2$ score\")\n",
    "plt.xlabel(\"# estimators\")\n",
    "_ = plt.title(\"Validation curve for RandomForest regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-might",
   "metadata": {},
   "source": [
    "In contrary to the AdaBoost regressor, we can see that increasing the number\n",
    "trees in the forest will increase the statistical performance of the random\n",
    "forest. In fact, a random forest has less chance to suffer from overfitting\n",
    "than AdaBoost when increasing the number of estimators."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
