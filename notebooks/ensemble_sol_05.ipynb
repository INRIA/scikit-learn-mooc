{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 05\n",
    "\n",
    "The aim of the exercise is to get familiar with the histogram\n",
    "gradient-boosting in scikit-learn. Besides, we will use this model within\n",
    "a cross-validation framework in order to inspect internal parameters found\n",
    "via grid-search.\n",
    "\n",
    "We will use the california housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a histogram gradient boosting regressor. You can set the number\n",
    "of trees to be large enough. Indeed, you fix the parameter such that model\n",
    "will use early-stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hist_gbdt = HistGradientBoostingRegressor(\n",
    "    max_iter=1000, early_stopping=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a grid-search to find some optimal parameter for this model.\n",
    "In this grid-search, you should search for the following parameters:\n",
    "\n",
    "* `max_depth: [3, 8]`;\n",
    "* `max_leaf_nodes: [15, 31]`;\n",
    "* `learning_rate: [0.1, 1]`.\n",
    "\n",
    "Feel free to explore more the space with additional values. Create the\n",
    "grid-search providing the previous gradient boosting instance as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [3, 8],\n",
    "    \"max_leaf_nodes\": [15, 31],\n",
    "    \"learning_rate\": [0.1, 1],\n",
    "}\n",
    "\n",
    "search = GridSearchCV(hist_gbdt, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run our experiment through cross-validation. In this regard,\n",
    "define a 5-fold cross-validation. Besides, be sure to shuffle the the data.\n",
    "Subsequently, use the function `sklearn.model_selection.cross_validate`\n",
    "to run the cross-validation. You should as well set `return_estimator=True`,\n",
    "such that we can investigate the inner model trained via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = cross_validate(\n",
    "    search, X, y, cv=cv, return_estimator=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the results of the cross-validation. First check what is the mean and\n",
    "standard deviation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R2 score with cross-validation:\\n\"\n",
    "      f\"{results['test_score'].mean():.3f} +/- \"\n",
    "      f\"{results['test_score'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then inspect the `estimator` entry of the results and check the best\n",
    "parameters values. Besides, check the number of trees used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in results[\"estimator\"]:\n",
    "    print(estimator.best_params_)\n",
    "    print(f\"# trees: {estimator.best_estimator_.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the parameters are varying. We can get the intuition that\n",
    "results of the inner CV are very close for certain set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results of the inner CV for each estimator of the outer CV.\n",
    "Aggregate the mean test score for each parameter combination and make a box\n",
    "plot of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index_columns = [f\"param_{name}\" for name in params.keys()]\n",
    "columns = index_columns + [\"mean_test_score\"]\n",
    "\n",
    "inner_cv_results = []\n",
    "for cv_idx, estimator in enumerate(results[\"estimator\"]):\n",
    "    search_cv_results = pd.DataFrame(estimator.cv_results_)\n",
    "    search_cv_results = search_cv_results[columns].set_index(index_columns)\n",
    "    search_cv_results = search_cv_results.rename(\n",
    "        columns={\"mean_test_score\": f\"CV {cv_idx}\"})\n",
    "    inner_cv_results.append(search_cv_results)\n",
    "inner_cv_results = pd.concat(inner_cv_results, axis=1).T\n",
    "inner_cv_results.columns = inner_cv_results.columns.to_flat_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=inner_cv_results, orient=\"h\", color=\"tab:blue\", whis=100)\n",
    "ax.set_xlabel(\"R2 score\")\n",
    "ax.set_ylabel(\"Parameters\")\n",
    "_ = ax.set_title(\"Inner CV results with parameters\\n\"\n",
    "                 \"(max_depth, max_leaf_nodes, learning_rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first 4 first ranked set of parameters are very close.\n",
    "Indeed, one would select any of these 4 combinations just due to random\n",
    "variations. It coincides with the results that we observe when inspecting the\n",
    "best parameters of the outer CV."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
