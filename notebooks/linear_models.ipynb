{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "In this notebook we will review linear models from `scikit-learn`.\n",
    "We will :\n",
    "- learn how to fit a simple linear model and interpret the coefficients;\n",
    "- discuss feature augmentation to fit a non-linear function;\n",
    "- use `LinearRegression` and its regularized version `Ridge` which is more\n",
    "  robust;\n",
    "- use `LogisticRegression` with `pipeline`;\n",
    "- see examples of linear separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The over-simplistic toy example\n",
    "To illustrate the main principle of linear regression, we will use a dataset\n",
    "that contains information about penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains measurements taken of penguins. We will formulate the\n",
    "following problem: using the flipper length of a penguin, we would like\n",
    "to infer its mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "feature_names = \"Flipper Length (mm)\"\n",
    "target_name = \"Body Mass (g)\"\n",
    "\n",
    "sns.scatterplot(data=data, x=feature_names, y=target_name)\n",
    "\n",
    "# select the features of interest\n",
    "X = data[[feature_names]].dropna()\n",
    "y = data[target_name].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In this problem, penguin mass is our target. It is a continuous\n",
    "variable that roughly varies between 2700 g and 6300 g. Thus, this is a\n",
    "regression problem (in contrast to classification). We also see that there is\n",
    "almost a linear relationship between the body mass of the penguin and the\n",
    "flipper length. The longer the flipper, the heavier the penguin.\n",
    "\n",
    "Thus, we could come up with a simple formula, where given a flipper length\n",
    "we could compute the body mass of a penguin using a linear relationship of\n",
    "of the form `y = a * x + b` where `a` and `b` are the 2 parameters of our\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_flipper_mass(\n",
    "    flipper_length, weight_flipper_length, intercept_body_mass\n",
    "):\n",
    "    \"\"\"Linear model of the form y = a * x + b\"\"\"\n",
    "    body_mass = weight_flipper_length * flipper_length + intercept_body_mass\n",
    "    return body_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model we defined above, we can check the body mass values\n",
    "predicted for a range of flipper lengths. We will set `weight_flipper_length`\n",
    "to be 45 and `intercept_body_mass` to be -5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass,\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"Compute and plot the prediction.\"\"\"\n",
    "    inferred_body_mass = linear_model_flipper_mass(\n",
    "        flipper_length_range,\n",
    "        weight_flipper_length=weight_flipper_length,\n",
    "        intercept_body_mass=intercept_body_mass,\n",
    "    )\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    sns.scatterplot(data=data, x=feature_names, y=target_name, ax=ax)\n",
    "    ax.plot(\n",
    "        flipper_length_range,\n",
    "        inferred_body_mass,\n",
    "        linewidth=3,\n",
    "        label=(\n",
    "            f\"{weight_flipper_length:.2f} (g / mm) * flipper length + \"\n",
    "            f\"{intercept_body_mass:.2f} (g)\"\n",
    "        ),\n",
    "    )\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "weight_flipper_length = 45\n",
    "intercept_body_mass = -5000\n",
    "\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `weight_flipper_length` is a weight applied to the feature\n",
    "`flipper_length` in\n",
    "order to make the inference. When this coefficient is positive, it means that\n",
    "penguins with longer flipper lengths will have larger body masses.\n",
    "If the coefficient is negative, it means that penguins with shorter flipper\n",
    "flipper lengths have larger body masses. Graphically, this coefficient is\n",
    "represented by the slope of the curve in the plot. Below we show what the\n",
    "curve would look like when the `weight_flipper_length` coefficient is\n",
    "negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "weight_flipper_length = -40\n",
    "intercept_body_mass = 13000\n",
    "\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, this coefficient has a meaningful unit: g/mm.\n",
    "For instance, a coefficient of 40 g/mm, means that for each\n",
    "additional millimeter in flipper length, the body weight predicted will\n",
    "increase by 40 g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_180 = linear_model_flipper_mass(\n",
    "    flipper_length=180, weight_flipper_length=40, intercept_body_mass=0\n",
    ")\n",
    "body_mass_181 = linear_model_flipper_mass(\n",
    "    flipper_length=181, weight_flipper_length=40, intercept_body_mass=0\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The body mass for a flipper length of 180 mm is {body_mass_180} g and \"\n",
    "    f\"{body_mass_181} g for a flipper length of 181 mm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that we have a parameter `intercept_body_mass` in our model.\n",
    "This parameter corresponds to the value on the y-axis if `flipper_length=0`\n",
    "(which in our case is only a mathematical consideration, as in our data,\n",
    " the value of `flipper_length` only goes from 170mm to 230mm). This y-value when  \n",
    "x=0 is called the y-intercept. \n",
    "If `intercept_body_mass` is 0, the curve will\n",
    "pass through the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_flipper_length = 25\n",
    "intercept_body_mass = 0\n",
    "\n",
    "flipper_length_range = np.linspace(0, X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, it will pass through the `intercept_body_mass` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_flipper_length = 45\n",
    "intercept_body_mass = -5000\n",
    "\n",
    "flipper_length_range = np.linspace(0, X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we understand how our model is inferring data, one should ask\n",
    "how do we find the best value for the parameters. Indeed, it seems that we\n",
    "can have many models, depending of the choice of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "for weight, intercept in zip([-40, 45, 90], [15000, -5000, -14000]):\n",
    "    plot_data_and_model(\n",
    "        flipper_length_range, weight, intercept, ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose a model, we could use a metric that indicates how good our model is\n",
    "at fitting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for weight, intercept in zip([-40, 45, 90], [15000, -5000, -14000]):\n",
    "    inferred_body_mass = linear_model_flipper_mass(\n",
    "        X,\n",
    "        weight_flipper_length=weight,\n",
    "        intercept_body_mass=intercept,\n",
    "    )\n",
    "    model_error = mean_squared_error(y, inferred_body_mass)\n",
    "    print(\n",
    "        f\"The following model \\n \"\n",
    "        f\"{weight:.2f} (g / mm) * flipper length + {intercept:.2f} (g) \\n\"\n",
    "        f\"has a mean squared error of: {model_error:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the best model will be the one with the lowest error.\n",
    "Hopefully, this problem of finding the best parameters values\n",
    "(i.e. that result in the lowest error)\n",
    "can be solved without the need to check every\n",
    "potential parameter combination. Indeed, this problem has a closed-form\n",
    "solution: the best parameter values can be found by solving an equation. This\n",
    "avoids the need for brute-force search. This strategy is\n",
    "implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance `linear_regression` will store the parameter values in the\n",
    "attributes `coef_` and `intercept_`. We can check what the optimal model\n",
    "found is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_flipper_length = linear_regression.coef_[0]\n",
    "intercept_body_mass = linear_regression.intercept_\n",
    "\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")\n",
    "\n",
    "inferred_body_mass = linear_regression.predict(X)\n",
    "model_error = mean_squared_error(y, inferred_body_mass)\n",
    "print(f\"The error of the optimal model is {model_error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if your data doesn't have a linear relationship\n",
    "Now, we will define a new problem where the feature and the target are not\n",
    "linearly linked. For instance, we could define `x` to be the years of\n",
    "experience (normalized) and `y` the salary (normalized). Therefore, the\n",
    "problem here would be to infer the salary given the years of experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data generation\n",
    "# fix the seed for reproduction\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_sample = 100\n",
    "x_max, x_min = 1.4, -1.4\n",
    "len_x = (x_max - x_min)\n",
    "x = rng.rand(n_sample) * len_x - len_x/2\n",
    "noise = rng.randn(n_sample) * .3\n",
    "y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(x, y,  color='k', s=9)\n",
    "plt.xlabel('x', size=26)\n",
    "_ = plt.ylabel('y', size=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "In this exercise, you are asked to approximate the target `y` using a linear\n",
    "function `f(x)`. i.e. find the best coefficients of the function `f` in order\n",
    "to minimize the mean squared error. Here you shall find the coefficient manually\n",
    "via trial and error (just as in the previous cells with weight and intercept).\n",
    "\n",
    "Then you can compare the mean squared error of your model with the mean\n",
    "squared error found by `LinearRegression` (which shall be the minimal one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    intercept = 0  # TODO: update the parameters here\n",
    "    weight = 0  # TODO: update the parameters here\n",
    "    y_predict = weight * x + intercept\n",
    "    return y_predict\n",
    "\n",
    "\n",
    "# plot the slope of f\n",
    "grid = np.linspace(x_min, x_max, 300)\n",
    "plt.scatter(x, y, color='k', s=9)\n",
    "plt.plot(grid, f(grid), linewidth=3)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, f(x))\n",
    "print(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1. by fiting a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "linear_regression = linear_model.LinearRegression()\n",
    "# X should be 2D for sklearn\n",
    "X = x.reshape((-1, 1))\n",
    "linear_regression.fit(X, y)\n",
    "\n",
    "# plot the best slope\n",
    "y_best = linear_regression.predict(grid.reshape(-1, 1))\n",
    "plt.plot(grid, y_best, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the coefficients learnt by `LinearRegression` is the best curve that\n",
    "fits the data. We can inspect the coefficients using the attributes of the\n",
    "model learnt as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"best coef: w1 = {linear_regression.coef_[0]:.2f}, \"\n",
    "    f\"best intercept: w0 = {linear_regression.intercept_:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the model learnt will not be able to handle\n",
    "the non-linear relationship between `x` and `y` since linear models assume\n",
    "the relationship between `x` and `y` to be linear. To obtain a better model,\n",
    "we have 3 main solutions:\n",
    "\n",
    "1. choose a model that natively can deal with non-linearity,\n",
    "2. \"augment\" features by including expert knowledge which can be used by\n",
    "   the model, or\n",
    "2. use a \"kernel\" to have a locally-based decision function instead of a\n",
    "   global linear decision function.\n",
    "\n",
    "Let's illustrate quickly the first point by using a decision tree regressor\n",
    "which can natively handle non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(X, y)\n",
    "y_pred = tree.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, tree.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model can handle non-linearity. Instead of having a model\n",
    "which can natively deal with non-linearity, we could also modify our data: we\n",
    "could create new features, derived from the original features, using some\n",
    "expert knowledge. For instance, here we know that we have a cubic and squared\n",
    "relationship between `x` and `y` (because we generated the data). Indeed,\n",
    "we could create two new features (`x^2` and `x^3`) using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([x, x ** 2, x ** 3]).T\n",
    "\n",
    "linear_regression.fit(X, y)\n",
    "\n",
    "grid_augmented = np.vstack([grid, grid ** 2, grid ** 3]).T\n",
    "y_pred = linear_regression.predict(grid_augmented)\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with a linear model, we can overcome the linearity\n",
    "limitation of the model by adding the non-linear component into the design of\n",
    "additional\n",
    "features. Here, we created new feature by knowing the way the target was\n",
    "generated. In practice, this is usually not the case. Instead, one is usually\n",
    "creating interaction between features (e.g. $x_1 * x_2$) with different orders \n",
    "(e.g. $x_1, x_1^2, x_1^3$), at the risk of\n",
    "creating a model with too much expressivity and which might overfit. In\n",
    "scikit-learn, the `PolynomialFeatures` is a transformer to create such\n",
    "feature interactions which we could have used instead of manually creating\n",
    "new features.\n",
    "\n",
    "\n",
    "To demonstrate `PolynomialFeatures`, we are going to use a scikit-learn\n",
    "pipeline which will first create the new features and then fit the model.\n",
    "We come back to scikit-learn pipelines and discuss them in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=3), LinearRegression()\n",
    ")\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, model.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we saw that `PolynomialFeatures` is actually doing the same\n",
    "operation that we did manually above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME: it might be to complex to be introduced here but it seems good in\n",
    "the flow. However, we go away from linear model.**\n",
    "\n",
    "The last possibility to make a linear model more expressive is to use a\n",
    "\"kernel\". Instead of learning a weight per feature as we previously\n",
    "emphasized, a weight will be assign by sample instead. However, not all\n",
    "samples will be used. This is the base of the support vector machine\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel=\"linear\").fit(X, y)\n",
    "y_pred = svr.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, svr.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm can be modified such that it can use non-linear kernel. Then,\n",
    "it will compute interaction between samples using this non-linear\n",
    "interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"poly\", degree=3).fit(X, y)\n",
    "y_pred = svr.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, svr.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, kernel can make a model more expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in higher dimension\n",
    "In the previous example, we only used a single feature. But we have\n",
    "already shown that we could add new feature to make the model more expressive\n",
    "by deriving new features, based on the original feature.\n",
    "\n",
    "Indeed, we could also use additional features (not related to the\n",
    "original feature) and these could help us to predict the target.\n",
    "\n",
    "We will load a dataset about house prices in California.\n",
    "The dataset consists of 8 features regarding the demography and geography of\n",
    "districts in California and the aim is to predict the median house price of\n",
    "each district. We will use all 8 features to predict the target, median\n",
    "house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the score of `LinearRegression` and `Ridge` (which is a\n",
    "regularized version of linear regression).\n",
    "\n",
    "The scorer we will use to evaluate our model is the mean squared error, as in\n",
    "the previous example. The lower the score, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will divide our data into a training set, a validation set and a\n",
    "testing set.\n",
    "The validation set will be used to evaluate selection of the\n",
    "hyper-parameters, while the testing set should only be used to calculate the\n",
    "score of our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "    X, y, random_state=1\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, y_train_valid, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the first example, we did not care about scaling our data in\n",
    "order to keep the original units and have better intuition. However, it is\n",
    "good practice to scale the data such that each feature has a similar standard\n",
    "deviation. It will be even more important if the solver used by the model\n",
    "is a gradient-descent-based solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit(X_train).transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides several tools to preprocess the data. The\n",
    "`StandardScaler` transforms the data such that each feature will have a mean\n",
    "of zero and a standard deviation of 1.\n",
    "\n",
    "This scikit-learn estimator is known as a transformer: it computes some\n",
    "statistics (i.e the mean and the standard deviation) and stores them as\n",
    " attributes (scaler.mean_, scaler.scale_)\n",
    "when calling `fit`. Using these statistics, it\n",
    "transform the data when `transform` is called. Therefore, it is important to\n",
    "note that `fit` should only be called on the training data, similar to\n",
    "classifiers and regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean records on the training set:', scaler.mean_)\n",
    "print('standard deviation records on the training set:', scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, `X_train_scaled` is the data scaled, using the\n",
    "mean and standard deviation of each feature, computed using the training\n",
    "data `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train_scaled, y_train)\n",
    "y_pred = linear_regression.predict(X_valid_scaled)\n",
    "print(\n",
    "    f\"Mean squared error on the validation set: \"\n",
    "    f\"{mean_squared_error(y_valid, y_pred):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling the transformer to transform the data and then calling\n",
    "the regressor, scikit-learn provides a `Pipeline`, which 'chains' the\n",
    "transformer and regressor together. The pipeline allows you to use a\n",
    "sequence of transformer(s) followed by a regressor or a classifier, in one\n",
    "call. (i.e. fitting the pipeline will fit both the transformer(s) and the regressor. \n",
    "Then predicting from the pipeline will first transform the data through the transformer(s)\n",
    "then predict with the regressor from the transformed data)\n",
    "\n",
    "This pipeline exposes the same API as the regressor and classifier\n",
    "and will manage the calls to `fit` and `transform` for you, avoiding any\n",
    "problems with data leakage (when knowledge of the test data was \n",
    "inadvertently included in training a model, as when fitting a transformer\n",
    "on the test data).\n",
    "\n",
    "We already presented `Pipeline` in the second notebook and we will use it\n",
    "here to combine both the scaling and the linear regression.\n",
    "\n",
    "We will can create a `Pipeline` by using `make_pipeline` and giving as\n",
    "arguments the transformation(s) to be performed (in order) and the regressor\n",
    "model.\n",
    "\n",
    "So the two cells above can be reduced to this new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "linear_regression = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "linear_regression.fit(X_train, y_train)\n",
    "y_pred_valid = linear_regression.predict(X_valid)\n",
    "linear_regression_score = mean_squared_error(y_valid, y_pred_valid)\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "\n",
    "print(\n",
    "    f\"Mean squared error on the validation set: \"\n",
    "    f\"{mean_squared_error(y_valid, y_pred_valid):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare this basic `LinearRegression` versus its regularized\n",
    "form `Ridge`.\n",
    "\n",
    "We will tune the parameter `alpha` in `Ridge` and compare the results with\n",
    "the `LinearRegression` model which is not regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), Ridge())\n",
    "\n",
    "list_alphas = np.logspace(-2, 2.1, num=40)\n",
    "list_ridge_scores = []\n",
    "for alpha in list_alphas:\n",
    "    ridge.set_params(ridge__alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_valid)\n",
    "    list_ridge_scores.append(mean_squared_error(y_valid, y_pred))\n",
    "\n",
    "plt.plot(\n",
    "    list_alphas, [linear_regression_score] * len(list_alphas), '--',\n",
    "    label='LinearRegression',\n",
    ")\n",
    "plt.plot(list_alphas, list_ridge_scores, \"+-\", label='Ridge')\n",
    "plt.xlabel('alpha (regularization strength)')\n",
    "plt.ylabel('Mean squared error (lower is better')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, just like adding salt in cooking, adding regularization in our\n",
    "model could improve its error on the validation set. But too much\n",
    "regularization, like too much salt, decreases its performance.\n",
    "\n",
    "We can see visually that the best `alpha` should be around 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = list_alphas[np.argmin(list_ridge_scores)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, we selected this alpha *without* using the testing set ; but\n",
    "instead by using the validation set which is a subset of the training\n",
    "data. This is so we do not \"overfit\" the test data and\n",
    "can be seen in the lesson *basic hyper-parameters tuning*.\n",
    "We can finally compare the performance of the `LinearRegression` model to the\n",
    "best `Ridge` model, on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "ridge.set_params(ridge__alpha=alpha)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "# FIXME add explication why Ridge is not better (equivalent) than linear \n",
    "# regression here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameter search could have been made using `GridSearchCV`\n",
    "instead of manually splitting the training data (into training and\n",
    "validation subsets) and selecting the best alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = GridSearchCV(\n",
    "    make_pipeline(StandardScaler(), Ridge()),\n",
    "    param_grid={\"ridge__alpha\": list_alphas},\n",
    ")\n",
    "ridge.fit(X_train_valid, y_train_valid)\n",
    "print(ridge.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` tests all possible given `alpha` values and picks\n",
    "the best one with a cross-validation scheme. We can now compare with\n",
    "`LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "linear_regression.fit(X_train_valid, y_train_valid)\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to know that several regressors and classifiers\n",
    "in scikit-learn are optimized to make this parameter tuning. They usually\n",
    "finish with the term \"CV\" for \"Cross Validation\" (e.g. `RidgeCV`).\n",
    "They are more efficient than using `GridSearchCV` and you should use them\n",
    "instead.\n",
    "\n",
    "We will repeat the equivalent of the hyper-parameter search but instead of\n",
    "using a `GridSearchCV`, we will use `RidgeCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = make_pipeline(\n",
    "    StandardScaler(), RidgeCV(alphas=[.1, .5, 1, 5, 10, 50, 100])\n",
    ")\n",
    "ridge.fit(X_train_valid, y_train_valid)\n",
    "ridge[-1].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best hyper-parameter value is different because the\n",
    "cross-validation used in the different approach is internally different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "In regression, we saw that the target to be predicted was a continuous\n",
    "variable. In classification, this target will be discrete (e.g. categorical).\n",
    "\n",
    "We will go back to our penguin dataset. However, this time we will try to\n",
    "predict the penguin species using the culmen information. We will also\n",
    "simplify our classification problem by selecting only 2 of the penguin\n",
    "species to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "# select the features of interest\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data = data[culmen_columns + [target_column]]\n",
    "data[target_column] = data[target_column].str.split().str[0]\n",
    "data = data[data[target_column].apply(lambda x: x in (\"Adelie\", \"Chinstrap\"))]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly start by visualizing the feature distribution by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(data=data, hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we have quite a simple problem. When the culmen\n",
    "length increases, the probability that the penguin is a Chinstrap is closer\n",
    "to 1. However, the culmen depth is not helpful for predicting the penguin\n",
    "species.\n",
    "\n",
    "For model fitting, we will separate the target from the data and\n",
    "we will create a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To visualize the separation found by our classifier, we will define an helper\n",
    "function `plot_decision_function`. In short, this function will fit our\n",
    "classifier and plot the edge of the decision function, where the probability\n",
    "to be an Adelie or Chinstrap will be equal (p=0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, title=\"auto\", ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    feature_0_min, feature_0_max = (\n",
    "        X.iloc[:, 0].min() - 1,\n",
    "        X.iloc[:, 0].max() + 1,\n",
    "    )\n",
    "    feature_1_min, feature_1_max = (\n",
    "        X.iloc[:, 1].min() - 1,\n",
    "        X.iloc[:, 1].max() + 1,\n",
    "    )\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(feature_0_min, feature_0_max, plot_step),\n",
    "        np.arange(feature_1_min, feature_1_max, plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    sns.scatterplot(\n",
    "        data=pd.concat([X, y], axis=1),\n",
    "        x=X.columns[0],\n",
    "        y=X.columns[1],\n",
    "        hue=y.name,\n",
    "        ax=ax,\n",
    "    )\n",
    "    if title == \"auto\":\n",
    "        C = clf[-1].C if hasattr(clf[-1], \"C\") else clf[-1].C_\n",
    "        ax.set_title(f\"C={C}\\n with coef={clf[-1].coef_[0]}\")\n",
    "    else:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-penalized logistic regression\n",
    "\n",
    "The linear regression that we previously saw will predict a continuous\n",
    "output. When the target is a binary outcome, one can use the logistic\n",
    "function to model the probability. This model is known as logistic\n",
    "regression.\n",
    "\n",
    "Scikit-learn provides the class `LogisticRegression` which implements this\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(penalty=\"none\")\n",
    ")\n",
    "plot_decision_function(X_train, y_train, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that our decision function is represented by a line separating\n",
    "the 2 classes. Since the line is oblique, it means that we used a\n",
    "combination of both features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_regression[-1].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, both coefficients are non-null.\n",
    "\n",
    "### Apply some regularization when fitting the logistic model\n",
    "\n",
    "The `LogisticRegression` model allows one to apply regularization via the\n",
    "parameter `C`. It would be equivalent to shifting from `LinearRegression`\n",
    "to `Ridge`. Ccontrary to `Ridge`, the value of the\n",
    "`C` parameter is inversely proportional to the regularization strength:\n",
    "a smaller `C` will lead to a more regularized model. We can check the effect\n",
    "of regularization on our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "\n",
    "for ax, C in zip(axs, [0.02, 0.1, 1]):\n",
    "    logistic_regression = make_pipeline(\n",
    "        StandardScaler(), LogisticRegression(C=C)\n",
    "    )\n",
    "    plot_decision_function(\n",
    "        X_train, y_train, logistic_regression, ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more regularized model will make the coefficients tend to 0. Since one of\n",
    "the features is considered less important when fitting the model (lower\n",
    "coefficient magnitude), only one of the feature will be used when C is small.\n",
    "This feature is the culmen length which is in line with our first insight\n",
    "when plotting the marginal feature probabilities.\n",
    "\n",
    "Just like the `RidgeCV` class which automatically finds the optimal `alpha`,\n",
    "one can use `LogisticRegressionCV` to find the best `C` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), LogisticRegressionCV(Cs=[0.01, 0.1, 1, 10])\n",
    ")\n",
    "plot_decision_function(X_train, y_train, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond linear separation\n",
    "\n",
    "As we saw in regression, the linear classification model expects the data\n",
    "to be linearly separable. When this assumption does not hold, the model\n",
    "is not expressive enough to properly fit the data. One needs to apply the\n",
    "same tricks as in regression: feature augmentation (potentially using\n",
    "expert-knowledge) or using a kernel based method.\n",
    "\n",
    "We will provide examples where we will use a kernel support vector machine\n",
    "to perform classification on some toy-datasets where it is impossible to\n",
    "find a perfect linear separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import (\n",
    "    make_moons, make_classification, make_gaussian_quantiles,\n",
    ")\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=.13, random_state=42)\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=500, n_features=2, n_redundant=0, n_informative=2,\n",
    "    random_state=2,\n",
    ")\n",
    "X_gauss, y_gauss = make_gaussian_quantiles(\n",
    "    n_samples=50, n_features=2, n_classes=2, random_state=42,\n",
    ")\n",
    "\n",
    "datasets = [\n",
    "    [pd.DataFrame(X_moons, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_moons, name=\"class\")],\n",
    "    [pd.DataFrame(X_class, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_class, name=\"class\")],\n",
    "    [pd.DataFrame(X_gauss, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_gauss, name=\"class\")],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, nrows=2, figsize=(12, 9))\n",
    "\n",
    "linear_model = make_pipeline(StandardScaler(), SVC(kernel=\"linear\"))\n",
    "kernel_model = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\"))\n",
    "\n",
    "for ax, (X, y) in zip(axs[0], datasets):\n",
    "    plot_decision_function(X, y, linear_model, title=\"Linear kernel\", ax=ax)\n",
    "for ax, (X, y) in zip(axs[1], datasets):\n",
    "    plot_decision_function(X, y, kernel_model, title=\"RBF kernel\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $R^2$ score decreases on each dataset, so we can say that each\n",
    "dataset is \"less linearly separable\" than the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main take away\n",
    "\n",
    "- `LinearRegression` find the best slope which minimize the mean squared\n",
    "  error on the train set\n",
    "- `Ridge` could be better on the test set, thanks to its regularization\n",
    "- `RidgeCV` and `LogisiticRegressionCV` find the best relugarization thanks\n",
    "  to cross validation on the training data\n",
    "- `pipeline` can be used to combinate a scaler and a model\n",
    "- If the data are not linearly separable, we shall use a more complex model\n",
    "  or use feature augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
