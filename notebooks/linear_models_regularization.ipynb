{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organic-gauge",
   "metadata": {},
   "source": [
    "# Regularization of linear regression model\n",
    "\n",
    "In this notebook, we will see the limitations of linear regression models and\n",
    "the advantage of using regularized models instead.\n",
    "\n",
    "Besides, we will also present the preprocessing required when dealing\n",
    "with regularized models, furthermore when the regularization parameter\n",
    "needs to be tuned.\n",
    "\n",
    "We will start by highlighting the over-fitting issue that can arise with\n",
    "a simple linear regression model.\n",
    "\n",
    "## Effect of regularization\n",
    "\n",
    "We will first load the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-chicken",
   "metadata": {},
   "source": [
    "In one of the previous notebook, we showed that linear models could be used\n",
    "even in settings where `data` and `target` are not linearly linked.\n",
    "\n",
    "We showed that one can use the `PolynomialFeatures` transformer to create\n",
    "additional features encoding non-linear interactions between features.\n",
    "\n",
    "Here, we will use this transformer to augment the feature space.\n",
    "Subsequently, we will train a linear regression model. We will use the\n",
    "out-of-sample test set to evaluate the generalization capabilities of our\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = make_pipeline(PolynomialFeatures(degree=2),\n",
    "                                  LinearRegression())\n",
    "cv_results = cross_validate(linear_regression, data, target, cv=10,\n",
    "                            return_train_score=True,\n",
    "                            return_estimator=True)\n",
    "test_score = cv_results[\"test_score\"]\n",
    "print(f\"R2 score of linear regresion model on the test set:\\n\"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-wales",
   "metadata": {},
   "source": [
    "We see that we obtain an $R^2$ score below zero.\n",
    "\n",
    "It means that our model is far worse at predicting the mean of `y_train`.\n",
    "This issue is due to overfitting.\n",
    "We can compute the score on the training set to confirm this intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = cv_results[\"train_score\"]\n",
    "print(f\"R2 score of linear regresion model on the train set:\\n\"\n",
    "      f\"{train_score.mean():.3f} +/- {train_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-manitoba",
   "metadata": {},
   "source": [
    "The score on the training set is much better. This statistical performance\n",
    "gap between the training and testing score is an indication that our model\n",
    "overfitted our training set.\n",
    "\n",
    "Indeed, this is one of the danger when augmenting the number of features\n",
    "with a `PolynomialFeatures` transformer. Our model will focus on some\n",
    "specific features. We can check the weights of the model to have a\n",
    "confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Define the style in the boxplot\n",
    "boxplot_property = {\n",
    "    \"vert\": False, \"whis\": 100, \"patch_artist\": True, \"widths\": 0.3,\n",
    "    \"boxprops\": dict(linewidth=3, color='black', alpha=0.9),\n",
    "    \"medianprops\": dict(linewidth=2.5, color='black', alpha=0.9),\n",
    "    \"whiskerprops\": dict(linewidth=3, color='black', alpha=0.9),\n",
    "    \"capprops\": dict(linewidth=3, color='black', alpha=0.9),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_linear_regression = pd.DataFrame(\n",
    "    [est[-1].coef_ for est in cv_results[\"estimator\"]],\n",
    "    columns=cv_results[\"estimator\"][0][0].get_feature_names(\n",
    "        input_features=data.columns))\n",
    "_, ax = plt.subplots(figsize=(6, 16))\n",
    "weights_linear_regression.plot.box(ax=ax, **boxplot_property)\n",
    "_ = ax.set_title(\"Linear regression coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-journal",
   "metadata": {},
   "source": [
    "We can force the linear regression model to consider all features in a more\n",
    "homogeneous manner. In fact, we could force large positive or negative weight\n",
    "to shrink toward zero. This is known as regularization. We will use a ridge\n",
    "model which enforces such behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(PolynomialFeatures(degree=2),\n",
    "                      Ridge(alpha=100))\n",
    "cv_results = cross_validate(ridge, data, target, cv=10,\n",
    "                            return_train_score=True,\n",
    "                            return_estimator=True)\n",
    "test_score = cv_results[\"test_score\"]\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = cv_results[\"train_score\"]\n",
    "print(f\"R2 score of ridge model on the train set:\\n\"\n",
    "      f\"{train_score.mean():.3f} +/- {train_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-protein",
   "metadata": {},
   "source": [
    "We see that the training and testing scores are much closer, indicating that\n",
    "our model is less overfitting. We can compare the values of the weights of\n",
    "ridge with the un-regularized linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ridge = pd.DataFrame(\n",
    "    [est[-1].coef_ for est in cv_results[\"estimator\"]],\n",
    "    columns=cv_results[\"estimator\"][0][0].get_feature_names(\n",
    "        input_features=data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(12, 16))\n",
    "weights_linear_regression.plot.box(ax=axs[0], **boxplot_property)\n",
    "weights_ridge.plot.box(ax=axs[1], **boxplot_property)\n",
    "axs[1].set_yticklabels([\"\"] * len(weights_ridge.columns))\n",
    "axs[0].set_title(\"Linear regression weights\")\n",
    "_ = axs[1].set_title(\"Ridge weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-portsmouth",
   "metadata": {},
   "source": [
    "We see that the magnitude of the weights are shrunk towards zero in\n",
    "comparison with the linear regression model.\n",
    "\n",
    "However, in this example, we omitted two important aspects: (i) the need to\n",
    "scale the data and (ii) the need to search for the best regularization\n",
    "parameter.\n",
    "\n",
    "## Scale your data!\n",
    "\n",
    "Regularization will add constraints on weights of the model.\n",
    "We saw in the previous example that a ridge model will enforce\n",
    "that all weights have a similar magnitude.\n",
    "\n",
    "Indeed, the larger alpha is, the larger this enforcement will be.\n",
    "\n",
    "This procedure should make us think about feature rescaling.\n",
    "Let's consider the case where features have an identical data dispersion:\n",
    "if two features are found equally important by the model, they will be\n",
    "affected similarly by regularization strength.\n",
    "\n",
    "Now, let's consider the scenario where features have completely different\n",
    "data dispersion (for instance age in years and annual revenue in dollars).\n",
    "If two features are as important, our model will boost the weights of\n",
    "features with small dispersion and reduce the weights of features with\n",
    "high dispersion.\n",
    "\n",
    "We recall that regularization forces weights to be closer. Therefore, we get\n",
    "an intuition that if we want to use regularization, dealing with rescaled\n",
    "data would make it easier to find an optimal regularization parameter and\n",
    "thus an adequate model.\n",
    "\n",
    "As a side note, some solvers based on gradient computation are expecting such\n",
    "rescaled data. Unscaled data will be detrimental when computing the optimal\n",
    "weights. Therefore, when working with a linear model and numerical data, it\n",
    "is generally good practice to scale the data.\n",
    "\n",
    "Thus, we will add a `StandardScaler` in the machine learning pipeline. This\n",
    "scaler will be placed just before the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      Ridge(alpha=0.5))\n",
    "cv_results = cross_validate(ridge, data, target, cv=10,\n",
    "                            return_train_score=True,\n",
    "                            return_estimator=True)\n",
    "test_score = cv_results[\"test_score\"]\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = cv_results[\"train_score\"]\n",
    "print(f\"R2 score of ridge model on the train set:\\n\"\n",
    "      f\"{train_score.mean():.3f} +/- {train_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-cooling",
   "metadata": {},
   "source": [
    "As we can see in this example, using a pipeline simplifies the manual\n",
    "handling.\n",
    "\n",
    "When creating the model, keeping the same `alpha` does not give good results.\n",
    "It depends on the data provided. Therefore, it needs to be tuned for each\n",
    "dataset.\n",
    "\n",
    "In the next section, we will present the steps to tune this parameter.\n",
    "\n",
    "## Fine tuning the regularization parameter\n",
    "\n",
    "As mentioned, the regularization parameter needs to be tuned on each dataset.\n",
    "The default parameter will not lead to the optimal model. Therefore, we need\n",
    "to tune the `alpha` parameter.\n",
    "\n",
    "Model hyperparameters tuning should be done with care. Indeed, we want to\n",
    "find an optimal parameter that maximizes some metrics. Thus, it requires both\n",
    "a training set and testing set.\n",
    "\n",
    "However, this testing set should be different from the out-of-sample testing\n",
    "set that we used to evaluate our model: if we use the same one, we are using\n",
    "an `alpha` which was optimized for this testing set and it breaks the\n",
    "out-of-sample rule.\n",
    "\n",
    "Therefore, we should include search of the hyperparameter `alpha` within the\n",
    "cross-validation. As we saw in previous notebooks, we could use a\n",
    "grid-search. However, some predictor in scikit-learn are available with\n",
    "an integrated hyperparameter search, more efficient than using a grid-search.\n",
    "The name of these predictors finishes by `CV`. In the case of `Ridge`,\n",
    "scikit-learn provides a `RidgeCV` regressor.\n",
    "\n",
    "Therefore, we can use this predictor as the last step of the pipeline.\n",
    "Including the pipeline a cross-validation allows to make a nested\n",
    "cross-validation: the inner cross-validation will search for the best\n",
    "alpha, while the outer cross-validation will give an estimate of the\n",
    "testing score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-2, 0, num=20)\n",
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      RidgeCV(alphas=alphas, store_cv_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, random_state=1)\n",
    "cv_results = cross_validate(ridge, data, target, cv=cv,\n",
    "                            return_train_score=True,\n",
    "                            return_estimator=True, n_jobs=-1)\n",
    "test_score = cv_results[\"test_score\"]\n",
    "print(f\"R2 score of ridge model with optimal alpha on the test set:\\n\"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = cv_results[\"train_score\"]\n",
    "print(f\"R2 score of ridge model on the train set:\\n\"\n",
    "      f\"{train_score.mean():.3f} +/- {train_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-chester",
   "metadata": {},
   "source": [
    "By optimizing `alpha`, we see that the training an testing scores are closed.\n",
    "It indicates that our model is not overfitting.\n",
    "\n",
    "When fitting the ridge regressor, we also requested to store the error found\n",
    "during cross-validation (by setting the parameter `store_cv_values=True`).\n",
    "We will plot the mean MSE for the different `alphas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_alphas = pd.DataFrame(\n",
    "    [est[-1].cv_values_.mean(axis=0) for est in cv_results[\"estimator\"]],\n",
    "    columns=alphas)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "cv_alphas.mean(axis=0).plot(ax=ax, marker=\"+\")\n",
    "ax.set_ylabel(\"Mean squared error\\n (lower is better)\")\n",
    "ax.set_xlabel(\"alpha\")\n",
    "_ = ax.set_title(\"Error obtained by cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-advertising",
   "metadata": {},
   "source": [
    "As we can see, regularization is just like salt in cooking: one must balance\n",
    "its amount to get the best statistical performance. We can check if the best\n",
    "`alpha` found is stable across the cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alphas = [est[-1].alpha_ for est in cv_results[\"estimator\"]]\n",
    "best_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-stereo",
   "metadata": {},
   "source": [
    "In this notebook, you learned about the concept of regularization and\n",
    "the importance of preprocessing and parameter tuning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
