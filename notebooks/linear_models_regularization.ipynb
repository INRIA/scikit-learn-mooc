{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization of linear regression model\n",
    "\n",
    "In this notebook, we will see the limitation of linear regression model and\n",
    "the advantage of using regularized models instead. Besides, we will also\n",
    "present the preprocessing required when dealing with regularized model,\n",
    "furthermore when the regularization parameter needs to be fine tuned.\n",
    "\n",
    "We will start by highlighting the over-fitting issue that can arise with\n",
    "a simple linear regression model.\n",
    "\n",
    "## Effect of regularization\n",
    "\n",
    "We will first load the california housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous exercise, we will use an independent test set to evaluate\n",
    "the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of the previous notebook, we show that linear model could be used\n",
    "even in setting where `X` and `y` are not linearly linked. We showed that one\n",
    "can use the `PolynomialFeatures` transformer to create new feature encoding\n",
    "non-linear interactions between features. Here, we will use this transformer\n",
    "to augment the feature space. Subsequently, we train a linear regresion\n",
    "model. We will use the out-of-sample test set to evaluate the generalization\n",
    "capabilities of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = make_pipeline(PolynomialFeatures(degree=2),\n",
    "                                  LinearRegression())\n",
    "linear_regression.fit(X_train, y_train)\n",
    "test_score = linear_regression.score(X_test, y_test)\n",
    "\n",
    "print(f\"R2 score of linear regresion model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we obtain an $R^2$ score below zero. It means that our model is\n",
    "far worth than predicting the mean of `y_train`. This is issue is due to\n",
    "overfitting. We can compute the score on the training set to confirm this\n",
    "intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = linear_regression.score(X_train, y_train)\n",
    "print(f\"R2 score of linear regresion model on the train set:\\n\"\n",
    "      f\"{train_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training set is much better. This performance gap between\n",
    "the training and testing score is an indication that our model overfitted\n",
    "our training set.\n",
    "\n",
    "Indeed, this is one of the danger when augmenting the number of features\n",
    "with a `PolynomialFeatures` transformer. Our model will focus on some\n",
    "specific features. We can check the weights of the model to have a\n",
    "confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "weights_linear_regression = pd.Series(\n",
    "    linear_regression[-1].coef_,\n",
    "    index=linear_regression[0].get_feature_names(input_features=X.columns))\n",
    "_, ax = plt.subplots(figsize=(6, 16))\n",
    "_ = weights_linear_regression.plot(kind=\"barh\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force the linear regression model to consider all features in a more\n",
    "homogeneous manner. In fact, we could force large positive or negative weight\n",
    "to shrink toward zero. This is known as regularization. We will use a ridge\n",
    "model which enforce such behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(PolynomialFeatures(degree=2),\n",
    "                      Ridge(alpha=0.5))\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = ridge.score(X_train, y_train)\n",
    "print(f\"R2 score of ridge model on the train set:\\n\"\n",
    "      f\"{train_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = ridge.score(X_test, y_test)\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training and testing score are much closer, indicating that\n",
    "our model is less overfitting. We can compare the values of the weights of\n",
    "ridge with the un-regularized linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ridge = pd.Series(\n",
    "    ridge[-1].coef_,\n",
    "    index=ridge[0].get_feature_names(input_features=X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.concat(\n",
    "    [weights_linear_regression, weights_ridge], axis=1,\n",
    "    keys=[\"Linear regression\", \"Ridge\"])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(6, 16))\n",
    "weights.plot(kind=\"barh\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the magnitude of the weights are shrinked towards zero in\n",
    "comparison with the linear regression model.\n",
    "\n",
    "However, in this example, we omitted two important aspects: (i) the need to\n",
    "scale the data and (ii) the need to search for the best regularization\n",
    "parameter.\n",
    "\n",
    "## You shall scale your data\n",
    "\n",
    "Regularization will add constraints on weights of the model. We saw in the\n",
    "previous example that a ridge model will enforce that all weights to have a\n",
    "similar magnitude. Indeed, larger is alpha, larger will be this enforcement.\n",
    "\n",
    "This procedure should make us think about feature rescaling. Let's think\n",
    "about the case where features have an identical data dispersion, if two\n",
    "features are found equally important by the model, they will be affected\n",
    "weights close in term of norm.\n",
    "\n",
    "Now, let's think about the scenario but where features will have completely\n",
    "different data dispersion (e.g. age of person in year and it annual revenue\n",
    "in $). If two features are as important, our model will boost the weight of\n",
    "feature with small dispersion and reduce the weight of the feature with high\n",
    "dispersion. We recall that regularization force weights to be closer.\n",
    "\n",
    "Therefore, we get an intuition that if we want to use regularization, dealing\n",
    "with rescaled data would make it easier to find an optimal regularization\n",
    "parameter and thus an adequate model. As a side note, some solver based on\n",
    "gradient computation are expecting such rescaled data. Unscaled data will be\n",
    "detrimental when computing the optimal weights. Therefore, when working with\n",
    "a linear model and numerical data, this is in general a good practice to\n",
    "scale the data.\n",
    "\n",
    "In the remaining of this section, we will present the basics on how to\n",
    "incorporate a scaler within your machine learning pipeline. Scikit-learn\n",
    "provides several tools to preprocess the data. The `StandardScaler`\n",
    "transforms the data such that each feature will have a mean of zero and a\n",
    "standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit(X_train).transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scikit-learn estimator is known as a transformer: it computes some\n",
    "statistics (i.e the mean and the standard deviation) and stores them as\n",
    "attributes (`scaler.mean_`, `scaler.scale_`) when calling `fit`. Using these\n",
    "statistics, it transform the data when `transform` is called. Therefore, it\n",
    "is important to note that `fit` should only be called on the training data,\n",
    "similar to classifiers and regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean records on the training set:\\n', scaler.mean_)\n",
    "print('standard deviation records on the training set:\\n', scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, `X_train_scaled` is the data scaled, using the\n",
    "mean and standard deviation of each feature, computed using the training\n",
    "data `X_train`.\n",
    "\n",
    "Thus, we can use these scaled dataset to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train_scaled, y_train)\n",
    "test_score = ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling the transformer to transform the data and then calling the\n",
    "regressor, scikit-learn provides a `Pipeline`, which 'chains' the transformer\n",
    "and regressor together. The pipeline allows you to use a sequence of\n",
    "transformer(s) followed by a regressor or a classifier, in one call. (i.e.\n",
    "fitting the pipeline will fit both the transformer(s) and the regressor. Then\n",
    "predicting from the pipeline will first transform the data through the\n",
    "transformer(s) then predict with the regressor from the transformed data)\n",
    "\n",
    "This pipeline exposes the same API as the regressor and classifier and will\n",
    "manage the calls to `fit` and `transform` for you, avoiding any problems with\n",
    "data leakage (when knowledge of the test data was inadvertently included in\n",
    "training a model, as when fitting a transformer on the test data).\n",
    "\n",
    "We already use such `Pipeline` to create the polynomial features before to\n",
    "train the model.\n",
    "\n",
    "We will can create a `Pipeline` by using `make_pipeline` and giving as\n",
    "arguments the transformation(s) to be performed (in order) and the regressor\n",
    "model.\n",
    "\n",
    "Here, we can integrate the scaling phase before to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      Ridge(alpha=0.5))\n",
    "ridge.fit(X_train, y_train)\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we see the benefit of using a pipeline. It\n",
    "simplifies the manual handling.\n",
    "\n",
    "When creating the model, keeping the same `alpha` does not give good results.\n",
    "It depends on the data provided. Therefore, it needs to be tuned for each\n",
    "dataset.\n",
    "\n",
    "In the next section, we will present the steps to tune the parameters.\n",
    "\n",
    "## Fine tuning the regularization parameter\n",
    "\n",
    "As mentioned, the regularization parameter needs to be tuned on each dataset.\n",
    "The default parameter will not lead to the optimal model. Therefore, we need\n",
    "to tune the `alpha` parameter.\n",
    "\n",
    "Tuning model hyperparameter should be done with care. Indeed, we want to find\n",
    "an optimal parameter that maximize some metrics. Thus, it requires a training\n",
    "and testing sets. However, this testing set should be different from the\n",
    "out-of-sample testing set that we used to evaluate our model. If we use\n",
    "the same test, we are using an `alpha` which was optimized for this testing\n",
    "set and it breaks the out-of-sample rule.\n",
    "\n",
    "Therefore, we can split our previous training set into two subsets: a\n",
    "new training set and a validation set allowing to later pick the optimal\n",
    "alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub_train, X_valid, y_sub_train, y_valid = train_test_split(\n",
    "    X_train, y_train, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.logspace(-10, -1, num=30)\n",
    "list_ridge_scores = []\n",
    "for alpha in alphas:\n",
    "    ridge.set_params(ridge__alpha=alpha)\n",
    "    ridge.fit(X_sub_train, y_sub_train)\n",
    "    list_ridge_scores.append(ridge.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, list_ridge_scores, \"+-\", label='Ridge')\n",
    "plt.xlabel('alpha (regularization strength)')\n",
    "plt.ylabel('R2 score (higher is better)')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, just like adding salt in cooking, adding regularization in our\n",
    "model could improve its error on the validation set. But too much\n",
    "regularization, like too much salt, decreases its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmax(list_ridge_scores)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrain a ridge model on the full training set and set the alpha and\n",
    "check the score on the left out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      Ridge(alpha=best_alpha))\n",
    "ridge.fit(X_train, y_train)\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise, you will use a scikit-learn estimator which allows to\n",
    "make some parameters tuning instead of programming yourself a `for` loop by\n",
    "hand.\n",
    "\n",
    "As a conclusion, you learnt in this notebook about the concept of\n",
    "regularization and the importance of preprocessing and parameter tuning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
