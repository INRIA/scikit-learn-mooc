{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 04\n",
    "\n",
    "In the previous notebook, we illustrated how a regularization parameter of\n",
    "ridge model need to be optimized by hand. However, this way of optimizing\n",
    "hyperparameters is not effective: only a single split was used while\n",
    "cross-validation could be used.\n",
    "\n",
    "This exercise will make you implement the same search but using the class\n",
    "`GridSearchCV`.\n",
    "\n",
    "First, we will:\n",
    "\n",
    "* load the california housing dataset;\n",
    "* split the data into a training and testing set;\n",
    "* create a machine learning pipeline composed of a standard scaler to\n",
    "  normalize the data, and a ridge regression as a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the exercise is to use a `GridSearchCV` estimator to optimize the\n",
    "parameter `alpha` of the ridge regressor. We redefine all the regularization\n",
    "parameter candidates that we saw in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.logspace(-1, 2, num=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refer to the documentation of `GridSearchCV`\n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "to check how to use this estimator.\n",
    "First, train the grid-search using the previous machine learning pipeline and\n",
    "check what is the value of the best parameters found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to check the name of the parameter to tune in the pipeline.\n",
    "We can use `get_params()` to know more about the available parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in ridge.get_params().keys():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(ridge, param_grid={\"ridge__alpha\": alphas})\n",
    "search.fit(X_train, y_train)\n",
    "print(\n",
    "    f\"Best alpha found via the grid-search:\\n\"\n",
    "    f\"{search.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that you found the best parameter `alpha`, use the grid-search estimator\n",
    "that you created to predict and estimate the $R^2$ score of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R2 score of the optimal ridge is:\\n\"\n",
    "    f\"{search.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to know that several regressors and classifiers\n",
    "in scikit-learn are optimized to make this parameter tuning. They usually\n",
    "finish with the term \"CV\" for \"Cross Validation\" (e.g. `RidgeCV`).\n",
    "They are more efficient than using `GridSearchCV` and you should use them\n",
    "instead.\n",
    "\n",
    "Repeat the previous exercise but using `RidgeCV` estimator instead of a\n",
    "grid-search. Refer to the documentation of `RidgeCV` to have more information\n",
    "on how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), RidgeCV(alphas=alphas))\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\n",
    "    f\"Best alpha found via the grid-search:\\n\"\n",
    "    f\"{ridge[-1].alpha_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R2 score of the optimal ridge is:\\n\"\n",
    "    f\"{ridge.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
