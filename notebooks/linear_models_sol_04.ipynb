{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcc3 Solution for Exercise M4.04\n",
    "\n",
    "In the previous notebook, we saw the effect of applying some regularization\n",
    "on the coefficient of a linear model.\n",
    "\n",
    "In this exercise, we will study the advantage of using some regularization\n",
    "when dealing with correlated features.\n",
    "\n",
    "We will first create a regression dataset. This dataset will contain 2,000\n",
    "samples and 5 features from which only 2 features will be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "data, target, coef = make_regression(\n",
    "    n_samples=2_000,\n",
    "    n_features=5,\n",
    "    n_informative=2,\n",
    "    shuffle=False,\n",
    "    coef=True,\n",
    "    random_state=0,\n",
    "    noise=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating the dataset, `make_regression` returns the true coefficient\n",
    "used to generate the dataset. Let's plot this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = [\n",
    "    \"Relevant feature #0\",\n",
    "    \"Relevant feature #1\",\n",
    "    \"Noisy feature #0\",\n",
    "    \"Noisy feature #1\",\n",
    "    \"Noisy feature #2\",\n",
    "]\n",
    "coef = pd.Series(coef, index=feature_names)\n",
    "coef.plot.barh()\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LinearRegression` regressor and fit on the entire dataset and\n",
    "check the value of the coefficients. Are the coefficients of the linear\n",
    "regressor close to the coefficients used to generate the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data, target)\n",
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Relevant feature #0\",\n",
    "    \"Relevant feature #1\",\n",
    "    \"Noisy feature #0\",\n",
    "    \"Noisy feature #1\",\n",
    "    \"Noisy feature #2\",\n",
    "]\n",
    "coef = pd.Series(linear_regression.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We see that the coefficients are close to the coefficients used to generate\n",
    "the dataset. The dispersion is indeed cause by the noise injected during the\n",
    "dataset generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a new dataset that will be the same as `data` with 4 additional\n",
    "columns that will repeat twice features 0 and 1. This procedure will create\n",
    "perfectly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "import numpy as np\n",
    "\n",
    "data = np.concatenate([data, data[:, [0, 1]], data[:, [0, 1]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit again the linear regressor on this new dataset and check the\n",
    "coefficients. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data, target)\n",
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Relevant feature #0\",\n",
    "    \"Relevant feature #1\",\n",
    "    \"Noisy feature #0\",\n",
    "    \"Noisy feature #1\",\n",
    "    \"Noisy feature #2\",\n",
    "    \"First repetition of feature #0\",\n",
    "    \"First repetition of  feature #1\",\n",
    "    \"Second repetition of  feature #0\",\n",
    "    \"Second repetition of  feature #1\",\n",
    "]\n",
    "coef = pd.Series(linear_regression.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We see that the coefficient values are far from what one could expect.\n",
    "By repeating the informative features, one would have expected these\n",
    "coefficients to be similarly informative.\n",
    "\n",
    "Instead, we see that some coefficients have a huge norm ~1e14. It indeed\n",
    "means that we try to solve an mathematical ill-posed problem. Indeed, finding\n",
    "coefficients in a linear regression involves inverting the matrix\n",
    "`np.dot(data.T, data)` which is not possible (or lead to high numerical\n",
    "errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ridge regressor and fit on the same dataset. Check the coefficients.\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(data, target)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(ridge.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We see that the penalty applied on the weights give a better results: the\n",
    "values of the coefficients do not suffer from numerical issues. Indeed, the\n",
    "matrix to be inverted internally is `np.dot(data.T, data) + alpha * I`.\n",
    "Adding this penalty `alpha` allow the inversion without numerical issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find the relationship between the ridge coefficients and the original\n",
    "coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "ridge.coef_[:5] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Repeating three times each informative features induced to divide the\n",
    "ridge coefficients by three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p>We advise to always use a penalty to shrink the magnitude of the weights\n",
    "toward zero (also called \"l2 penalty\"). In scikit-learn, <tt class=\"docutils literal\">LogisticRegression</tt>\n",
    "applies such penalty by default. However, one needs to use <tt class=\"docutils literal\">Ridge</tt> (and even\n",
    "<tt class=\"docutils literal\">RidgeCV</tt> to tune the parameter <tt class=\"docutils literal\">alpha</tt>) instead of <tt class=\"docutils literal\">LinearRegression</tt>.</p>\n",
    "<p class=\"last\">Other kinds of regularizations exist but will not be covered in this course.</p>\n",
    "</div>\n",
    "\n",
    "## Dealing with correlation between one-hot encoded features\n",
    "\n",
    "In this section, we will focus on how to deal with correlated features that\n",
    "arise naturally when one-hot encoding categorical features.\n",
    "\n",
    "Let's first load the Ames housing dataset and take a subset of features that\n",
    "are only categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ames_housing = pd.read_csv(\"../datasets/house_prices.csv\", na_values='?')\n",
    "ames_housing = ames_housing.drop(columns=\"Id\")\n",
    "\n",
    "categorical_columns = [\"Street\", \"Foundation\", \"CentralAir\", \"PavedDrive\"]\n",
    "target_name = \"SalePrice\"\n",
    "X, y = ames_housing[categorical_columns], ames_housing[target_name]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "We previously presented that a `OneHotEncoder` creates as many columns as\n",
    "categories. Therefore, there is always one column (i.e. one encoded category)\n",
    "that can be inferred from the others. Thus, `OneHotEncoder` creates\n",
    "collinear features.\n",
    "\n",
    "We illustrate this behaviour by considering the \"CentralAir\" feature that\n",
    "contains only two categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "X_train[\"CentralAir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "single_feature = [\"CentralAir\"]\n",
    "encoder = OneHotEncoder(sparse=False, dtype=np.int32)\n",
    "X_trans = encoder.fit_transform(X_train[single_feature])\n",
    "X_trans = pd.DataFrame(\n",
    "    X_trans,\n",
    "    columns=encoder.get_feature_names_out(input_features=single_feature),\n",
    ")\n",
    "X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "Here, we see that the encoded category \"CentralAir_N\" is the opposite of the\n",
    "encoded category \"CentralAir_Y\". Therefore, we observe that using a\n",
    "`OneHotEncoder` creates two features having the problematic pattern observed\n",
    "earlier in this exercise. Training a linear regression model on such a\n",
    "of one-hot encoded binary feature can therefore lead to numerical\n",
    "problems, especially without regularization. Furthermore, the two one-hot\n",
    "features are redundant as they encode exactly the same information in\n",
    "opposite ways.\n",
    "\n",
    "Using regularization helps to overcome the numerical issues that we highlighted\n",
    "earlier in this exercise.\n",
    "\n",
    "Another strategy is to arbitrarily drop one of the encoded categories.\n",
    "Scikit-learn provides such an option by setting the parameter `drop` in the\n",
    "`OneHotEncoder`. This parameter can be set to `first` to always drop the\n",
    "first encoded category or `binary_only` to only drop a column in the case of\n",
    "binary categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop=\"first\", sparse=False, dtype=np.int32)\n",
    "X_trans = encoder.fit_transform(X_train[single_feature])\n",
    "X_trans = pd.DataFrame(\n",
    "    X_trans,\n",
    "    columns=encoder.get_feature_names_out(input_features=single_feature),\n",
    ")\n",
    "X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "We see that only the second column of the previous encoded data is kept.\n",
    "Dropping one of the one-hot encoded column is a common practice,\n",
    "especially for binary categorical features. Note however that this breaks\n",
    "symmetry between categories and impacts the number of coefficients of the\n",
    "model, their values, and thus their meaning, especially when applying\n",
    "strong regularization.\n",
    "\n",
    "Let's finally illustrate how to use this option is a machine-learning pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(OneHotEncoder(drop=\"first\", dtype=np.int32), Ridge())\n",
    "model.fit(X_train, y_train)\n",
    "n_categories = [X_train[col].nunique() for col in X_train.columns]\n",
    "print(\n",
    "    f\"R2 score on the testing set: {model.score(X_test, y_test):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Our model contains {model[-1].coef_.size} features while \"\n",
    "    f\"{sum(n_categories)} categories are originally available.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}