{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 05\n",
    "In the previous notebook, we presented a non-penalized logistic regression\n",
    "classifier. This classifier accepts a parameter `penalty` to add a\n",
    "regularization. The regularization strength is set using the parameter `C`.\n",
    "\n",
    "In this exercise, we ask you to train a l2-penalized logistic regression\n",
    "classifier and to find by yourself the effect of the parameter `C`.\n",
    "\n",
    "We will start by loading the dataset and create the helper function to show\n",
    "the decision separation as in the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "# only keep the Adelie and Chinstrap classes\n",
    "data = data.set_index(\"Species\").loc[[\"Adelie\", \"Chinstrap\"]].reset_index()\n",
    "\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0,\n",
    ")\n",
    "range_features = {\n",
    "    feature_name: (X[feature_name].min() - 1, X[feature_name].max() + 1)\n",
    "    for feature_name in X\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(fitted_classifier, range_features, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[1])\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following candidate for the parameter `C`, find out what is the\n",
    "effect of the value of this parameter on the decision boundary and on the\n",
    "weights magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Cs = [0.01, 0.1, 1, 10]\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(penalty=\"l2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "_, axs = plt.subplots(ncols=4, sharey=True, sharex=True, figsize=(16, 4))\n",
    "\n",
    "weights_ridge = []\n",
    "for ax, C in zip(axs, Cs):\n",
    "    logistic_regression.set_params(logisticregression__C=C)\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    # plot the decision function\n",
    "    plot_decision_function(logistic_regression, range_features, ax=ax)\n",
    "    sns.scatterplot(\n",
    "        x=X_test.iloc[:, 0], y=X_test.iloc[:, 1], hue=y_test,\n",
    "        palette=[\"tab:red\", \"tab:blue\"], ax=ax)\n",
    "    ax.set_title(f\"C: {C}\")\n",
    "    # store the weights\n",
    "    weights_ridge.append(pd.Series(\n",
    "        logistic_regression[-1].coef_.ravel(), index=X.columns))\n",
    "plt.subplots_adjust(wspace=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ridge = pd.concat(\n",
    "    weights_ridge, axis=1, keys=[f\"C: {C}\" for C in Cs])\n",
    "_ = weights_ridge.plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a small `C` will shrink the weights values toward zero. It means\n",
    "that a small `C` provides a more regularized model. Thus, `C` is the inverse\n",
    "of the `alpha` coefficient in the `Ridge` model.\n",
    "\n",
    "Besides, with a strong penalty (i.e. small `C` value), the weight of the\n",
    "feature \"Culmen Depth (mm)\" is almost zero. It explains why the decision\n",
    "separation in the plot is almost perpendicular to the \"Culmen Length (mm)\"\n",
    "feature."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
