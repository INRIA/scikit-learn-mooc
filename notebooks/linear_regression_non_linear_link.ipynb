{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with non-linear link between data and target\n",
    "\n",
    "In the previous exercise, you were asked to train a linear regression model\n",
    "on a dataset where the matrix `X` and the target `y` do not have a linear\n",
    "link.\n",
    "\n",
    "In this notebook, we show that even if the parametrization of linear models\n",
    "is not natively adapated to data with non-linearity, it is still possible\n",
    "to make linear model more flexible and expressive.\n",
    "\n",
    "To illustrate these concepts, we will reuse the same dataset generated in the\n",
    "previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_sample = 100\n",
    "x_max, x_min = 1.4, -1.4\n",
    "len_x = (x_max - x_min)\n",
    "x = rng.rand(n_sample) * len_x - len_x / 2\n",
    "sorted_idx = np.argsort(x)\n",
    "noise = rng.randn(n_sample) * .3\n",
    "y = x ** 3 - 0.5 * x ** 2 + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "To ease the plotting, we will create a Pandas dataframe containing the data\n",
    "and target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame({\"x\": x, \"y\": y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "_ = sns.scatterplot(data=data, x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will highlight the limitations of fitting a linear regression model as\n",
    "done in the previous exercise.\n",
    "\n",
    "```{warning}\n",
    "In scikit-learn, by convention `X` should be a 2D matrix of shape\n",
    "`(n_samples, n_features)`. If `X` is a 1D vector, you need to reshape it\n",
    "into a matrix with a single column if the vector represents a feature or a\n",
    "single row if the vector represents a sample.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "# X should be 2D for sklearn\n",
    "X = x.reshape((-1, 1))\n",
    "linear_regression.fit(X, y)\n",
    "\n",
    "y_pred = linear_regression.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "ax = sns.scatterplot(data=data, x=\"x\", y=\"y\")\n",
    "ax.plot(x, y_pred, color=\"tab:orange\")\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the coefficients learnt by `LinearRegression` is the best \"straight\n",
    "line\" that fits the data. We can inspect the coefficients using the\n",
    "attributes of the model learnt as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"weight: {linear_regression.coef_[0]:.2f}, \"\n",
    "      f\"intercept: {linear_regression.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the model learnt will not be able to handle\n",
    "the non-linear relationship between `x` and `y` since linear models assume\n",
    "the relationship between `x` and `y` to be linear.\n",
    "\n",
    "Indeed, there is 3 possibilities to alleviate this issue:\n",
    "\n",
    "1. choose a model that natively can deal with non-linearity,\n",
    "2. \"augment\" features by including expert knowledge which can be used by\n",
    "   the model, or\n",
    "3. use a \"kernel\" to have a locally-based decision function instead of a\n",
    "   global linear decision function.\n",
    "\n",
    "Let's illustrate quickly the first point by using a decision tree regressor\n",
    "which can natively handle non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(X, y)\n",
    "y_pred = tree.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "ax = sns.scatterplot(data=data, x=\"x\", y=\"y\")\n",
    "ax.plot(x[sorted_idx], y_pred[sorted_idx], color=\"tab:orange\")\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model can handle non-linearity. Instead of having a model\n",
    "which can natively deal with non-linearity, we could also modify our data: we\n",
    "could create new features, derived from the original features, using some\n",
    "expert knowledge. For instance, here we know that we have a cubic and squared\n",
    "relationship between `x` and `y` (because we generated the data). Indeed,\n",
    "we could create two new features (`x^2` and `x^3`) using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([x, x ** 2, x ** 3]).T\n",
    "\n",
    "linear_regression.fit(X, y)\n",
    "y_pred = linear_regression.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "ax = sns.scatterplot(data=data, x=\"x\", y=\"y\")\n",
    "ax.plot(x[sorted_idx], y_pred[sorted_idx],\n",
    "        linewidth=4, color=\"tab:orange\")\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with a linear model, we can overcome the linearity\n",
    "limitation of the model by adding the non-linear component into the design of\n",
    "additional features. Here, we created new feature by knowing the way the\n",
    "target was generated. In practice, this is usually not the case.\n",
    "\n",
    "Instead, one is usually creating interaction between features (e.g. $x_1\n",
    "\\times x_2$) with different orders (e.g. $x_1, x_1^2, x_1^3$), at the risk of\n",
    "creating a model with too much flexibility where the polynomial terms allows\n",
    "to fit noise in the dataset and thus lead overfit. In scikit-learn, the\n",
    "`PolynomialFeatures` is a transformer to create such feature interactions\n",
    "which we could have used instead of manually creating new features.\n",
    "\n",
    "To demonstrate `PolynomialFeatures`, we are going to use a scikit-learn\n",
    "pipeline which will first create the new features and then fit the model.\n",
    "We come back to scikit-learn pipelines and discuss them in more detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=3),\n",
    "                      LinearRegression())\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "ax = sns.scatterplot(data=data, x=\"x\", y=\"y\")\n",
    "ax.plot(x[sorted_idx], y_pred[sorted_idx], color=\"tab:orange\")\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we saw that `PolynomialFeatures` is actually doing the same\n",
    "operation that we did manually above.\n",
    "\n",
    "The last possibility is to make a linear model more expressive is to use a\n",
    "\"kernel\". Instead of learning a weight per feature as we previously\n",
    "emphasized, a weight will be assign by sample instead. However, not all\n",
    "samples will be used. This is the base of the support vector machine\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel=\"linear\")\n",
    "svr.fit(X, y)\n",
    "y_pred = svr.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "ax = sns.scatterplot(data=data, x=\"x\", y=\"y\")\n",
    "ax.plot(x[sorted_idx], y_pred[sorted_idx], color=\"tab:orange\")\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm can be modified such that it can use non-linear kernel. Then,\n",
    "it will compute interaction between samples using this non-linear\n",
    "interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"poly\", degree=3)\n",
    "svr.fit(X, y)\n",
    "y_pred = svr.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "ax = sns.scatterplot(data=data, x=\"x\", y=\"y\")\n",
    "ax.plot(x[sorted_idx], y_pred[sorted_idx], color=\"tab:orange\")\n",
    "_ = ax.set_title(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method supporting kernel, as SVM, allows to efficiently create a non-linear\n",
    "model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
