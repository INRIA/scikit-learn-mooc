{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond linear separation in classification\n",
    "\n",
    "As we saw in regression, the linear classification model expects the data\n",
    "to be linearly separable. When this assumption does not hold, the model\n",
    "is not expressive enough to properly fit the data. One needs to apply the\n",
    "same tricks as in regression: feature augmentation (potentially using\n",
    "expert-knowledge) or using a kernel based method.\n",
    "\n",
    "We will provide examples where we will use a kernel support vector machine\n",
    "to perform classification on some toy-datasets where it is impossible to\n",
    "find a perfect linear separation.\n",
    "\n",
    "First, we redefine our plotting utility to show the decision boundary of a\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(fitted_classifier, range_features, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate some synthetic data with special pattern which are known to\n",
    "be non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import (\n",
    "    make_moons, make_classification, make_gaussian_quantiles,\n",
    ")\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=.13, random_state=42)\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=500, n_features=2, n_redundant=0, n_informative=2,\n",
    "    random_state=2,\n",
    ")\n",
    "X_gauss, y_gauss = make_gaussian_quantiles(\n",
    "    n_samples=500, n_features=2, n_classes=2, random_state=42,\n",
    ")\n",
    "\n",
    "datasets = [\n",
    "    [pd.DataFrame(X_moons, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_moons, name=\"class\")],\n",
    "    [pd.DataFrame(X_class, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_class, name=\"class\")],\n",
    "    [pd.DataFrame(X_gauss, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_gauss, name=\"class\")],\n",
    "]\n",
    "range_features = {\"Feature #0\": (-5, 5), \"Feature #1\": (-5, 5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first visualize the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, sharey=True, figsize=(14, 4))\n",
    "\n",
    "for ax, (X, y) in zip(axs, datasets):\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y,\n",
    "                    palette=[\"tab:red\", \"tab:blue\"], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting these three datasets, it is clear that a linear model cannot\n",
    "separate the two classes. Now, we will train a SVC classifier where we will\n",
    "use a linear kernel to show the limitation of such linear model on the\n",
    "following dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "linear_model = make_pipeline(StandardScaler(), SVC(kernel=\"linear\"))\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, sharey=True, figsize=(14, 4))\n",
    "for ax, (X, y) in zip(axs, datasets):\n",
    "    linear_model.fit(X, y)\n",
    "    plot_decision_function(linear_model, range_features, ax=ax)\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y,\n",
    "                    palette=[\"tab:red\", \"tab:blue\"], ax=ax)\n",
    "    ax.set_title(f\"Accuracy: {linear_model.score(X, y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the linear model parametrization is not enough to adapt the\n",
    "synthetic dataset.\n",
    "\n",
    "Now, we will fit an SVC with an RBF kernel that will handle the non-linearity\n",
    "using the kernel trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_model = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\"))\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, sharey=True, figsize=(14, 4))\n",
    "for ax, (X, y) in zip(axs, datasets):\n",
    "    kernel_model.fit(X, y)\n",
    "    plot_decision_function(kernel_model, range_features, ax=ax)\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y,\n",
    "                    palette=[\"tab:red\", \"tab:blue\"], ax=ax)\n",
    "    ax.set_title(f\"Accuracy: {kernel_model.score(X, y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this later case, we can see that the accuracy is close to be perfect and\n",
    "that the decision boundary is non-linear. Thus, kernel trick or data\n",
    "augmentation are the tricks to make a linear classifier more expressive."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
