{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of your predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Machine-learning models rely on optimizing an objective function, by seeking\n",
    "its minimum or maximum. It is important to understand that this objective\n",
    "function is usually decoupled from the evaluation metric that we want to\n",
    "optimize in practice. The objective function serves as a proxy to the\n",
    "evaluation metric.\n",
    "FIXME: add information about a loss function depending of the notebooks\n",
    "presented before the notebook about metrics.\n",
    "\n",
    "While other notebooks will give insights regarding algorithms and their\n",
    "associated objective functions, in this notebook we will focus on the\n",
    "metrics used to evaluate the performance of a predictive model.\n",
    "\n",
    "Selecting an evaluation metric will mainly depend on the model chosen to\n",
    "solve our datascience problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "We can recall that in a classification setting, the target `y` is categorical\n",
    "rather than continuous. We will use the blood transfusion dataset that will\n",
    "be fetched from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    name=\"blood-transfusion-service-center\",\n",
    "    as_frame=True, return_X_y=True,\n",
    ")\n",
    "# Make columns and classes more human-readable\n",
    "X.columns = [\"Recency\", \"Frequency\", \"Monetary\", \"Time\"]\n",
    "y = y.apply(\n",
    "    lambda x: \"donated\" if x == \"2\" else \"not donated\"\n",
    ").astype(\"category\")\n",
    "y.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target `y` contains 2 categories corresponding to whether\n",
    "or not a subject gave blood or not. We will use a logistic regression\n",
    "classifier to predict this outcome.\n",
    "\n",
    "First, we split the data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, random_state=0, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data are split, we can learn a logistic regression classifier solely\n",
    "on the training data, keeping the testing data for the evaluation of the\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that our classifier is trained, we can provide some information about a\n",
    "subject and the classifier can predict whether or not the subject will donate\n",
    "blood.\n",
    "\n",
    "Let's create a synthetic sample corresponding to the following potential new\n",
    "donor: he/she donated blood 6 month ago and gave twice blood in the past for\n",
    "a total of 1000 c.c. He/she gave blood for the first time 20 months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_donor = [[6, 2, 1000, 20]]\n",
    "classifier.predict(new_donor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these information, our classifier predicted that this synthetic subject\n",
    "is more likely to not donate blood. However, we have no possibility to ensure\n",
    "if the prediction is correct or not. That's why, we can now use the testing\n",
    "set for this purpose. First, we can predict whether or not a subject will\n",
    "give blood with the help of the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as a baseline\n",
    "Now that we have these predictions, we could compare them with the true\n",
    "predictions (sometimes called ground-truth) which we did not use up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test == y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comparison above, a `True` value means that the value predicted by our\n",
    "classifier is identical to the real `prediction` while a `False` means that\n",
    "our classifier made a mistake. One way to get an overall statistic telling us\n",
    "how good the performance of our classifier are is to compute the number of\n",
    "time our classifier was right and divide it by the number of samples in our\n",
    "set (i.e. taking the mean of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(y_test == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure is also known as the accuracy. Here, our classifier is 78%\n",
    "accurate at classifying if subject will give blood. `scikit-learn` provides a\n",
    "function to compute this metric in the module `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also have a build-in method named `score` which compute by\n",
    "default the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix and derived metrics\n",
    "The comparison that we did above and the accuracy that we deducted did not\n",
    "take into account which type of error our classifier was doing. The accuracy\n",
    "is an aggregate of the error. However, we might be interested in a lower\n",
    "granularity level to know separately the error for the two following case:\n",
    "- we predicted that a person will give blood but she/he is not;\n",
    "- we predicted that a person will not give blood but she/he is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-diagonal numbers are related to predictions that agree\n",
    "with the true labels while off-diagonal numbers are related to\n",
    "misclassification. Besides, we now know the type of true or erroneous\n",
    "predictions the classifier did:\n",
    "\n",
    "* the top left corner is called true positive (TP) and correspond to a person\n",
    "  who gave blood and was predicted as such by the classifier;\n",
    "* the bottom right corner is called the true negative (TN) and correspond to\n",
    "  a person who did not gave blood and was predicted as such by the\n",
    "  classifier;\n",
    "* the top right corner is called false negative (FN) and correspond to a\n",
    "  person who gave blood but was predicted as not giving blood;\n",
    "* the bottom left corner is called false positive (FP) and correspond to a\n",
    "  person who did not give blood but was predicted as giving blood.\n",
    "\n",
    "Once we have split these information, we can compute statistics for\n",
    "highlighting the performance of our classifier in a particular setting. For\n",
    "instance, one could be interested in the fraction of persons who really gave\n",
    "blood when the classifier predicted so or the fraction of people predicted as\n",
    "giving blood among the total population that actually did so.\n",
    "\n",
    "The former statistic is known as the precision defined as TP / (TP + FP)\n",
    "while the latter statistic is known as the recall defined as TP / (TP + FN)\n",
    "We could, similarly than with the accuracy, manually compute these values.\n",
    "But scikit-learn provides functions to compute these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\n",
    "    f\"Precision score: {precision_score(y_test, y_pred, pos_label='donated')}\"\n",
    "    f\"\\nRecall score: {recall_score(y_test, y_pred, pos_label='donated')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are in line with what we could see in the confusion matrix.\n",
    "In the left column, more than half of the predictions were corrected leading\n",
    "to a precision above 0.5. However, our classifier mislabeled a lot of persons\n",
    "who gave blood as \"not donated\" leading to a very low recall of around 0.1.\n",
    "\n",
    "The precision and recall can be combined in a single score called the F1\n",
    "score (which is the harmonic mean of precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred, pos_label='donated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The issue of class imbalance\n",
    "At this stage, we could ask ourself a reasonable question. While the accuracy\n",
    "did not look bad (i.e. 77%), the F1 score is relatively low (i.e. 21%).\n",
    "\n",
    "As we mentioned, precision and recall only focus on the positive label while\n",
    "the accuracy is taking both aspects into account. In addition,\n",
    "we omit to look at the ratio class\n",
    "occurrence. We could check this ratio in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = pd.Series(Counter(y_train))\n",
    "class_counts /= class_counts.sum()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can observed that the positive class `'donated'` is only 24% of the\n",
    "total number of instances. The good accuracy of our classifier is then linked\n",
    "to its capability of predicting correctly the negative class `'not donated'`\n",
    "which could be relevant or not depending of the application. We can\n",
    "illustrate the issue using a dummy classifier as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(\n",
    "    strategy=\"constant\", constant=\"not donated\"\n",
    ")\n",
    "dummy_classifier.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dummy classifier will always predict the negative class `'not donated'`.\n",
    "We obtain an accuracy score of 76%. Therefore, it means that this classifier,\n",
    "without learning anything from the data `X` is capable of predicting as\n",
    "accurately than our logistic regression. 76% represents the baseline that\n",
    "any classifier should overperform to not be a random classifier.\n",
    "\n",
    "The problem illustrated above is also known as the class imbalance problem\n",
    "where the accuracy should not be used. In this case, one should either use\n",
    "the precision, recall, or F1 score as presented above or the balanced\n",
    "accuracy score instead of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balanced accuracy is equivalent to the accuracy in the context of\n",
    "balanced classes. It is defined as the average recall obtained on each class.\n",
    "\n",
    "### Evaluation with different probability threshold\n",
    "\n",
    "All statistics that we presented up to now rely on `classifier.predict` which\n",
    "provide the most likely label. However, we don't use the probability\n",
    "associated with this prediction or in other words how sure are the classifier\n",
    "confident about this prediction. By default, the prediction of a classifier\n",
    "correspons to a thresholding at a 0.5 probability, in a binary classification\n",
    "problem. We can quickly check this relationship with the classifier that\n",
    "we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = pd.DataFrame(\n",
    "    classifier.predict_proba(X_test),\n",
    "    columns=classifier.classes_\n",
    ")\n",
    "y_proba[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since probabilities sum to 1 we can get the class with the highest\n",
    "# probability without using the threshold 0.5\n",
    "equivalence_pred_proba = (\n",
    "    y_proba.idxmax(axis=1).to_numpy() == y_pred\n",
    ")\n",
    "np.all(equivalence_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default decision threshold (0.5) might not be the best threshold leading\n",
    "to optimal performance of our classifier. In this case, one can vary the\n",
    "decision threshold and therefore the underlying prediction and compute the\n",
    "same statistic than presented earlier. Usually, two metrics are computed and\n",
    "reported as a curve. Each metric is belonging to a graph axis and a point on\n",
    "the graph corresponds to a specific decision threshold. Let's start by\n",
    "computing the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "y_pred = classifier.predict_proba(X_test)\n",
    "pos_label = \"donated\"\n",
    "precision, recall, threshold = precision_recall_curve(\n",
    "    y_test, y_pred[:, 0], pos_label=pos_label,\n",
    ")\n",
    "average_precision = average_precision_score(\n",
    "    y_test, y_pred[:, 0], pos_label=pos_label,\n",
    ")\n",
    "plt.plot(\n",
    "    recall, precision,\n",
    "    color=\"tab:orange\", linewidth=3,\n",
    "    marker=\".\", markerfacecolor=\"tab:blue\", markeredgecolor=\"tab:blue\",\n",
    "    label=f\"Average Precision: {average_precision:.2f}\",\n",
    ")\n",
    "plt.xlabel(f\"Recall\\n (Positive label: {pos_label})\")\n",
    "plt.ylabel(f\"Precision\\n (Positive label: {pos_label})\")\n",
    "plt.legend()\n",
    "\n",
    "# # FIXME: to be used when solved in scikit-learn\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "# disp = plot_precision_recall_curve(\n",
    "#     classifier, X_test, y_test, pos_label='donated',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this curve, each blue dot correspond to a certain level of probability\n",
    "which we used as a decision threshold. We can see that by varying this\n",
    "decision threshold, we get different compromise precision vs. recall.\n",
    "\n",
    "A perfect classifier is expected to have a precision at 1 even when varying\n",
    "the recall. A metric characterizing the curve is linked to the area under the\n",
    "curve (AUC), named averaged precision. With a ideal classifier, the\n",
    "average precision will be 1.\n",
    "\n",
    "While the precision and recall metric focuses on the positive class, one\n",
    "might be interested into the compromise between performance to discriminate\n",
    "positive and negative classes. The statistics used in this case are the\n",
    "sensitivity and specificity. The sensitivity is just another denomination for\n",
    "recall. However, the specificity measures the proportion of well classified\n",
    "samples from the negative class defined as TN / (TN + FP). Similarly to the\n",
    "precision-recall curve, sensitivity and specificity are reported with a curve\n",
    "called the receiver operating characteristic (ROC) curve. We will show such\n",
    "curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred[:, 0], pos_label=pos_label)\n",
    "# FIXME: roc_auc_score has a bug and we need to give the inverse probability\n",
    "# vector. Should be changed when the following is merged and released:\n",
    "# https://github.com/scikit-learn/scikit-learn/pull/17594\n",
    "roc_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "plt.plot(\n",
    "    fpr, tpr,\n",
    "    color=\"tab:orange\", linewidth=3,\n",
    "    marker=\".\", markerfacecolor=\"tab:blue\", markeredgecolor=\"tab:blue\",\n",
    "    label=f\"ROC-AUC: {roc_auc:.2f}\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"tab:green\", label=\"Chance\")\n",
    "plt.xlabel(f\"1 - Specificity\\n (Positive label: {pos_label})\")\n",
    "plt.ylabel(f\"Sensitivity\\n (Positive label: {pos_label})\")\n",
    "plt.legend()\n",
    "\n",
    "# # FIXME: to be used when solved in scikit-learn\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# plot_roc_curve(classifier, X_test, y_test, pos_label='donated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This curve is built with the same principle than with the precision-recall\n",
    "curve: we vary the probability threshold to compute \"hard\" prediction and\n",
    "compute the metrics. As with the precision-recall curve as well, we can\n",
    "compute the area under the ROC (ROC-AUC) to characterize the performance of\n",
    "our classifier. However, this is important to observer that the lower bound\n",
    "of the ROC-AUC is 0.5. Indeed, we represented the performance of a dummy\n",
    "classifier (i.e. green dashed line) to show that the worse performance\n",
    "obtained will always be above this line.\n",
    "\n",
    "### Link between confusion matrix, precision-recall curve and ROC curve\n",
    "\n",
    "TODO: ipywidgets to play with interactive curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(classifier, X_test, y_test, pos_label,\n",
    "                  probability_threshold, ax):\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    precision, recall, threshold = precision_recall_curve(\n",
    "        y_test, y_pred[:, 0], pos_label=pos_label,\n",
    "    )\n",
    "    average_precision = average_precision_score(\n",
    "        y_test, y_pred[:, 0], pos_label=pos_label,\n",
    "    )\n",
    "    ax.plot(\n",
    "        recall, precision,\n",
    "        color=\"tab:orange\", linewidth=3,\n",
    "        label=f\"Average Precision: {average_precision:.2f}\",\n",
    "    )\n",
    "    threshold_idx = np.searchsorted(\n",
    "        threshold, probability_threshold,\n",
    "    )\n",
    "    ax.plot(\n",
    "        recall[threshold_idx], precision[threshold_idx],\n",
    "        color=\"tab:blue\", marker=\".\", markersize=10,\n",
    "    )\n",
    "    ax.plot(\n",
    "        [recall[threshold_idx], recall[threshold_idx]],\n",
    "        [0, precision[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        [0, recall[threshold_idx]],\n",
    "        [precision[threshold_idx], precision[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.set_xlabel(f\"Recall\")\n",
    "    ax.set_ylabel(f\"Precision\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(classifier, X_test, y_test, pos_label,\n",
    "                   probability_threshold, ax):\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred[:, 0], pos_label=pos_label)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "    ax.plot(\n",
    "        fpr, tpr,\n",
    "        color=\"tab:orange\", linewidth=3,\n",
    "        label=f\"ROC-AUC: {roc_auc:.2f}\"\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"tab:green\", label=\"Chance\")\n",
    "    threshold_idx = np.searchsorted(\n",
    "        threshold[::-1], probability_threshold,\n",
    "    )\n",
    "    threshold_idx = len(threshold) - threshold_idx - 1\n",
    "    ax.plot(\n",
    "        fpr[threshold_idx], tpr[threshold_idx],\n",
    "        color=\"tab:blue\", marker=\".\", markersize=10,\n",
    "    )\n",
    "    ax.plot(\n",
    "        [fpr[threshold_idx], fpr[threshold_idx]],\n",
    "        [0, tpr[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        [0, fpr[threshold_idx]],\n",
    "        [tpr[threshold_idx], tpr[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.set_xlabel(f\"1 - Specificity\")\n",
    "    ax.set_ylabel(f\"Sensitivity\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_with_threshold(classifier, X_test, y_test, pos_label,\n",
    "                                         probability_threshold, ax):\n",
    "    from itertools import product\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    class_idx = np.where(classifier.classes_ == pos_label)[0][0]\n",
    "    n_classes = len(classifier.classes_)\n",
    "\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    y_pred = (y_pred[:, class_idx] > probability_threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(\n",
    "        (y_test == pos_label).astype(int), y_pred,\n",
    "    )\n",
    "    im_ = ax.imshow(cm, interpolation='nearest')\n",
    "\n",
    "    text_ = None\n",
    "    cmap_min, cmap_max = im_.cmap(0), im_.cmap(256)\n",
    "\n",
    "    text_ = np.empty_like(cm, dtype=object)\n",
    "\n",
    "    # print text with appropriate color depending on background\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "\n",
    "    for i, j in product(range(n_classes), range(n_classes)):\n",
    "        color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "\n",
    "        text_cm = format(cm[i, j], '.2g')\n",
    "        if cm.dtype.kind != 'f':\n",
    "            text_d = format(cm[i, j], 'd')\n",
    "            if len(text_d) < len(text_cm):\n",
    "                text_cm = text_d\n",
    "\n",
    "        text_[i, j] = ax.text(\n",
    "            j, i, text_cm, ha=\"center\", va=\"center\", color=color\n",
    "        )\n",
    "\n",
    "    ax.set(\n",
    "        xticks=np.arange(n_classes),\n",
    "        yticks=np.arange(n_classes),\n",
    "        xticklabels=classifier.classes_[[int(not bool(class_idx)), class_idx]],\n",
    "        yticklabels=classifier.classes_[[int(not bool(class_idx)), class_idx]],\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_roc(threshold):\n",
    "    # FIXME: we could optimize the plotting by only updating the the\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(21, 6))\n",
    "    plot_pr_curve(\n",
    "        classifier, X_test, y_test, pos_label=\"donated\",\n",
    "        probability_threshold=threshold, ax=axs[0],\n",
    "    )\n",
    "    plot_roc_curve(\n",
    "        classifier, X_test, y_test, pos_label=\"donated\",\n",
    "        probability_threshold=threshold, ax=axs[1]\n",
    "    )\n",
    "    plot_confusion_matrix_with_threshold(\n",
    "        classifier, X_test, y_test, pos_label=\"donated\",\n",
    "        probability_threshold=threshold, ax=axs[2]\n",
    "    )\n",
    "    fig.suptitle(\"Overall performance with positive class 'donated'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_roc_interactive():\n",
    "    from ipywidgets import interactive, FloatSlider\n",
    "    slider = FloatSlider(min=0, max=1, step=0.01, value=0.5)\n",
    "    return interactive(plot_pr_roc, threshold=slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_roc_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Unlike in the classification problem, the target `y` is a continuous\n",
    "variable in regression problem. Therefore, the classification metrics can be\n",
    "used to evaluate the performance of a model. Instead, there exists a set of\n",
    "metric dedicated to regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    (\"https://raw.githubusercontent.com/christophM/interpretable-ml-book/\"\n",
    "     \"master/data/bike.csv\"),\n",
    ")\n",
    "# rename the columns with human-readable names\n",
    "data = data.rename(columns={\n",
    "    \"yr\": \"year\", \"mnth\": \"month\", \"temp\": \"temperature\", \"hum\": \"humidity\",\n",
    "    \"cnt\": \"count\", \"days_since_2011\": \"days since 2011\"\n",
    "})\n",
    "# convert the categorical columns with a proper category data type\n",
    "for col in data.columns:\n",
    "    if data[col].dtype.kind == \"O\":\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "\n",
    "# separate the target from the original data\n",
    "X = data.drop(columns=[\"count\"])\n",
    "y = data[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=50, density=True)\n",
    "plt.xlabel(\"Number of bike rentals\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Target distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem can be formulated as follow: we would like to infer the number of\n",
    "bike rentals from data related to the current day. The number of bike rentals\n",
    "is a number that can vary in the interval [0, infinity) (if the number of\n",
    "bike available is infinite). As in the previous section, we will train a\n",
    "model and we will evaluate its performance by introducing the different\n",
    "regression metrics.\n",
    "\n",
    "First, we split the data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "We will use a random forest as a model. However, we first need to check the\n",
    "type of data that we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some features are numeric, some have been tagged as `category`. These\n",
    "features need to be encoded in a proper way such that our random forest can\n",
    "deal with them. The simplest solution is to use an `OrdinalEncoder`.\n",
    "Regarding, the numerical features, we don't need to do anything. Thus, we\n",
    "will create a preprocessing steps to take care about this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_selector = selector(dtype_include=\"category\")\n",
    "preprocessor = make_column_transformer(\n",
    "    (OrdinalEncoder(), categorical_selector),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(\n",
    "    preprocessor.fit_transform(X_train),\n",
    "    columns=(\n",
    "        categorical_selector(X_train) +\n",
    "        [col for col in X_train.columns\n",
    "         if col not in categorical_selector(X_train)]\n",
    "    )\n",
    ")\n",
    "X_train_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to have some insights about the preprocessing, we manually preprocessed\n",
    "the training data and we can observe that the original strings were encoded\n",
    "with numbers. We can now create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = make_pipeline(preprocessor, RandomForestRegressor())\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for classifiers, regressors have a `score` method which will compute the\n",
    ":math:`R^2` score (also known as the coefficient of determination) by\n",
    "default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The :math:`R^2` score represents the proportion of variance of the target\n",
    "explained by the independent variables in the model. The best score possible\n",
    "is 1 but there is no lower bound. However, a model which would predict the\n",
    "expected value of the target would get a score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regressor.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The :math:`R^2` score gives insights regarding the goodness of fit of the\n",
    "model. However, this score cannot be compared from one dataset to another and\n",
    "the value obtained does not have a meaningful interpretation regarding the\n",
    "original unit of the target. If we want to get such interpretable score, we\n",
    "will be interested into the median or mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\n",
    "    f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.0f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computing the mean absolute error, we can interpret that our model is\n",
    "predicting in average 507 bike rentals away from the truth. The mean can be\n",
    "impacted by large error while for some application, we would like to discard\n",
    "them and we can in this case opt for the median absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "print(\n",
    "    f\"Median absolute error: {median_absolute_error(y_test, y_pred):.0f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our model make an error of 405 bikes.\n",
    "FIXME: **not sure how to introduce the `mean_squared_error`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In addition of metrics, we can visually represent the results by plotting\n",
    "the predicted values versus the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_true, y_pred, title=None):\n",
    "    plt.scatter(y_true, y_pred)\n",
    "\n",
    "    max_value = np.max([y_true.max(), y_pred.max()])\n",
    "    plt.plot(\n",
    "        [0, max_value],\n",
    "        [0, max_value],\n",
    "        color=\"tab:orange\",\n",
    "        linewidth=3,\n",
    "        label=\"Perfect fit\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"True values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.legend()\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, the perfect prediction will lay on the diagonal line. This plot\n",
    "allows to detect if the model have a specific regime where our model does not\n",
    "work as expected or has some kinda of bias.\n",
    "\n",
    "Let's take an example using the house prices in Ames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "X, y = fetch_openml(name=\"house_prices\", as_frame=True, return_X_y=True)\n",
    "X = X.select_dtypes(np.number).drop(\n",
    "    columns=[\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a ridge regressor on the data and plot the prediction versus the\n",
    "actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), RidgeCV())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred, title=\"House prices in Ames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, we see that for the large \"True values\", our model tend to\n",
    "under-estimate the price of the house. Typically, this issue arises when\n",
    "the target to predict does not follow a normal distribution and the model\n",
    "could benefit of an intermediate target transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model_transformed_target = TransformedTargetRegressor(\n",
    "    regressor=model,\n",
    "    transformer=QuantileTransformer(\n",
    "        n_quantiles=900, output_distribution=\"normal\"\n",
    "    ),\n",
    ")\n",
    "model_transformed_target.fit(X_train, y_train)\n",
    "y_pred = model_transformed_target.predict(X_test)\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred, title=\"House prices in Ames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, once we transformed the target, we see that we corrected some of the\n",
    "high values.\n",
    "\n",
    "## Summary\n",
    "In this notebook, we presented the metrics and plots useful to evaluate and\n",
    "get insights about models. We both focus on regression and classification\n",
    "problems."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2e973fb4cd3f423c809178329e222694": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_48d2541858504c44bf2c31292b152f9a",
        "IPY_MODEL_b3b31f1ce58f499790ed556809124c17"
       ],
       "layout": "IPY_MODEL_bca0f21489854654a399ec8e31147915"
      }
     },
     "342dcc730a9f47ae81aed939504a642a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "48d2541858504c44bf2c31292b152f9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": true,
       "description": "threshold",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_eb988ec7640b437ba4dee621466653f9",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.01,
       "style": "IPY_MODEL_342dcc730a9f47ae81aed939504a642a",
       "value": 0.5
      }
     },
     "491306f974644a86b4307e0941cceb7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b3b31f1ce58f499790ed556809124c17": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_491306f974644a86b4307e0941cceb7c",
       "msg_id": "",
       "outputs": []
      }
     },
     "bca0f21489854654a399ec8e31147915": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb988ec7640b437ba4dee621466653f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
