{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of your predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Machine-learning models rely on optimizing an objective function, by seeking\n",
    "its minimum or maximum. It is important to understand that this objective\n",
    "function is usually decoupled from the evaluation metric that we want to\n",
    "optimize in practice. The objective function serves as a proxy for the\n",
    "evaluation metric.\n",
    "FIXME: add information about a loss function depending of the notebooks\n",
    "presented before the notebook about metrics.\n",
    "\n",
    "While other notebooks will give insight about machine-learning algorithms and\n",
    "their associated objective functions, in this notebook we will focus on the\n",
    "metrics used to evaluate the performance of a predictive model.\n",
    "\n",
    "Evaluation metric selection will mainly depend on the model chosen to\n",
    "solve our data science problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "We can recall that in a classification setting, the target `y` is categorical\n",
    "rather than continuous. We will use the blood transfusion dataset that will\n",
    "be fetched from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    name=\"blood-transfusion-service-center\",\n",
    "    as_frame=True, return_X_y=True,\n",
    ")\n",
    "# Make columns and classes more human-readable\n",
    "X.columns = [\"Recency\", \"Frequency\", \"Monetary\", \"Time\"]\n",
    "y = y.apply(\n",
    "    lambda x: \"donated\" if x == \"2\" else \"not donated\"\n",
    ").astype(\"category\")\n",
    "y.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target `y` contains 2 categories corresponding to whether\n",
    "or not a subject gave blood or not. We will use a logistic regression\n",
    "classifier to predict this outcome.\n",
    "\n",
    "First, we split the data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, random_state=0, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data are split, we can learn a logistic regression classifier using\n",
    "only the training data, keeping the testing data for evaluation of the\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our classifier is trained, we can provide some information about a\n",
    "subject and the classifier can predict whether or not the subject will donate\n",
    "blood.\n",
    "\n",
    "Let's create a synthetic sample for a new potential\n",
    "donor: he/she donated blood 6 months ago and has given a total of 1000 c.c.\n",
    "of blood, twice in the past. He/she gave blood for the first time 20\n",
    "months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_donor = [[6, 2, 1000, 20]]\n",
    "classifier.predict(new_donor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, our classifier predicts that this synthetic subject\n",
    "is more likely to not donate blood again. However, we cannot check if the\n",
    "prediction is correct or not (we do not know the true target value). That's\n",
    "the purpose of the testing set. First, we predict whether or not a\n",
    "subject will give blood with the help of the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as a baseline\n",
    "Now that we have these predictions, we can compare them with the true\n",
    "predictions (sometimes called ground-truth) which we did not use up to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test == y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comparison above, a `True` value means that the value predicted by our\n",
    "classifier is identical to the real `prediction` while a `False` means that\n",
    "our classifier made a mistake. One way to get an overall statistic that tells\n",
    "us how good the performance of our classifier is, is to compute the number of\n",
    "times our classifier was right and divide it by the number of samples in our\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(y_test == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure is also known as the accuracy. Here, our classifier is 78%\n",
    "accurate at classifying if a subject will give blood. `scikit-learn` provides\n",
    "a function that computes this metric in the module `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also has a method named `score`, built into\n",
    "`LogisticRegression`, which computes the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix and derived metrics\n",
    "The comparison that we did above and the accuracy that we calculated did not\n",
    "take into account the type of error our classifier was making. Accuracy\n",
    "is an aggregate of the errors made by the classifier. We may be interested\n",
    "in finer granularity - to know independently what the error is for each of\n",
    "the two following cases:\n",
    "- we predicted that a person will give blood but she/he did not;\n",
    "- we predicted that a person will not give blood but she/he did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-diagonal numbers are related to predictions that were correct\n",
    "while off-diagonal numbers are related to incorrect predictions\n",
    "(misclassifications). We now know the four types of correct and erroneous\n",
    "predictions:\n",
    "\n",
    "* the top left corner are true positives (TP) and corresponds to people\n",
    "  who gave blood and was predicted as such by the classifier;\n",
    "* the bottom right corner are true negatives (TN) and correspond to\n",
    "  a people who did not give blood and was predicted as such by the\n",
    "  classifier;\n",
    "* the top right corner are false negatives (FN) and correspond to\n",
    "  people who gave blood but was predicted to not have given blood;\n",
    "* the bottom left corner are false positives (FP) and correspond to\n",
    "  people who did not give blood but was predicted to have given blood.\n",
    "\n",
    "Once we have split this information, we can compute statistics tp\n",
    "highlight the performance of our classifier in a particular setting. For\n",
    "instance, we could be interested in the fraction of people who really gave\n",
    "blood when the classifier predicted so or the fraction of people predicted\n",
    "to have given blood out of the total population that actually did so.\n",
    "\n",
    "The former statistic, known as the precision, is defined as TP / (TP + FP)\n",
    "and represents how likely the person actually gave blood when the classifier\n",
    "predicted that they did.\n",
    "The latter statistic, known as the recall, defined as TP / (TP + FN) and\n",
    "assesses how well the classifier is able to correctly identify people who\n",
    "did give blood.\n",
    "We could, similar to accuracy, manually compute these values\n",
    "but scikit-learn provides functions to compute these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\n",
    "    f\"Precision score: {precision_score(y_test, y_pred, pos_label='donated')}\"\n",
    "    f\"\\nRecall score: {recall_score(y_test, y_pred, pos_label='donated')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are in line with what was seen in the confusion matrix.\n",
    "Looking at the left column, more than half of the \"donated\" predictions were\n",
    "correct, leading\n",
    "to a precision above 0.5. However, our classifier mislabeled a lot of people\n",
    "who gave blood as \"not donated\", leading to a very low recall of around 0.1.\n",
    "\n",
    "The precision and recall can be combined in a single score called the F1\n",
    "score (which is the harmonic mean of precision and recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred, pos_label='donated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The issue of class imbalance\n",
    "At this stage, we could ask ourself a reasonable question. While the accuracy\n",
    "did not look bad (i.e. 77%), the F1 score is relatively low (i.e. 21%).\n",
    "\n",
    "As we mentioned, precision and recall only focuses on samples predicted to\n",
    "be positive, while\n",
    "accuracy takes both into account. In addition,\n",
    "we did not look at the ratio of classes (labels).\n",
    "We could check this ratio in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_counts = pd.Series(Counter(y_train))\n",
    "class_counts /= class_counts.sum()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the positive class, `'donated'`, comprises only 24% of\n",
    "the of the samples. The good accuracy of our classifier is then linked\n",
    "to its ability to predict correctly the negative class `'not donated'`\n",
    "which may or may not be relevant, depending on the application. We can\n",
    "illustrate the issue using a dummy classifier as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(\n",
    "    strategy=\"constant\", constant=\"not donated\"\n",
    ")\n",
    "dummy_classifier.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dummy classifier, which always predicts the negative class\n",
    "`'not donated'`,\n",
    "we obtain an accuracy score of 76%. Therefore, it means that this classifier,\n",
    "without learning anything from the data `X`, is capable of predicting as\n",
    "accurately as our logistic regression model.\n",
    "\n",
    "The problem illustrated above is also known as the class imbalance problem.\n",
    "When the classes are imbalanced, accuracy should not be used. In this case,\n",
    "one should either use\n",
    "the precision, recall, or F1 score as presented above or the balanced\n",
    "accuracy score instead of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balanced accuracy is equivalent to accuracy in the context of\n",
    "balanced classes. It is defined as the average recall obtained on each class.\n",
    "\n",
    "### Evaluation and different probability thresholds\n",
    "\n",
    "All statistics that we presented up to now rely on `classifier.predict` which\n",
    "outputs the most likely label. We haven't made use use of the probability\n",
    "associated with this prediction, which gives the confidence of the\n",
    "classifier in this prediction. By default, the prediction of a classifier\n",
    "corresponds to a threshold of 0.5 probability in a binary classification\n",
    "problem. We can quickly check this relationship with the classifier that\n",
    "we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = pd.DataFrame(\n",
    "    classifier.predict_proba(X_test),\n",
    "    columns=classifier.classes_\n",
    ")\n",
    "y_proba[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since probabilities sum to 1 we can get the class with the highest\n",
    "# probability without using the threshold 0.5.\n",
    "equivalence_pred_proba = (\n",
    "    y_proba.idxmax(axis=1).to_numpy() == y_pred\n",
    ")\n",
    "np.all(equivalence_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default decision threshold (0.5) might not be the best threshold that\n",
    "leads to optimal performance of our classifier. In this case, one can vary\n",
    "the decision threshold, and therefore the underlying prediction, and compute\n",
    "the same statistics presented earlier. Usually, the two metrics recall and\n",
    "precision are computed and plotted on a graph. Each metric plotted on a\n",
    "graph axis and each point on\n",
    "the graph corresponds to a specific decision threshold. Let's start by\n",
    "computing the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "y_pred = classifier.predict_proba(X_test)\n",
    "pos_label = \"donated\"\n",
    "precision, recall, threshold = precision_recall_curve(\n",
    "    y_test, y_pred[:, 0], pos_label=pos_label,\n",
    ")\n",
    "average_precision = average_precision_score(\n",
    "    y_test, y_pred[:, 0], pos_label=pos_label,\n",
    ")\n",
    "plt.plot(\n",
    "    recall, precision,\n",
    "    color=\"tab:orange\", linewidth=3,\n",
    "    marker=\".\", markerfacecolor=\"tab:blue\", markeredgecolor=\"tab:blue\",\n",
    "    label=f\"Average Precision: {average_precision:.2f}\",\n",
    ")\n",
    "plt.xlabel(f\"Recall\\n (Positive label: {pos_label})\")\n",
    "plt.ylabel(f\"Precision\\n (Positive label: {pos_label})\")\n",
    "plt.legend()\n",
    "\n",
    "# # FIXME: to be used when solved in scikit-learn\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "# disp = plot_precision_recall_curve(\n",
    "#     classifier, X_test, y_test, pos_label='donated',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this curve, each blue dot corresponds to a level of probability\n",
    "which we used as a decision threshold. We can see that by varying this\n",
    "decision threshold, we get different precision vs. recall values.\n",
    "\n",
    "A perfect classifier would have a precision of 1 for all recall\n",
    "values. A metric characterizing the curve is linked to the area under the\n",
    "curve (AUC) and is named average precision. With an ideal classifier, the\n",
    "average precision would be 1.\n",
    "\n",
    "The precision and recall metric focuses on the positive class however, one\n",
    "might be interested in the compromise between accurately discriminating the\n",
    "positive class and accurately discriminating the negative classes. The\n",
    "statistics used for this are sensitivity and specificity. Sensitivity is just\n",
    "another name for recall. However, specificity measures the proportion of\n",
    "correctly classified samples in the negative class defined as:\n",
    "TN / (TN + FP). Similar to the precision-recall curve, sensitivity and\n",
    "specificity are generally plotted as a curve called the receiver operating\n",
    "characteristic (ROC) curve. Below is such a curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred[:, 0], pos_label=pos_label)\n",
    "# FIXME: roc_auc_score has a bug and we need to give the inverse probability\n",
    "# vector. Should be changed when the following is merged and released:\n",
    "# https://github.com/scikit-learn/scikit-learn/pull/17594\n",
    "roc_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "plt.plot(\n",
    "    fpr, tpr,\n",
    "    color=\"tab:orange\", linewidth=3,\n",
    "    marker=\".\", markerfacecolor=\"tab:blue\", markeredgecolor=\"tab:blue\",\n",
    "    label=f\"ROC-AUC: {roc_auc:.2f}\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"tab:green\", label=\"Chance\")\n",
    "plt.xlabel(f\"1 - Specificity\\n (Positive label: {pos_label})\")\n",
    "plt.ylabel(f\"Sensitivity\\n (Positive label: {pos_label})\")\n",
    "plt.legend()\n",
    "\n",
    "# # FIXME: to be used when solved in scikit-learn\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# plot_roc_curve(classifier, X_test, y_test, pos_label='donated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This curve was built using the same principle as the precision-recall\n",
    "curve: we vary the probability threshold for determining \"hard\" prediction\n",
    "and compute the metrics. As with the precision-recall curve, we can\n",
    "compute the area under the ROC (ROC-AUC) to characterize the performance of\n",
    "our classifier. However, it is important to observer that the lower bound\n",
    "of the ROC-AUC is 0.5. Indeed, we show the performance of a dummy\n",
    "classifier (the green dashed line) to show that the even worst performance\n",
    "obtained will always be above this line.\n",
    "\n",
    "### Link between confusion matrix, precision-recall curve and ROC curve\n",
    "\n",
    "TODO: ipywidgets to play with interactive curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(classifier, X_test, y_test, pos_label,\n",
    "                  probability_threshold, ax):\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    precision, recall, threshold = precision_recall_curve(\n",
    "        y_test, y_pred[:, 0], pos_label=pos_label,\n",
    "    )\n",
    "    average_precision = average_precision_score(\n",
    "        y_test, y_pred[:, 0], pos_label=pos_label,\n",
    "    )\n",
    "    ax.plot(\n",
    "        recall, precision,\n",
    "        color=\"tab:orange\", linewidth=3,\n",
    "        label=f\"Average Precision: {average_precision:.2f}\",\n",
    "    )\n",
    "    threshold_idx = np.searchsorted(\n",
    "        threshold, probability_threshold,\n",
    "    )\n",
    "    ax.plot(\n",
    "        recall[threshold_idx], precision[threshold_idx],\n",
    "        color=\"tab:blue\", marker=\".\", markersize=10,\n",
    "    )\n",
    "    ax.plot(\n",
    "        [recall[threshold_idx], recall[threshold_idx]],\n",
    "        [0, precision[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        [0, recall[threshold_idx]],\n",
    "        [precision[threshold_idx], precision[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.set_xlabel(f\"Recall\")\n",
    "    ax.set_ylabel(f\"Precision\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(classifier, X_test, y_test, pos_label,\n",
    "                   probability_threshold, ax):\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred[:, 0], pos_label=pos_label)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "    ax.plot(\n",
    "        fpr, tpr,\n",
    "        color=\"tab:orange\", linewidth=3,\n",
    "        label=f\"ROC-AUC: {roc_auc:.2f}\"\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"tab:green\", label=\"Chance\")\n",
    "    threshold_idx = np.searchsorted(\n",
    "        threshold[::-1], probability_threshold,\n",
    "    )\n",
    "    threshold_idx = len(threshold) - threshold_idx - 1\n",
    "    ax.plot(\n",
    "        fpr[threshold_idx], tpr[threshold_idx],\n",
    "        color=\"tab:blue\", marker=\".\", markersize=10,\n",
    "    )\n",
    "    ax.plot(\n",
    "        [fpr[threshold_idx], fpr[threshold_idx]],\n",
    "        [0, tpr[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        [0, fpr[threshold_idx]],\n",
    "        [tpr[threshold_idx], tpr[threshold_idx]],\n",
    "        '--', color=\"tab:blue\",\n",
    "    )\n",
    "    ax.set_xlabel(f\"1 - Specificity\")\n",
    "    ax.set_ylabel(f\"Sensitivity\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_with_threshold(classifier, X_test, y_test, pos_label,\n",
    "                                         probability_threshold, ax):\n",
    "    from itertools import product\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    class_idx = np.where(classifier.classes_ == pos_label)[0][0]\n",
    "    n_classes = len(classifier.classes_)\n",
    "\n",
    "    y_pred = classifier.predict_proba(X_test)\n",
    "    y_pred = (y_pred[:, class_idx] > probability_threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(\n",
    "        (y_test == pos_label).astype(int), y_pred,\n",
    "    )\n",
    "    im_ = ax.imshow(cm, interpolation='nearest')\n",
    "\n",
    "    text_ = None\n",
    "    cmap_min, cmap_max = im_.cmap(0), im_.cmap(256)\n",
    "\n",
    "    text_ = np.empty_like(cm, dtype=object)\n",
    "\n",
    "    # print text with appropriate color depending on background\n",
    "    thresh = (cm.max() + cm.min()) / 2.0\n",
    "\n",
    "    for i, j in product(range(n_classes), range(n_classes)):\n",
    "        color = cmap_max if cm[i, j] < thresh else cmap_min\n",
    "\n",
    "        text_cm = format(cm[i, j], '.2g')\n",
    "        if cm.dtype.kind != 'f':\n",
    "            text_d = format(cm[i, j], 'd')\n",
    "            if len(text_d) < len(text_cm):\n",
    "                text_cm = text_d\n",
    "\n",
    "        text_[i, j] = ax.text(\n",
    "            j, i, text_cm, ha=\"center\", va=\"center\", color=color\n",
    "        )\n",
    "\n",
    "    ax.set(\n",
    "        xticks=np.arange(n_classes),\n",
    "        yticks=np.arange(n_classes),\n",
    "        xticklabels=classifier.classes_[[int(not bool(class_idx)), class_idx]],\n",
    "        yticklabels=classifier.classes_[[int(not bool(class_idx)), class_idx]],\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_roc(threshold):\n",
    "    # FIXME: we could optimize the plotting by only updating the the\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(21, 6))\n",
    "    plot_pr_curve(\n",
    "        classifier, X_test, y_test, pos_label=\"donated\",\n",
    "        probability_threshold=threshold, ax=axs[0],\n",
    "    )\n",
    "    plot_roc_curve(\n",
    "        classifier, X_test, y_test, pos_label=\"donated\",\n",
    "        probability_threshold=threshold, ax=axs[1]\n",
    "    )\n",
    "    plot_confusion_matrix_with_threshold(\n",
    "        classifier, X_test, y_test, pos_label=\"donated\",\n",
    "        probability_threshold=threshold, ax=axs[2]\n",
    "    )\n",
    "    fig.suptitle(\"Overall performance with positive class 'donated'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_roc_interactive():\n",
    "    from ipywidgets import interactive, FloatSlider\n",
    "    slider = FloatSlider(min=0, max=1, step=0.01, value=0.5)\n",
    "    return interactive(plot_pr_roc, threshold=slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_roc_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Unlike in classification problems, the target `y` is a continuous\n",
    "variable in regression problems. Therefore, classification metrics cannot\n",
    "be used to evaluate the performance of regression models. Instead, there\n",
    "exists a set of metrics dedicated to regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    (\"https://raw.githubusercontent.com/christophM/interpretable-ml-book/\"\n",
    "     \"master/data/bike.csv\"),\n",
    ")\n",
    "# rename the columns with human-readable names\n",
    "data = data.rename(columns={\n",
    "    \"yr\": \"year\", \"mnth\": \"month\", \"temp\": \"temperature\", \"hum\": \"humidity\",\n",
    "    \"cnt\": \"count\", \"days_since_2011\": \"days since 2011\"\n",
    "})\n",
    "# convert the categorical columns with a proper category data type\n",
    "for col in data.columns:\n",
    "    if data[col].dtype.kind == \"O\":\n",
    "        data[col] = data[col].astype(\"category\")\n",
    "\n",
    "# separate the target from the original data\n",
    "X = data.drop(columns=[\"count\"])\n",
    "y = data[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=50, density=True)\n",
    "plt.xlabel(\"Number of bike rentals\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Target distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem can be formulated as follows: we would like to infer the number\n",
    "of bike rentals in a day using information about the day. The number of bike\n",
    "rentals is a number that can vary in the interval [0, max_number_of_bikes).\n",
    "As in the previous section, we will train a\n",
    "model and evaluate its performance while introducing different\n",
    "regression metrics.\n",
    "\n",
    "First, we split the data into training and a testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=True, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "We will use a random forest as a model. However, we first need to check the\n",
    "type of data that we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some features are numeric, some have been tagged as `category`. These\n",
    "features need to be encoded such that our random forest can\n",
    "deal with them. The simplest solution is to use an `OrdinalEncoder`.\n",
    "Regarding, the numerical features, we don't need to do anything. Thus, we\n",
    "will create preprocessing steps to take care of the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_selector = selector(dtype_include=\"category\")\n",
    "preprocessor = make_column_transformer(\n",
    "    (OrdinalEncoder(), categorical_selector),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "X_train_preprocessed = pd.DataFrame(\n",
    "    preprocessor.fit_transform(X_train),\n",
    "    columns=(\n",
    "        categorical_selector(X_train) +\n",
    "        [col for col in X_train.columns\n",
    "         if col not in categorical_selector(X_train)]\n",
    "    )\n",
    ")\n",
    "X_train_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to have some insight about the preprocessing, we preprocess\n",
    "the training data show the result. We can observe that the original strings\n",
    "are now encoded with numbers. We can now create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = make_pipeline(preprocessor, RandomForestRegressor())\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for scikit-learn classifiers, scikit-learn regressors have a `score`\n",
    "method that computes the\n",
    ":math:`R^2` score (also known as the coefficient of determination):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The :math:`R^2` score represents the proportion of variance of the target\n",
    "that is explained by the independent variables in the model. The best score\n",
    "possible\n",
    "is 1 but there is no lower bound. However, a model that predicts the\n",
    "expected value of the target would get a score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regressor.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The :math:`R^2` score gives insight into the goodness of fit of the\n",
    "model. However, this score cannot be compared from one dataset to another and\n",
    "the value obtained does not have a meaningful interpretation relative the\n",
    "original unit of the target. If we wanted to get an interpretable score, we\n",
    "would be interested in the median or mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(\n",
    "    f\"Mean absolute error: {mean_absolute_error(y_test, y_pred):.0f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computing the mean absolute error, we can interpret that our model is\n",
    "predicting on average 507 bike rentals away from the truth. A disadvantage\n",
    "of this metric is that the mean can be\n",
    "impacted by large error. For some applications, we might not want these\n",
    "large errors to have such a big influence on our metric. In this case we can\n",
    "use the median absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "print(\n",
    "    f\"Median absolute error: {median_absolute_error(y_test, y_pred):.0f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric tells us that, our model makes a median error of 405 bikes.\n",
    "FIXME: **not sure how to introduce the `mean_squared_error`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In addition of metrics, we can visually represent the results by plotting\n",
    "the predicted values versus the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_true, y_pred, title=None):\n",
    "    plt.scatter(y_true, y_pred)\n",
    "\n",
    "    max_value = np.max([y_true.max(), y_pred.max()])\n",
    "    plt.plot(\n",
    "        [0, max_value],\n",
    "        [0, max_value],\n",
    "        color=\"tab:orange\",\n",
    "        linewidth=3,\n",
    "        label=\"Perfect fit\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"True values\")\n",
    "    plt.ylabel(\"Predicted values\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.legend()\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, correct predictions would lie on the diagonal line. This plot\n",
    "allows us to detect if the model makes errors in a consistent way, i.e.\n",
    "has some bias.\n",
    "\n",
    "Let's take an example using the house prices in Ames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "X, y = fetch_openml(name=\"house_prices\", as_frame=True, return_X_y=True)\n",
    "X = X.select_dtypes(np.number).drop(\n",
    "    columns=[\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a ridge regressor on the data and plot the prediction versus the\n",
    "actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), RidgeCV())\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred, title=\"House prices in Ames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, we see that for the large True price values, our model tends to\n",
    "under-estimate the price of the house. Typically, this issue arises when\n",
    "the target to predict does not follow a normal distribution. In these cases\n",
    "the model would benefit from target transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model_transformed_target = TransformedTargetRegressor(\n",
    "    regressor=model,\n",
    "    transformer=QuantileTransformer(\n",
    "        n_quantiles=900, output_distribution=\"normal\"\n",
    "    ),\n",
    ")\n",
    "model_transformed_target.fit(X_train, y_train)\n",
    "y_pred = model_transformed_target.predict(X_test)\n",
    "\n",
    "plot_predicted_vs_actual(y_test, y_pred, title=\"House prices in Ames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, once we transformed the target, we see that we corrected some of the\n",
    "high values.\n",
    "\n",
    "## Summary\n",
    "In this notebook, we presented the metrics and plots useful to evaluate and\n",
    "get insights about models. We both focus on regression and classification\n",
    "problems."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
