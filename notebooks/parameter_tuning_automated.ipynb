{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated hyperparameters tuning in scikit-learn\n",
    "\n",
    "In the previous notebook, we saw that hyperparameters can affect the\n",
    "performance of a model. In this notebook, we will show:\n",
    "\n",
    "* how to tune these hyperparameters with a grid-search and randomized search;\n",
    "* how to evaluate the model performance together with hyperparameter\n",
    "  tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reload the dataset as we did previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=[target_name, \"fnlwgt\", \"education-num\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we split it into a training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a pipeline as seen in the first module. It will handle both\n",
    "numerical and categorical features. We use an ordinal since we will use a\n",
    "tree-based model as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "categories = [\n",
    "    data[column].unique() for column in data[categorical_columns]]\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(categories=categories)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat-preprocessor', categorical_preprocessor, categorical_columns)],\n",
    "    remainder='passthrough', sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use a tree-based classifier (i.e. histogram gradient-boosting) to\n",
    "predict whether or not a person earns more than 50,000 dollars a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# for the moment this line is required to import HistGradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",\n",
    "     HistGradientBoostingClassifier(random_state=42, max_leaf_nodes=4))])\n",
    "model.fit(df_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using a grid-search\n",
    "\n",
    "Instead of manually writing the two `for` loops, scikit-learn provides a\n",
    "class called `GridSearchCV` which implement the exhaustive search implemented\n",
    "during the exercise.\n",
    "\n",
    "Let see how to use the `GridSearchCV` estimator for doing such search.\n",
    "Since the grid-search will be costly, we will only explore the combination\n",
    "learning-rate and the maximum number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': (0.05, 0.1, 0.5, 1, 5),\n",
    "    'classifier__max_leaf_nodes': (3, 10, 30, 100)}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid,\n",
    "                                 n_jobs=4, cv=2)\n",
    "model_grid_search.fit(df_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will check the accuracy of our model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_grid_search.score(df_test, target_test)\n",
    "print(\n",
    "    f\"The test accuracy score of the grid-searched pipeline is: \"\n",
    "    f\"{accuracy:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` estimator takes a `param_grid` parameter which defines\n",
    "all hyperparameters and their associated values. The grid-search will be in\n",
    "charge of creating all possible combinations and test them.\n",
    "\n",
    "The number of combinations will be equal to the product of the\n",
    "number of values to explore for each parameter (e.g. in our example 4 x 4\n",
    "combinations). Thus, adding new parameters with their associated values to be\n",
    "explored become rapidly computationally expensive.\n",
    "\n",
    "Once the grid-search is fitted, it can be used as any other predictor by\n",
    "calling `predict` and `predict_proba`. Internally, it will use the model with\n",
    "the best parameters found during `fit`.\n",
    "\n",
    "Get predictions for the 5 first samples using the estimator with the best\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search.predict(df_test.iloc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know about these parameters by looking at the `best_params_`\n",
    "attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: \"\n",
    "      f\"{model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and the best parameters of the grid-searched pipeline are\n",
    "similar to the ones we found in the previous exercise, where we searched the\n",
    "best parameters \"by hand\" through a double for loop.\n",
    "\n",
    "In addition, we can inspect all results which are stored in the attribute\n",
    "`cv_results_` of the grid-search. We will filter some specific columns\n",
    "from these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus on the most interesting columns and shorten the parameter\n",
    "names to remove the `\"param_classifier__\"` prefix for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parameter names\n",
    "column_results = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "column_results += [\n",
    "    \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "cv_results = cv_results[column_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_param(param_name):\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 2 parameters, we might want to visualize the grid-search as a\n",
    "heatmap. We need to transform our `cv_results` into a dataframe where:\n",
    "\n",
    "- the rows will correspond to the learning-rate values;\n",
    "- the columns will correspond to the maximum number of leaf;\n",
    "- the content of the dataframe will be the mean test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_cv_results = cv_results.pivot_table(\n",
    "    values=\"mean_test_score\", index=[\"learning_rate\"],\n",
    "    columns=[\"max_leaf_nodes\"])\n",
    "\n",
    "pivoted_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a heatmap representation to show the above dataframe visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "ax = sns.heatmap(pivoted_cv_results, annot=True, cmap=\"YlGnBu\", vmin=0.7,\n",
    "                 vmax=0.9)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tables highlights the following things:\n",
    "\n",
    "* for too high values of `learning_rate`, the performance of the model is\n",
    "  degraded and adjusting the value of `max_leaf_nodes` cannot fix that\n",
    "  problem;\n",
    "* outside of this pathological region, we observe that the optimal choice\n",
    "  of `max_leaf_nodes` depends on the value of `learning_rate`;\n",
    "* in particular, we observe a \"diagonal\" of good models with an accuracy\n",
    "  close to the maximal of 0.87: when the value of `max_leaf_nodes` is\n",
    "  increased, one should increase the value of `learning_rate` accordingly\n",
    "  to preserve a good accuracy.\n",
    "\n",
    "The precise meaning of those two parameters will be explained in a latter\n",
    "notebook.\n",
    "\n",
    "For now we will note that, in general, **there is no unique optimal parameter\n",
    "setting**: 6 models out of the 16 parameter configuration reach the maximal\n",
    "accuracy (up to small random fluctuations caused by the sampling of the\n",
    "training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning using a randomized-search\n",
    "\n",
    "With the `GridSearchCV` estimator, the parameters need to be specified\n",
    "explicitly. We already mentioned that exploring a large number of values for\n",
    "different parameters will be quickly untractable.\n",
    "\n",
    "Instead, we can randomly generate the parameter candidates. The\n",
    "`RandomizedSearchCV` allows for such stochastic search. It is used similarly\n",
    "to the `GridSearchCV` but the sampling distributions need to be specified\n",
    "instead of the parameter values. For instance, we will draw candidates using\n",
    "a log-uniform distribution also called reciprocal distribution. In addition,\n",
    "we will optimize 3 other parameters:\n",
    "\n",
    "* `max_iter`: it corresponds to the number of trees in the ensemble;\n",
    "* `min_samples_leaf`: it corresponds to the minimum number of samples\n",
    "  required in a leaf;\n",
    "* `max_bins`: it corresponds to the maximum number of bins to construct the\n",
    "  histograms.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">The <tt class=\"docutils literal\">reciprocal</tt> function from SciPy returns a floating number. Since we\n",
    "want to us this distribution to create integer, we will create a class that\n",
    "will cast the floating number into an integer.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "\n",
    "class reciprocal_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = reciprocal(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the randomized search using the different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__l2_regularization': reciprocal(1e-6, 1e3),\n",
    "    'classifier__learning_rate': reciprocal(0.001, 10),\n",
    "    'classifier__max_leaf_nodes': reciprocal_int(2, 256),\n",
    "    'classifier__min_samples_leaf': reciprocal_int(1, 100),\n",
    "    'classifier__max_bins': reciprocal_int(2, 255)}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_iter=10,\n",
    "    n_jobs=4, cv=5)\n",
    "model_random_search.fit(df_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compute the accuracy score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_random_search.score(df_test, target_test)\n",
    "\n",
    "print(f\"The test accuracy score of the best model is \" f\"{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"The best parameters are:\")\n",
    "pprint(model_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the results using the attributes `cv_results` as we previously\n",
    "did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parameter names\n",
    "column_results = [\n",
    "    f\"param_{name}\" for name in param_distributions.keys()]\n",
    "column_results += [\n",
    "    \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "\n",
    "cv_results = pd.DataFrame(model_random_search.cv_results_)\n",
    "cv_results = cv_results[column_results].sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a randomized hyperparameter search is usually run with a large\n",
    "number of iterations. In order to avoid the computation cost and still make a\n",
    "decent analysis, we load the results obtained from a similar search with 200\n",
    "iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_random_search = RandomizedSearchCV(\n",
    "#     model, param_distributions=param_distributions, n_iter=500,\n",
    "#     n_jobs=4, cv=5)\n",
    "# model_random_search.fit(df_train, target_train)\n",
    "# cv_results =  pd.DataFrame(model_random_search.cv_results_)\n",
    "# cv_results.to_csv(\"../figures/randomized_search_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.read_csv(\"../figures/randomized_search_results.csv\",\n",
    "                         index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have more than 2 paramters in our grid-search, we cannot visualize the\n",
    "results using a heatmap. However, we can us a parallel coordinates plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cv_results[column_results].rename(\n",
    "    shorten_param, axis=1).sort_values(\"mean_test_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    cv_results.rename(shorten_param, axis=1).apply({\n",
    "        \"learning_rate\": np.log10,\n",
    "        \"max_leaf_nodes\": np.log2,\n",
    "        \"max_bins\": np.log2,\n",
    "        \"min_samples_leaf\": np.log10,\n",
    "        \"l2_regularization\": np.log10,\n",
    "        \"mean_test_score\": lambda x: x}),\n",
    "    color=\"mean_test_score\",\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel coordinates plot will display the values of the hyperparameters\n",
    "on different columns while the performance metric is color coded. Thus, we\n",
    "are able to quickly inspect if there is a range of hyperparameters which is\n",
    "working or not.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">We <strong>transformed most axis values by taking a log10 or log2</strong> to\n",
    "spread the active ranges and improve the readability of the plot.</p>\n",
    "</div>\n",
    "\n",
    "It is possible to **select a range of results by clicking and holding on\n",
    "any axis** of the parallel coordinate plot. You can then slide (move)\n",
    "the range selection and cross two selections to see the intersections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizz\n",
    "\n",
    "Select the worst performing models (for instance models with a\n",
    "\"mean_test_score\" lower than 0.7): what do have all these models in common\n",
    "(choose one):\n",
    "\n",
    "\n",
    "|                               |      |\n",
    "|-------------------------------|------|\n",
    "| too large `l2_regularization` |      |\n",
    "| too small `l2_regularization` |      |\n",
    "| too large `learning_rate`     |      |\n",
    "| too low `learning_rate`       |      |\n",
    "| too large `max_bins`          |      |\n",
    "| too large `max_bins`          |      |\n",
    "\n",
    "\n",
    "Using the above plot, identify ranges of values for hyperparameter that\n",
    "always prevent the model to reach a test score higher than 0.86, irrespective\n",
    "of the other values:\n",
    "\n",
    "\n",
    "|                               | True | False |\n",
    "|-------------------------------|------|-------|\n",
    "| too large `l2_regularization` |      |       |\n",
    "| too small `l2_regularization` |      |       |\n",
    "| too large `learning_rate`     |      |       |\n",
    "| too low `learning_rate`       |      |       |\n",
    "| too large `max_bins`          |      |       |\n",
    "| too large `max_bins`          |      |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have:\n",
    "\n",
    "* automatically tuned the hyperparameters of a machine-learning pipeline by\n",
    "  exhaustively searching the best combination from a defined grid;\n",
    "* automatically tuned the hyperparameters of a machine-learning pipeline by\n",
    "  drawing values candidates from some predefined distributions;\n",
    "* a grid-search is a costly exhaustive search and does scale with the number\n",
    "  of parameters to search;\n",
    "* a randomized-search will always run with a fixed given budget;\n",
    "* when assessing the performance of a model, hyperparameters search should\n",
    "  be tuned on the training data of a predefined train test split."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
