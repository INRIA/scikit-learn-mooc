{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree in depth\n",
    "\n",
    "In this notebook, we will discuss in detail the internal algorithm used to\n",
    "build the decision tree. First, we will focus on the classification decision\n",
    "tree. Then, we will highlight the fundamental difference between the\n",
    "decision tree used for classification and regression. Finally, we will\n",
    "quickly discuss the importance of the hyper-parameters to be aware of when\n",
    "using decision trees.\n",
    "\n",
    "## Presentation of the dataset\n",
    "\n",
    "We will use the\n",
    "[Palmer penguins dataset](https://allisonhorst.github.io/palmerpenguins/).\n",
    "This dataset is comprised of penguin records and ultimately, we want to\n",
    "predict the species each penguin belongs to.\n",
    "\n",
    "Each penguin is from one of the three following species: Adelie, Gentoo, and\n",
    "Chinstrap. See the illustration below depicting the three different penguin\n",
    "species:\n",
    "\n",
    "![Image of penguins](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png)\n",
    "\n",
    "This problem is a classification problem since the target is categorical.\n",
    "We will limit our input data to a subset of the original features\n",
    "to simplify our explanations when presenting the decision tree algorithm.\n",
    "Indeed, we will use feature based on penguins' culmen measurement. You can\n",
    "learn more about the penguins' culmen with illustration below:\n",
    "\n",
    "![Image of culmen](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "# select the features of interest\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data = data[culmen_columns + [target_column]]\n",
    "data[target_column] = data[target_column].str.split().str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset more into details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there are 2 missing records in this dataset and for the\n",
    "sake of simplicity, we will drop the records corresponding to these 2\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separate the target from the data and create a training and a\n",
    "testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into detail about the decision tree algorithm, we will quickly\n",
    "inspect our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.pairplot(data=data, hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first check the feature distributions by looking at the diagonal plots\n",
    "of the pairplot. We can build the following intuitions:\n",
    "\n",
    "* The Adelie species is separable from the Gentoo and Chinstrap species using\n",
    "  the culmen length;\n",
    "* The Gentoo species is separable from the Adelie and Chinstrap species using\n",
    "  the culmen depth.\n",
    "\n",
    "## How are decision tree built?\n",
    "\n",
    "In a previous notebook, we learnt that a linear classifier will define a\n",
    "linear separation to split classes using a linear combination of the input\n",
    "features. In our 2-dimensional space, it means that a linear classifier will\n",
    "define some oblique lines that best separate our classes. We define a\n",
    "function below that, given a set of data points and a classifier, will plot\n",
    "the decision boundaries learnt by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(X, y, clf, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    feature_0_min, feature_0_max = (X.iloc[:, 0].min() - 1,\n",
    "                                    X.iloc[:, 0].max() + 1)\n",
    "    feature_1_min, feature_1_max = (X.iloc[:, 1].min() - 1,\n",
    "                                    X.iloc[:, 1].max() + 1)\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(feature_0_min, feature_0_max, plot_step),\n",
    "        np.arange(feature_1_min, feature_1_max, plot_step)\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    sns.scatterplot(\n",
    "        data=pd.concat([X, y], axis=1),\n",
    "        x=X.columns[0], y=X.columns[1], hue=y.name,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for a linear classifier, we will obtain the following decision\n",
    "boundaries. These boundaries lines indicate where the model changes its \n",
    "prediction from one class to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "linear_model = LogisticRegression()\n",
    "plot_decision_function(X_train, y_train, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the lines are a combination of the input features since they are\n",
    "not perpendicular a specific axis. In addition, it seems that the linear\n",
    "model would be a good candidate model for such problem as it gives good\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {linear_model.__class__.__name__}: \"\n",
    "    f\"{linear_model.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear models, decision trees will partition the space by considering\n",
    "a single feature at a time. Let's illustrate this behaviour by having\n",
    "a decision tree that only makes a single split to partition the feature\n",
    "space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=1)\n",
    "plot_decision_function(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitions found by the algorithm separates the data along the axis\n",
    "\"Culmen Length\",\n",
    "discarding the feature \"Culmen Depth\". Thus, it highlights that a decision\n",
    "tree does not use a combination of feature when making a split.\n",
    "\n",
    "However, such a split is not powerful enough to separate the three species\n",
    "and the model accuracy is low when compared to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {tree.__class__.__name__}: \"\n",
    "    f\"{tree.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is not a surprise. We saw earlier that a single feature will not\n",
    "be able to separate all three species. However, from the previous analysis we\n",
    "saw that by using both features we should be able to get fairly good results.\n",
    "Considering the splitting mechanism of the decision tree illustrated above, we should\n",
    "repeat the partitioning on the resulting rectangles created by the first\n",
    "split. In this regard, we expect that the two partitions at the second level of the tree will be using\n",
    "the feature \"Culmen Depth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.set_params(max_depth=2)\n",
    "plot_decision_function(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the decision tree made 2 new partitions using the \"Culmen\n",
    "Depth\". Now, our tree is more powerful with similar performance to our linear\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {tree.__class__.__name__}: \"\n",
    "    f\"{tree.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have the intuition that a decision tree is built by\n",
    "successively partitioning the feature space, considering one feature at a\n",
    "time.\n",
    "Subsequently, we will present the details of the partitioning\n",
    "mechanism.\n",
    "\n",
    "## Partitioning mechanism\n",
    "\n",
    "Let's isolate a single feature. We will present the mechanism allowing us to\n",
    "find the optimal partitions for this one-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature = X_train[\"Culmen Length (mm)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check once more the distribution of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for klass in y_train.unique():\n",
    "    mask_penguin_species = y_train == klass\n",
    "    plt.hist(\n",
    "        single_feature[mask_penguin_species], alpha=0.7,\n",
    "        label=f'{klass}', density=True\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(single_feature.name)\n",
    "_ = plt.ylabel('Class probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this graph, we can easily separate the Adelie species from\n",
    "the other species. This can also been seen on a scatter plot of all the\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [single_feature, y_train,\n",
    "     pd.Series([\"\"] * y_train.size, index=single_feature.index, name=\"\")],\n",
    "    axis=1,\n",
    ")\n",
    "_ = sns.swarmplot(x=single_feature.name, y=\"\", hue=y_train.name, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a split requires us to define a threshold value which will be used to\n",
    "separate the different classes. To give an example, we will pick a random\n",
    "threshold value and we will quantify the quality of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "random_indice = rng.choice(single_feature.index)\n",
    "threshold_value = single_feature.loc[random_indice]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "_ = sns.swarmplot(\n",
    "    x=single_feature.name, y=\"\", hue=y_train.name, data=df, ax=ax\n",
    ")\n",
    "ax.axvline(threshold_value, linestyle=\"--\", color=\"black\")\n",
    "_ = ax.set_title(f\"Random threshold value: {threshold_value} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random split does not ensure that we pick a threshold value that\n",
    "best separates the species. Thus, an intuition would be to find a\n",
    "threshold value that best divides the Adelie class from other classes. A\n",
    "threshold around 42 mm would be ideal. Once this split is defined, we could\n",
    "specify that the sample < 42 mm would belong to the class Adelie and the\n",
    "samples > 42 mm would belong to the class the most probable (the one most\n",
    "represented in the partition). In this\n",
    "case, it seems to be the Gentoo species, which is in line with what we\n",
    "observed earlier when fitting a `DecisionTreeClassifier` with a\n",
    "`max_depth=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 42\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "_ = sns.swarmplot(\n",
    "    x=single_feature.name, y=\"\", hue=y_train.name, data=df, ax=ax\n",
    ")\n",
    "ax.axvline(threshold_value, linestyle=\"--\", color=\"black\")\n",
    "_ = ax.set_title(f\"Manual threshold value: {threshold_value} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we expect the best possible threshold to be around this value\n",
    "(42 mm) because it is the split leading to the least amount of error. Thus,\n",
    "if we want to automatically find such a threshold, we would need a way to\n",
    "evaluate the goodness (or pureness) of a given threshold.\n",
    "\n",
    "### The split purity criterion\n",
    "\n",
    "To evaluate the effectiveness of a split, we will use a criterion to qualify\n",
    "the class purity on the resulting partitions.\n",
    "\n",
    "First, let's define a threshold at 42 mm. Then, we will divide the data into\n",
    "2 sub-groups: a group for samples < 42 mm and a group for samples >= 42 mm.\n",
    "Finally, we will store the class label for these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 42\n",
    "mask_below_threshold = single_feature < threshold_value\n",
    "labels_below_threshold = y_train[mask_below_threshold]\n",
    "labels_above_threshold = y_train[~mask_below_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the proportion of samples of each class in both partitions. This\n",
    "proportion is the probability of each class when considering the samples\n",
    "in the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_below_threshold.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_above_threshold.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we visually assessed, the partition (i.e. the part of the data)\n",
    "defined by < 42 mm has mainly Adelie\n",
    "penguin and only 2 samples that are misclassified. However,\n",
    "in the partition >= 42 mm, we cannot differentiate well between Gentoo and\n",
    "Chinstrap (while they are almost twice more Gentoo).\n",
    "\n",
    "We should use a statistical measure that uses all the class\n",
    "probabilities, as the criterion to qualify the purity\n",
    "of a partition.\n",
    "We will choose as an example the entropy criterion (also used\n",
    "in scikit-learn) which is one of the possible classification criterion.\n",
    "\n",
    "The entropy $H$ of the data remaining in one partition is defined as: \n",
    "\n",
    "$H = - \\sum_{k=1}^{K} p_k \\log p_k$\n",
    "\n",
    "where $p_k$ stands for the probability (here the proportions) \n",
    "of finding the class $k$ in this part.\n",
    "\n",
    "For a binary problem (e.g., only 2 classes of penguins), the entropy function\n",
    "for one of the class can be depicted as follows:\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/2/22/Binary_entropy_plot.svg)\n",
    "\n",
    "Therefore, the entropy will be maximum when the proportion of samples from\n",
    "each class is equal (i.e. $p_k$ is 50%) and minimum when only samples for\n",
    "a single class is present (i.e., $p_k$ is 100%, only class `X`,\n",
    "or 0%, only the other class). This idea can be extended to >2 classes.\n",
    "For example, for 3 classes, entropy would be highest when the proportion of\n",
    "samples is 33% for all 3 classes and lowest when the proportion of only one\n",
    "of the classes is 100%.\n",
    "\n",
    "Therefore, a good partition *minimizes* the entropy in each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_criterion(labels):\n",
    "    from scipy.stats import entropy\n",
    "    return entropy(\n",
    "        labels.value_counts(normalize=True).sort_index()\n",
    "    )\n",
    "\n",
    "\n",
    "entropy_below_threshold = classification_criterion(labels_below_threshold)\n",
    "entropy_above_threshold = classification_criterion(labels_above_threshold)\n",
    "\n",
    "print(f\"Entropy for partition below the threshold: \\n\"\n",
    "      f\"{entropy_below_threshold:.3f}\")\n",
    "print(f\"Entropy for partition above the threshold: \\n\"\n",
    "      f\"{entropy_above_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we can see that the entropy in the partition < 42 mm is close to\n",
    "0, meaning that this partition is \"pure\" and nearly entirely consists of a\n",
    "single class (Adelie). Conversely, the partition >= 42 mm is much higher\n",
    "because the species are still mixed, with large numbers of both Chinstrap\n",
    "and Gentoo penguins.\n",
    "\n",
    "With entropy, we are able to assess the quality of each partition. However,\n",
    "the ultimate goal is to evaluate the quality of the overall split\n",
    "and thus\n",
    "combine the measures of entropy in each partition (leaf) into a single statistic.\n",
    "\n",
    "### Information gain\n",
    "\n",
    "Information gain uses the entropy of\n",
    "the two partitions to give us a single statistic quantifying the quality\n",
    "of a split. The information gain is defined as the difference between the\n",
    "entropy\n",
    "before a split and the sum of the entropies of each partition,\n",
    "normalized by the frequencies of class samples in each partition. \n",
    "\n",
    "IG = H(X_unsplit)/N - ( H(split1)/N1 + H(split2)/N2 )\n",
    "\n",
    "The goal is to maximize the information gain (i.e. maximize the decrease in entropy after the split).\n",
    "\n",
    "We will define a function to compute the information gain given the\n",
    "partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def information_gain(labels_below_threshold, labels_above_threshold):\n",
    "    # compute the entropies in the different partitions\n",
    "    entropy_below_threshold = classification_criterion(labels_below_threshold)\n",
    "    entropy_above_threshold = classification_criterion(labels_above_threshold)\n",
    "    entropy_parent = classification_criterion(\n",
    "        pd.concat([labels_below_threshold, labels_above_threshold])\n",
    "    )\n",
    "\n",
    "    # compute the normalized entropies\n",
    "    n_samples_below_threshold = labels_below_threshold.size\n",
    "    n_samples_above_threshold = labels_above_threshold.size\n",
    "    n_samples_parent = n_samples_below_threshold + n_samples_above_threshold\n",
    "\n",
    "    normalized_entropy_below_threshold = (\n",
    "        (n_samples_below_threshold / n_samples_parent) *\n",
    "        entropy_below_threshold\n",
    "    )\n",
    "    normalized_entropy_above_threshold = (\n",
    "        (n_samples_above_threshold / n_samples_parent) *\n",
    "        entropy_above_threshold\n",
    "    )\n",
    "\n",
    "    # compute the information gain\n",
    "    return (entropy_parent -\n",
    "            normalized_entropy_below_threshold -\n",
    "            normalized_entropy_above_threshold)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"The information gain for the split with a threshold at 42 mm is \"\n",
    "    f\"{information_gain(labels_below_threshold, labels_above_threshold):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to quantify any split, we can evaluate all possible\n",
    "splits and compute the information gain for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_information_gain = []\n",
    "possible_thresholds = np.sort(single_feature.unique())[1:-1]\n",
    "for threshold_value in possible_thresholds:\n",
    "    mask_below_threshold = single_feature < threshold_value\n",
    "    labels_below_threshold = y_train.loc[mask_below_threshold]\n",
    "    labels_above_threshold = y_train.loc[~mask_below_threshold]\n",
    "    splits_information_gain.append(\n",
    "        information_gain(labels_below_threshold, labels_above_threshold)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(possible_thresholds, splits_information_gain)\n",
    "plt.xlabel(single_feature.name)\n",
    "_ = plt.ylabel(\"Information gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, we would like to find the threshold value maximizing\n",
    "the information gain. Below we draw a line in the plot, where the maximum\n",
    "information gain value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold_indice = np.argmax(splits_information_gain)\n",
    "best_threshold_value = possible_thresholds[best_threshold_indice]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(possible_thresholds, splits_information_gain)\n",
    "ax.set_xlabel(single_feature.name)\n",
    "ax.set_ylabel(\"Information gain\")\n",
    "ax.axvline(best_threshold_value, color=\"tab:orange\", linestyle=\"--\")\n",
    "ax.set_title(f\"Best threshold: {best_threshold_value} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this brute-force search, we find that the threshold maximizing the\n",
    "information gain is 43.3 mm.\n",
    "\n",
    "Let's check if this result is similar to the one found with the\n",
    "`DecisionTreeClassifier` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1)\n",
    "tree.fit(single_feature.to_frame(), y_train)\n",
    "_ = plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation in scikit-learn gives similar results: 43.25 mm. The\n",
    "slight difference is just due to some low-level implementation details.\n",
    "\n",
    "As we previously explained, the split mechanism will be repeated several\n",
    "times (until there is no classification error on the training set,\n",
    "i.e., all final partitions consist of only one class). In\n",
    "the above example, it corresponds to setting the `max_depth` parameter to\n",
    "`None`. This allows the algorithm to keep making splits until the final\n",
    "partitions are pure.\n",
    "\n",
    "## How does prediction work?\n",
    "\n",
    "We showed how a decision tree is constructed. However, we did not explain\n",
    "how predictions are makde from the decision tree.\n",
    "\n",
    "First, let's recall the tree structure that we fitted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that the threshold found is 43.25 mm. Thus, let's see the class\n",
    "prediction for a sample with a feature value below the threshold and another\n",
    "above the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The class predicted for a value below the threshold is: \"\n",
    "      f\"{tree.predict([[35]])}\")\n",
    "print(f\"The class predicted for a value above the threshold is: \"\n",
    "      f\"{tree.predict([[45]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict an Adelie penguin if the feature value is below the threshold,\n",
    "which is not surprising since this partition was almost pure. If the feature\n",
    "value is above the threshold, we\n",
    "predict the Gentoo penguin, the class that is most probable.\n",
    "\n",
    "## What about decision tree for regression?\n",
    "\n",
    "We explained the construction of the decision tree for a classification\n",
    "problem. The entropy criterion to determine how we split the nodes used the\n",
    "class probabilities. We cannot use this criterion the target `y` is\n",
    "continuous. In this case, we will need specific criterion adapted for\n",
    "regression problems.\n",
    "\n",
    "Before going into detail about regression criterion, let's observe and\n",
    "build some intuitions about the characteristics of decision trees used\n",
    "for regression.\n",
    "\n",
    "### Decision tree: a non-parametric model\n",
    "\n",
    "We will use the same penguins dataset however, this time we will formulate a\n",
    "regression problem instead of a classification problem. We will try to\n",
    "infer the body mass of a penguin given its flipper length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "data_columns = [\"Flipper Length (mm)\"]\n",
    "target_column = \"Body Mass (g)\"\n",
    "\n",
    "data = data[data_columns + [target_column]]\n",
    "data = data.dropna()\n",
    "\n",
    "X, y = data[data_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we deal with a regression problem because our target is a continuous\n",
    "variable ranging from 2.7 kg to 6.3 kg. From the scatter plot above, we can\n",
    "observe that we have a linear relationship between the flipper length\n",
    "and the body mass. The longer the flipper of a penguin, the heavier the\n",
    "penguin.\n",
    "\n",
    "For this problem, we would expect the simple linear model to be able to\n",
    "model this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a function in charge of plotting the dataset and\n",
    "all possible predictions. This function is equivalent to the earlier\n",
    "function used to plot the decision boundaries for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_model(X, y, model, extrapolate=False, ax=None):\n",
    "    \"\"\"Plot the dataset and the prediction of a learnt regression model.\"\"\"\n",
    "    # train our model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # make a scatter plot of the input data and target\n",
    "    training_data = pd.concat([X, y], axis=1)\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        data=training_data, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "        ax=ax, color=\"black\", alpha=0.5,\n",
    "    )\n",
    "\n",
    "    # only necessary if we want to see the extrapolation of our model\n",
    "    offset = 20 if extrapolate else 0\n",
    "\n",
    "    # generate a testing set spanning between min and max of the training set\n",
    "    X_test = np.linspace(\n",
    "        X.min() - offset, X.max() + offset, num=100\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    # predict for this testing set and plot the response\n",
    "    y_pred = model.predict(X_test)\n",
    "    ax.plot(\n",
    "        X_test, y_pred,\n",
    "        label=f\"{model.__class__.__name__} trained\", linewidth=3,\n",
    "    )\n",
    "    plt.legend()\n",
    "    # return the axes in case we want to add something to it\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "_ = plot_regression_model(X_train, y_train, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plot above, we see that a non-regularized `LinearRegression` is able\n",
    "to fit the data. A feature of this model is that all new predictions\n",
    "will be on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:10]\n",
    "ax = plot_regression_model(X_train, y_train, linear_model)\n",
    "y_pred = linear_model.predict(X_test_subset)\n",
    "ax.plot(\n",
    "    X_test_subset, y_pred, label=\"Test predictions\",\n",
    "    color=\"tab:green\", marker=\"^\", markersize=10, linestyle=\"\",\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to linear models, decision trees are non-parametric\n",
    "models, so they do not make assumptions about the way data are distributed.\n",
    "This will affect the prediction scheme. Repeating the\n",
    "above experiment will highlight the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_regression_model(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the decision tree model does not have a priori distribution\n",
    "for the data and we do not end-up\n",
    "with a straight line to regress flipper length and body mass.\n",
    "Having different body masses\n",
    "for a same flipper length, the tree will be predicting the mean of the\n",
    "targets.\n",
    "\n",
    "So in classification setting, we saw that the predicted value was the most\n",
    "probable value in the node of the tree. In the case of regression, the\n",
    "predicted value corresponds to the mean of the target in the leaf.\n",
    "\n",
    "This lead us to question whether or not our decision trees are able to\n",
    "extrapolate to unseen data. We can highlight that this is possible with the\n",
    "linear model because it is a parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_model(X_train, y_train, linear_model, extrapolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model will extrapolate using the fitted model for flipper lengths\n",
    "< 175 mm and > 235 mm. Let's see the difference between the classification\n",
    "and regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_regression_model(X_train, y_train, linear_model, extrapolate=True)\n",
    "_ = plot_regression_model(X_train, y_train, tree, extrapolate=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regression tree, we see that it cannot extrapolate outside of the\n",
    "flipper length range present in the training data.\n",
    "For flipper lengths below the minimum, the mass of the penguin in the\n",
    "training data with the shortest flipper length will always be predicted.\n",
    "Similarly, for flipper lengths above the maximum, the mass of the penguin\n",
    "in the training data with the longest flipper will always predicted.\n",
    "\n",
    "### The regression criterion\n",
    "\n",
    "In the previous section, we explained the differences between using decision\n",
    "tree for classification and for regression: the predicted value will be the\n",
    "most probable class for the classification case while the it will be the mean\n",
    "in the case of the regression. The second difference that we already\n",
    "mentioned is the criterion. The classification criterion cannot be applied\n",
    "in regression setting and we need to use a specific set of criterion.\n",
    "\n",
    "One of the criterion that can be used in regression is the mean squared\n",
    "error. In this case, we will compute this criterion for each partition,\n",
    "as in the case of the entropy, and select the split leading to the best\n",
    "improvement (i.e. information gain).\n",
    "\n",
    "## Importance of decision tree hyper-parameters on generalization\n",
    "\n",
    "This last section will illustrate the importance of some key hyper-parameters\n",
    "of the decision tree. We will illustrate it on both the classification and\n",
    "regression probelms that we previously used.\n",
    "\n",
    "### Creation of the classification and regression dataset\n",
    "\n",
    "We will first regenerate the classification and regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_clf_column = \"Species\"\n",
    "\n",
    "data_clf = data[\n",
    "    data_clf_columns + [target_clf_column]\n",
    "]\n",
    "data_clf[target_clf_column] = data_clf[\n",
    "    target_clf_column].str.split().str[0]\n",
    "data_clf = data_clf.dropna()\n",
    "\n",
    "X_clf, y_clf = data_clf[data_clf_columns], data_clf[target_clf_column]\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, stratify=y_clf, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_columns = [\"Flipper Length (mm)\"]\n",
    "target_reg_column = \"Body Mass (g)\"\n",
    "\n",
    "data_reg = data[data_reg_columns + [target_reg_column]]\n",
    "data_reg = data_reg.dropna()\n",
    "\n",
    "X_reg, y_reg = data_reg[data_reg_columns], data_reg[target_reg_column]\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    data=data_clf,\n",
    "    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "axs[0].set_title(\"Classification dataset\")\n",
    "sns.scatterplot(\n",
    "    data=data_reg, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "    ax=axs[1],\n",
    ")\n",
    "_ = axs[1].set_title(\"Regression dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Effect of the `max_depth` parameter\n",
    "\n",
    "In decision trees, the most important parameter to get a trade-off between\n",
    "under-fitting and over-fitting is the `max_depth` parameter. Let's build\n",
    "a shallow tree and then deeper tree (for both classification and regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = fig.suptitle(f\"Shallow tree with a max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 30\n",
    "tree_clf.set_params(max_depth=max_depth)\n",
    "tree_reg.set_params(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = fig.suptitle(f\"Deep tree with a max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both classification and regression setting, we can observe that\n",
    "increasing\n",
    "the depth will make the tree model more expressive. However, a tree that is\n",
    "too deep will overfit the training data, creating partitions which are only\n",
    "be correct for \"outliers\". The `max_depth` is one of the hyper-parameters\n",
    "that one should optimize via cross-validation and grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(2, 10, 1)}\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "axs[0].set_title(\n",
    "    f\"Optimal depth found via CV: {tree_clf.best_params_['max_depth']}\"\n",
    ")\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = axs[1].set_title(\n",
    "    f\"Optimal depth found via CV: {tree_reg.best_params_['max_depth']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other parameters are used to fine tune the decision tree and have less\n",
    "impact than `max_depth`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
