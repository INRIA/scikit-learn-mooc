{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree in depth\n",
    "\n",
    "In this notebook, we will go into details on the internal algorithm used to\n",
    "build the decision tree. First, we will focus on the decision tree used for\n",
    "classification. Then, we will highlight the fundamental difference between\n",
    "decision tree used in classification and in regression. Finally, we will\n",
    "quickly discuss the importance of the hyperparameters to be aware of when\n",
    "using decision trees.\n",
    "\n",
    "## Presentation of the dataset\n",
    "\n",
    "We use the\n",
    "[Palmer penguins dataset](https://allisonhorst.github.io/palmerpenguins/).\n",
    "This dataset is composed of penguins records and ultimately, we want to\n",
    "identify from which specie a penguin belongs to.\n",
    "\n",
    "A penguin is from one of the three following species: Adelie, Gentoo, and\n",
    "Chinstrap. See the illustration below depicting of the three different bird\n",
    "species:\n",
    "\n",
    "![Image of penguins](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png)\n",
    "\n",
    "This problem is a classification problem since the target is made of\n",
    "categories. We will limit our input data to a subset of the original features\n",
    "to simplify our explanations when presenting the decision tree algorithm.\n",
    "Indeed, we will use feature based on penguins' culmen measurement. You can\n",
    "learn more about the penguins' culmen with illustration below:\n",
    "\n",
    "![Image of culmen](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/culmen_depth.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "# select the features of interest\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data = data[culmen_columns + [target_column]]\n",
    "data[target_column] = data[target_column].str.split().str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset more into details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that they are 2 missing records in this dataset and for the\n",
    "sake of simplicity, we will drop the records corresponding to these 2\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separate the target from the data and we will create a training and a\n",
    "testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into details in the decision tree algorithm, we will quickly\n",
    "inspect our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.pairplot(data=data, hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first check the feature distributions by looking at the diagonal plots\n",
    "of the pairplot. We can build the following intuitions:\n",
    "\n",
    "* The Adelie specie is separable from the Gentoo and Chinstrap species using\n",
    "  the culmen length;\n",
    "* The Gentoo specie is separable from the Adelie and Chinstrap species using\n",
    "  the culmen depth.\n",
    "\n",
    "## How decision tree are built?\n",
    "\n",
    "In a previous notebook, we learnt that a linear classifier will define a\n",
    "linear separation to split classes using a linear combination of the input\n",
    "features. In our 2-dimensional space, it means that a linear classifier will\n",
    "defined some oblique lines that best separate our classes. We define a\n",
    "function below that given a set of data point and a classifier will plot the\n",
    "decision boundaries learnt by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(X, y, clf, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    feature_0_min, feature_0_max = (X.iloc[:, 0].min() - 1,\n",
    "                                    X.iloc[:, 0].max() + 1)\n",
    "    feature_1_min, feature_1_max = (X.iloc[:, 1].min() - 1,\n",
    "                                    X.iloc[:, 1].max() + 1)\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(feature_0_min, feature_0_max, plot_step),\n",
    "        np.arange(feature_1_min, feature_1_max, plot_step)\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    sns.scatterplot(\n",
    "        data=pd.concat([X, y], axis=1),\n",
    "        x=X.columns[0], y=X.columns[1], hue=y.name,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for a linear classifier, we will obtain the following decision\n",
    "boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "linear_model = LogisticRegression()\n",
    "plot_decision_function(X_train, y_train, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the lines are a combination of the input features since they are\n",
    "not perpendicular a specific axis. In addition, it seems that the linear\n",
    "model would be a good candidate model for such problem, giving a good\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {linear_model.__class__.__name__}: \"\n",
    "    f\"{linear_model.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear model, decision tree will partition the space considering a\n",
    "single feature at a time. Let's illustrate this behaviour by having\n",
    "a decision tree which makes a single split to partition the feature space.\n",
    "the decision tree to make a single split to partition our feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=1)\n",
    "plot_decision_function(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partition found separate the data along the axis \"Culmen Length\",\n",
    "discarding the feature \"Culmen Depth\". Thus, it highlights that a decision\n",
    "tree does not use a combination of feature when making a split.\n",
    "\n",
    "However, such a split is not powerful enough to isolate the three species and\n",
    "the model accuracy is low compared to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {tree.__class__.__name__}: \"\n",
    "    f\"{tree.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is not a surprise. We earlier saw that a single feature will not\n",
    "help separating the three species. However, from the previous analysis we\n",
    "saw that using both features should be useful to get fairly good results.\n",
    "Considering the mechanism of the decision tree illustrated above, we should\n",
    "repeat the partitioning on each rectangle that was previously created. In\n",
    "this regard, we expect that the partition will be using the feature \"Culmen\n",
    "Depth\" this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.set_params(max_depth=2)\n",
    "plot_decision_function(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the decision tree made 2 new partitions using the \"Culmen\n",
    "Depth\". Now, our tree is more powerful with similar performance to our linear\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Accuracy of the {tree.__class__.__name__}: \"\n",
    "    f\"{tree.fit(X_train, y_train).score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have the intuition that a decision tree is built by\n",
    "successively partitioning the feature space, considering one feature at a\n",
    "time.\n",
    "Subsequently, we will present the details regarding the partitioning\n",
    "mechanism.\n",
    "\n",
    "## Partitioning mechanism\n",
    "\n",
    "Let's isolate a single feature. We will present the mechanism allowing to\n",
    "find the optimal partition for these one-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature = X_train[\"Culmen Length (mm)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check once more the distribution of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for klass in y_train.unique():\n",
    "    mask_penguin_species = y_train == klass\n",
    "    plt.hist(\n",
    "        single_feature[mask_penguin_species], alpha=0.7,\n",
    "        label=f'{klass}', density=True\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(single_feature.name)\n",
    "_ = plt.ylabel('Class probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this graph, we can easily separate the Adelie specie from\n",
    "the other species. Alternatively, we can have a scatter plot of all\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [single_feature, y_train,\n",
    "     pd.Series([\"\"] * y_train.size, index=single_feature.index, name=\"\")],\n",
    "    axis=1,\n",
    ")\n",
    "_ = sns.swarmplot(x=single_feature.name, y=\"\", hue=y_train.name, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a split comes to define a threshold value which will be used to\n",
    "separate the different classes. To give an example, we will pick a random\n",
    "threshold value and we will qualify the quality of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "random_indice = rng.choice(single_feature.index)\n",
    "threshold_value = single_feature.loc[random_indice]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "_ = sns.swarmplot(\n",
    "    x=single_feature.name, y=\"\", hue=y_train.name, data=df, ax=ax\n",
    ")\n",
    "ax.axvline(threshold_value, linestyle=\"--\", color=\"black\")\n",
    "_ = ax.set_title(f\"Random threshold value: {threshold_value} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random split does not ensure that we pick up a threshold value which\n",
    "best separate the species. Thus, an intuition will be to find a\n",
    "threshold value that best divide the Adelie class from other classes. A\n",
    "threshold around 42 mm would be ideal. Once this split is defined, we could\n",
    "specify that the sample < 42 mm would belong to the class Adelie and the\n",
    "samples > 42 mm would belong to the class the most probable (the most\n",
    "represented in the partition) between the Gentoo and the Chinstrap. In this\n",
    "case, it seems to be the Gentoo specie, which is in-line with what we\n",
    "observed earlier when fitting a `DecisionTreeClassifier` with a\n",
    "`max_depth=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 42\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "_ = sns.swarmplot(\n",
    "    x=single_feature.name, y=\"\", hue=y_train.name, data=df, ax=ax\n",
    ")\n",
    "ax.axvline(threshold_value, linestyle=\"--\", color=\"black\")\n",
    "_ = ax.set_title(f\"Manual threshold value: {threshold_value} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we expect the best possible threshold to be around this value\n",
    "(42 mm) because it is the split leading to the least amount of error. Thus,\n",
    "if we want to automatically find such a threshold, we would need a way to\n",
    "evaluate the goodness (or pureness) of a given threshold.\n",
    "\n",
    "### The split purity criterion\n",
    "\n",
    "To evaluate the effectiveness of a split, we will use a criterion to qualify\n",
    "the class purity on the different partitions.\n",
    "\n",
    "First, let's define a threshold at 42 mm. Then, we will divide the data into\n",
    "2 sub-groups: a group for samples < 42 mm and a group for samples >= 42 mm.\n",
    "Then, we will store the class label for these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = 42\n",
    "mask_below_threshold = single_feature < threshold_value\n",
    "labels_below_threshold = y_train[mask_below_threshold]\n",
    "labels_above_threshold = y_train[~mask_below_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the proportion of samples of each class in both partitions. This\n",
    "proportion is the probability of each class when considering\n",
    "the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_below_threshold.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_above_threshold.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we visually assess, the partition defined by < 42 mm has mainly Adelie\n",
    "penguin and only 2 samples which we could considered misclassified. However,\n",
    "on the partition >= 42 mm, we cannot differentiate Gentoo and Chinstrap\n",
    "(while they are almost twice more Gentoo).\n",
    "\n",
    "We should come with a statistical measure which combine the class\n",
    "probabilities together that can be used as a criterion to qualify the purity\n",
    "of a partition. We will choose as an example the entropy criterion (also used\n",
    "in scikit-learn) which is one of the possible classification criterion.\n",
    "\n",
    "The entropy is defined as: $H(X) = - \\sum_{k=1}^{K} p(X_k) \\log p(X_k)$\n",
    "\n",
    "For a binary problem, the entropy function for one of the class can be\n",
    "depicted as follows:\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/2/22/Binary_entropy_plot.svg)\n",
    "\n",
    "Therefore, the entropy will be maximum when the proportion of sample from\n",
    "each class will be equal and minimum when only samples for a single class\n",
    "is present.\n",
    "\n",
    "Therefore, one searches to minimize the entropy in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_criterion(labels):\n",
    "    from scipy.stats import entropy\n",
    "    return entropy(\n",
    "        labels.value_counts(normalize=True).sort_index()\n",
    "    )\n",
    "\n",
    "\n",
    "entropy_below_threshold = classification_criterion(labels_below_threshold)\n",
    "entropy_above_threshold = classification_criterion(labels_above_threshold)\n",
    "\n",
    "print(f\"Entropy for partition below the threshold: \\n\"\n",
    "      f\"{entropy_below_threshold:.3f}\")\n",
    "print(f\"Entropy for partition above the threshold: \\n\"\n",
    "      f\"{entropy_above_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we can see that the entropy in the partition < 42 mm is close to\n",
    "0 meaning that this partition is \"pure\" and contain a single class while\n",
    "the partition >= 42 mm is much higher due to the fact that 2 of the classes\n",
    "are still mixed.\n",
    "\n",
    "Now, we are able to assess the quality of each partition. However, the\n",
    "ultimate goal is to evaluate the quality of the split and thus combine both\n",
    "measures of entropy to obtain a single statistic.\n",
    "\n",
    "### Information gain\n",
    "\n",
    "This statistic is known as the information gain. It combines the entropy of\n",
    "the different partitions to give us a single statistic qualifying the quality\n",
    "of a split. The information gain is defined as the difference of the entropy\n",
    "before making a split and the sum of the entropies of each partition,\n",
    "normalized by the frequencies of class samples on each partition. The goal is\n",
    "to maximize the information gain.\n",
    "\n",
    "We will define a function to compute the information gain given the different\n",
    "partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def information_gain(labels_below_threshold, labels_above_threshold):\n",
    "    # compute the entropies in the different partitions\n",
    "    entropy_below_threshold = classification_criterion(labels_below_threshold)\n",
    "    entropy_above_threshold = classification_criterion(labels_above_threshold)\n",
    "    entropy_parent = classification_criterion(\n",
    "        pd.concat([labels_below_threshold, labels_above_threshold])\n",
    "    )\n",
    "\n",
    "    # compute the normalized entropies\n",
    "    n_samples_below_threshold = labels_below_threshold.size\n",
    "    n_samples_above_threshold = labels_above_threshold.size\n",
    "    n_samples_parent = n_samples_below_threshold + n_samples_above_threshold\n",
    "\n",
    "    normalized_entropy_below_threshold = (\n",
    "        (n_samples_below_threshold / n_samples_parent) *\n",
    "        entropy_below_threshold\n",
    "    )\n",
    "    normalized_entropy_above_threshold = (\n",
    "        (n_samples_above_threshold / n_samples_parent) *\n",
    "        entropy_above_threshold\n",
    "    )\n",
    "\n",
    "    # compute the information gain\n",
    "    return (entropy_parent -\n",
    "            normalized_entropy_below_threshold -\n",
    "            normalized_entropy_above_threshold)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"The information gain for the split with a threshold at 42 mm is \"\n",
    "    f\"{information_gain(labels_below_threshold, labels_above_threshold):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to quantify any split. Thus, we can evaluate every possible\n",
    "split and compute the information gain for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_information_gain = []\n",
    "possible_thresholds = np.sort(single_feature.unique())[1:-1]\n",
    "for threshold_value in possible_thresholds:\n",
    "    mask_below_threshold = single_feature < threshold_value\n",
    "    labels_below_threshold = y_train.loc[mask_below_threshold]\n",
    "    labels_above_threshold = y_train.loc[~mask_below_threshold]\n",
    "    splits_information_gain.append(\n",
    "        information_gain(labels_below_threshold, labels_above_threshold)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(possible_thresholds, splits_information_gain)\n",
    "plt.xlabel(single_feature.name)\n",
    "_ = plt.ylabel(\"Information gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, we would like to find the threshold value maximizing\n",
    "the information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold_indice = np.argmax(splits_information_gain)\n",
    "best_threshold_value = possible_thresholds[best_threshold_indice]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(possible_thresholds, splits_information_gain)\n",
    "ax.set_xlabel(single_feature.name)\n",
    "ax.set_ylabel(\"Information gain\")\n",
    "ax.axvline(best_threshold_value, color=\"tab:orange\", linestyle=\"--\")\n",
    "ax.set_title(f\"Best threshold: {best_threshold_value} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By making this brute-force search, we find that the threshold maximizing the\n",
    "information gain is 43.3 mm.\n",
    "\n",
    "Let's check if this results is similar than the one found with the\n",
    "`DecisionTreeClassifier` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1)\n",
    "tree.fit(single_feature.to_frame(), y_train)\n",
    "_ = plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation in scikit-learn gives similar results: 43.25 mm. The\n",
    "slight difference are only due to some low-level implementation details.\n",
    "\n",
    "As we previously explained, the split mechanism will be repeated several\n",
    "times (until we don't have any classification error on the training set). In\n",
    "the above example, it corresponds to increasing the `max_depth` parameter.\n",
    "\n",
    "## How prediction works?\n",
    "\n",
    "We showed the way a decision tree is constructed. However, we did not explain\n",
    "how and what will be predicted from the decision tree.\n",
    "\n",
    "First, let's recall the tree structure that we fitted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that the threshold found is 43.25 mm. Thus, let's see the class\n",
    "prediction for a sample with a feature value below the threshold and another\n",
    "above the\n",
    "threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The class predicted for a value below the threshold is: \"\n",
    "      f\"{tree.predict([[35]])}\")\n",
    "print(f\"The class predicted for a value above the threshold is: \"\n",
    "      f\"{tree.predict([[45]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict an Adelie penguin for a value below the threshold which is not\n",
    "surprising since this partition was almost pure. In the other case we\n",
    "predicted the Gentoo penguin. Indeed, we predict the class the\n",
    "most probable.\n",
    "\n",
    "## What about decision tree for regression?\n",
    "\n",
    "We explained the construction of the decision tree in a classification\n",
    "problem. The entropy criterion to split the nodes used the class\n",
    "probabilities. Thus, this criterion is not adapted when the target `y` is\n",
    "continuous. In this case, we will need specific criterion adapted to\n",
    "regression problems.\n",
    "\n",
    "Before going into details with regression criterion, let's observe and\n",
    "build some intuitions on the characteristics of decision tree used\n",
    "in regression.\n",
    "\n",
    "### Decision tree: a non-parametric model\n",
    "\n",
    "We use the same penguins dataset. However, this time we will formulate a\n",
    "regression problem instead of a classification problem. Thus, we will try to\n",
    "infer the body mass of a penguin given its flipper length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "data_columns = [\"Flipper Length (mm)\"]\n",
    "target_column = \"Body Mass (g)\"\n",
    "\n",
    "data = data[data_columns + [target_column]]\n",
    "data = data.dropna()\n",
    "\n",
    "X, y = data[data_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we deal with a regression problem because our target is a continuous\n",
    "variable ranging from 2.7 kg to 6.3 kg. From the scatter plot above, we can\n",
    "observe that we have a linear relationship between the flipper length\n",
    "and the body mass. Longer is the flipper of a penguin, heavier will be the\n",
    "penguin.\n",
    "\n",
    "For this problem, we would expect the simpler linear model to be able to\n",
    "model this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a function in charge of plotting the dataset and\n",
    "all possible predictions. This function is equivalent to the earlier\n",
    "function used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_model(X, y, model, extrapolate=False, ax=None):\n",
    "    \"\"\"Plot the dataset and the prediction of a learnt regression model.\"\"\"\n",
    "    # train our model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # make a scatter plot of the input data and target\n",
    "    training_data = pd.concat([X, y], axis=1)\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        data=training_data, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "        ax=ax, color=\"black\", alpha=0.5,\n",
    "    )\n",
    "\n",
    "    # only necessary if we want to see the extrapolation of our model\n",
    "    offset = 20 if extrapolate else 0\n",
    "\n",
    "    # generate a testing set spanning between min and max of the training set\n",
    "    X_test = np.linspace(\n",
    "        X.min() - offset, X.max() + offset, num=100\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    # predict for this testing set and plot the response\n",
    "    y_pred = model.predict(X_test)\n",
    "    ax.plot(\n",
    "        X_test, y_pred,\n",
    "        label=f\"{model.__class__.__name__} trained\", linewidth=3,\n",
    "    )\n",
    "    plt.legend()\n",
    "    # return the axes in case we want to add something to it\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "_ = plot_regression_model(X_train, y_train, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plot above, we see that a non-regularized `LinearRegression` is able\n",
    "to fit the data. The specificity of the model is that any new predictions\n",
    "will occur on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:10]\n",
    "ax = plot_regression_model(X_train, y_train, linear_model)\n",
    "y_pred = linear_model.predict(X_test_subset)\n",
    "ax.plot(\n",
    "    X_test_subset, y_pred, label=\"Test predictions\",\n",
    "    color=\"tab:green\", marker=\"^\", markersize=10, linestyle=\"\",\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary of linear model, decision trees are non-parametric\n",
    "models, so they do not rely on the way data should be distributed. In this\n",
    "regard, it will affect the prediction scheme. Repeating the\n",
    "above experiment will highlights the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_regression_model(X_train, y_train, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the decision tree model does not have a priori and do not end-up\n",
    "with a straight line to regress flipper length and body mass. The prediction\n",
    "of a new sample, which was already present in the training set, will give the\n",
    "same target than this training sample. However, having different body masses\n",
    "for a same flipper length, the tree will be predicting the mean of the\n",
    "targets.\n",
    "\n",
    "So in classification setting, we saw that the predicted value was the most\n",
    "probable value in the node of the tree. In the case of regression, the\n",
    "predicted value corresponds to the mean of the target in the node.\n",
    "\n",
    "This lead us to question whether or not our decision trees are able to\n",
    "extrapolate to unseen data. We can highlight that this is possible with the\n",
    "linear model because it is a parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_model(X_train, y_train, linear_model, extrapolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model will extrapolate using the fitted model for flipper length\n",
    "< 175 mm and > 235 mm. Let's see the difference with the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_regression_model(X_train, y_train, linear_model, extrapolate=True)\n",
    "_ = plot_regression_model(X_train, y_train, tree, extrapolate=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tree, we see that it cannot extrapolate below and above the minimum\n",
    "and maximum, respectively, of the flipper length encountered during the\n",
    "training. Indeed, we are predicting the minimum and maximum values of the\n",
    "training set.\n",
    "\n",
    "### The regression criterion\n",
    "\n",
    "In the previous section, we explained the differences between using decision\n",
    "tree in classification or in regression: the predicted value will be the\n",
    "most probable class for the classification case while the it will be the mean\n",
    "in the case of the regression. The second difference that we already\n",
    "mentioned is the criterion. The classification criterion cannot be applied\n",
    "in regression setting and we need to use a specific set of criterion.\n",
    "\n",
    "One of the criterion that can be used in regression is the mean squared\n",
    "error. In this case, we will compute this criterion in each partition\n",
    "as in the case of the entropy and select the split leading to the best\n",
    "improvement (i.e. information gain).\n",
    "\n",
    "## Importance of decision tree hyper-parameters on generalization\n",
    "\n",
    "This last section will illustrate the importance of some key hyper-parameters\n",
    "of the decision tree. We will both illustrate it on classification and\n",
    "regression datasets that we previously used.\n",
    "\n",
    "### Creation of the classification and regression dataset\n",
    "\n",
    "We will first regenerate the classification and regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_clf_column = \"Species\"\n",
    "\n",
    "data_clf = data[\n",
    "    data_clf_columns + [target_clf_column]\n",
    "]\n",
    "data_clf[target_clf_column] = data_clf[\n",
    "    target_clf_column].str.split().str[0]\n",
    "data_clf = data_clf.dropna()\n",
    "\n",
    "X_clf, y_clf = data_clf[data_clf_columns], data_clf[target_clf_column]\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, stratify=y_clf, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_columns = [\"Flipper Length (mm)\"]\n",
    "target_reg_column = \"Body Mass (g)\"\n",
    "\n",
    "data_reg = data[data_reg_columns + [target_reg_column]]\n",
    "data_reg = data_reg.dropna()\n",
    "\n",
    "X_reg, y_reg = data_reg[data_reg_columns], data_reg[target_reg_column]\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    data=data_clf,\n",
    "    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\",\n",
    "    ax=axs[0],\n",
    ")\n",
    "axs[0].set_title(\"Classification dataset\")\n",
    "sns.scatterplot(\n",
    "    data=data_reg, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "    ax=axs[1],\n",
    ")\n",
    "_ = axs[1].set_title(\"Regression dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Effect of the `max_depth` parameter\n",
    "\n",
    "In decision tree, the most important parameter to get a trade-off between\n",
    "under-fitting and over-fitting is the `max_depth` parameter. Let's build\n",
    "a shallow tree (for both classification and regression) and a deeper tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = fig.suptitle(f\"Shallow tree with a max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 30\n",
    "tree_clf.set_params(max_depth=max_depth)\n",
    "tree_reg.set_params(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = fig.suptitle(f\"Deep tree with a max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both classification and regression setting, we can observe that increasing\n",
    "the depth will make the tree model more expressive. However, a tree which is\n",
    "too deep will overfit the training data, creating partitions which will only\n",
    "be correct for \"outliers\". The `max_depth` is one of the parameter that one\n",
    "would like to optimize via cross-validation and a grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(2, 10, 1)}\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_decision_function(X_train_clf, y_train_clf, tree_clf, ax=axs[0])\n",
    "axs[0].set_title(\n",
    "    f\"Optimal depth found via CV: {tree_clf.best_params_['max_depth']}\"\n",
    ")\n",
    "plot_regression_model(X_train_reg, y_train_reg, tree_reg, ax=axs[1])\n",
    "_ = axs[1].set_title(\n",
    "    f\"Optimal depth found via CV: {tree_reg.best_params_['max_depth']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other parameters are used to fine tune the decision tree and have less\n",
    "impact than `max_depth`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
