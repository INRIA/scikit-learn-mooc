{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classification decision tree\n",
    "\n",
    "We will illustrate how decision tree fit data with a simple classification\n",
    "problem using the penguins dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, we split the data into two subsets to investigate how trees will\n",
    "predict values based on an out-of-samples dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "range_features = {\n",
    "    feature_name: (X[feature_name].min() - 1, X[feature_name].max() + 1)\n",
    "    for feature_name in X.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous notebook, we learnt that a linear classifier will define a\n",
    "linear separation to split classes using a linear combination of the input\n",
    "features. In our 2-dimensional space, it means that a linear classifier will\n",
    "define some oblique lines that best separate our classes. We define a\n",
    "function below that, given a set of data points and a classifier, will plot\n",
    "the decision boundaries learnt by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(fitted_classifier, range_features, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for a linear classifier, we will obtain the following decision\n",
    "boundaries. These boundaries lines indicate where the model changes its\n",
    "prediction from one class to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# create a palette to be used in the scatterplot\n",
    "palette = [\"tab:red\", \"tab:blue\", \"black\"]\n",
    "\n",
    "linear_model = LogisticRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1], hue=target_column,\n",
    "                data=data, palette=palette, ax=ax)\n",
    "_ = plot_decision_function(linear_model, range_features, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the lines are a combination of the input features since they are\n",
    "not perpendicular a specific axis. Indeed, this is due to the model\n",
    "parametrization that we saw in the previous notebook, controlled by the\n",
    "model's weights and intercept.\n",
    "\n",
    "Besides, it seems that the linear model would be a good candidate model for\n",
    "such problem as it gives good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = linear_model.__class__.__name__\n",
    "test_score = linear_model.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Accuracy of the {model_name}: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear models, decision trees are non-parametric models: they are not\n",
    "controlled by a mathematical decision function and do not have weights or\n",
    "intercept to be optimized.\n",
    "\n",
    "Indeed, decision trees are based partition will partition the space by\n",
    "considering a single feature at a time. Let's illustrate this behaviour by\n",
    "having a decision tree that only makes a single split to partition the\n",
    "feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=1)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1], hue=target_column,\n",
    "                data=data, palette=palette, ax=ax)\n",
    "_ = plot_decision_function(tree, range_features, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitions found by the algorithm separates the data along the axis\n",
    "\"Culmen Length\", discarding the feature \"Culmen Depth\". Thus, it highlights\n",
    "that a decision tree does not use a combination of feature when making a\n",
    "split. We can look more in depth the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "_ = plot_tree(tree, feature_names=culmen_columns,\n",
    "              class_names=tree.classes_, impurity=False, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the split was done the culmen length feature. The original\n",
    "dataset was subdivided into 2 sets depending if the culmen depth was\n",
    "inferior or superior to 16.45 mm.\n",
    "\n",
    "This partition of the dataset is the one that minimize the class diversities\n",
    "in each sub-partitions. This measure is also known as called **criterion**\n",
    "and is a parameter that can be set in trees.\n",
    "\n",
    "If we look closely at the partition, the sample superior to 16.45 belong\n",
    "mainly to the Adelie class. Looking at the tree structure, we indeed observe\n",
    "103 Adelie samples. We also count 52 Chinstrap samples and 6 Gentoo samples.\n",
    "We can make similar interpretation for the partition defined by a threshold\n",
    "inferior to 16.45mm. In this case, the most represented class is the Gentoo\n",
    "specie.\n",
    "\n",
    "Let's see how our tree would work as a predictor. Let's start to see the\n",
    "class predicted when the culmen length is inferior to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict([[0, 15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class predicted is the Gentoo. We can now check if we pass a culmen\n",
    "depth superior to the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict([[0, 17]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the tree predicts the Adelie specie.\n",
    "\n",
    "Thus, we can conclude that a decision tree classifier will predict the most\n",
    "represented class within a partition.\n",
    "\n",
    "Since that during the training, we have a count of samples in each partition,\n",
    "we can also compute a probability to belong to a certain class within this\n",
    "partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = pd.Series(tree.predict_proba([[0, 17]])[0],\n",
    "                    index=tree.classes_)\n",
    "ax = y_proba.plot(kind=\"bar\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "_ = ax.set_title(\"Probability to belong to a penguin class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually compute the different probability directly from the tree\n",
    "structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Probabilities for the different classes:\\n\"\n",
    "      f\"Adelie: {103 / 161:.3f}\\n\"\n",
    "      f\"Chinstrap: {52 / 161:.3f}\\n\"\n",
    "      f\"Gentoo: {6 / 161:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to note that the culmen depth has been disregarded for\n",
    "the moment. It means that whatever the value given, it will not be used\n",
    "during the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict_proba([[10000, 17]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our classification problem, the split found with a maximum\n",
    "depth of 1 is not powerful enough to separate the three species and the model\n",
    "accuracy is low when compared to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = tree.__class__.__name__\n",
    "test_score = tree.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Accuracy of the {model_name}: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is not a surprise. We saw earlier that a single feature will not\n",
    "be able to separate all three species. However, from the previous analysis we\n",
    "saw that by using both features we should be able to get fairly good results.\n",
    "\n",
    "In the next exercise, you will increase the size of the tree depth. You will\n",
    "get intuitions on how the space partitioning is repeated over time."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
