{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of decision tree hyperparameters on generalization\n",
    "\n",
    "In this notebook will illustrate the importance of some key hyperparameters\n",
    "of the decision tree. We will illustrate it on both the classification and\n",
    "regression probelms that we previously used.\n",
    "\n",
    "## Load the classification and regression datasets\n",
    "\n",
    "First, we will load the classification and regression datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_clf_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_clf_column = \"Species\"\n",
    "data_clf = pd.read_csv(\"../datasets/penguins_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_columns = [\"Flipper Length (mm)\"]\n",
    "target_reg_column = \"Body Mass (g)\"\n",
    "data_reg = pd.read_csv(\"../datasets/penguins_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper functions\n",
    "\n",
    "We will create two functions that will:\n",
    "\n",
    "* fit a decision tree on some training data;\n",
    "* show the decision function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "\n",
    "def plot_classification(model, X, y, ax=None):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    model.fit(X, y)\n",
    "\n",
    "    range_features = {\n",
    "        feature_name: (X[feature_name].min() - 1, X[feature_name].max() + 1)\n",
    "        for feature_name in X.columns\n",
    "    }\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "    sns.scatterplot(\n",
    "        x=data_clf_columns[0], y=data_clf_columns[1], hue=target_clf_column,\n",
    "        data=data_clf, ax=axs[0], palette=[\"tab:red\", \"tab:blue\", \"black\"])\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_regression(model, X, y, ax=None):\n",
    "    model.fit(X, y)\n",
    "\n",
    "    X_test = pd.DataFrame(\n",
    "        np.arange(X.iloc[:, 0].min(), X.iloc[:, 0].max()),\n",
    "        columns=X.columns,\n",
    "    )\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=y, color=\"black\", alpha=0.5, ax=ax)\n",
    "    ax.plot(X_test, y_pred, linewidth=4)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Effect of the `max_depth` parameter\n",
    "\n",
    "In decision trees, the most important parameter to get a trade-off between\n",
    "under-fitting and over-fitting is the `max_depth` parameter. Let's build\n",
    "a shallow tree and then deeper tree (for both classification and regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "plot_classification(tree_clf, data_clf[data_clf_columns],\n",
    "                    data_clf[target_clf_column], ax=axs[0])\n",
    "plot_regression(tree_reg, data_reg[data_reg_columns],\n",
    "                data_reg[target_reg_column], ax=axs[1])\n",
    "fig.suptitle(f\"Shallow tree with a max-depth of {max_depth}\")\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 30\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree_reg = DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "plot_classification(tree_clf, data_clf[data_clf_columns],\n",
    "                    data_clf[target_clf_column], ax=axs[0])\n",
    "plot_regression(tree_reg, data_reg[data_reg_columns],\n",
    "                data_reg[target_reg_column], ax=axs[1])\n",
    "fig.suptitle(f\"Deep tree with a max-depth of {max_depth}\")\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both classification and regression setting, we can observe that\n",
    "increasing the depth will make the tree model more expressive. However, a\n",
    "tree that is too deep will overfit the training data, creating partitions\n",
    "which are only correct for \"outliers\". The `max_depth` is one of the\n",
    "hyperparameters that one should optimize via cross-validation and\n",
    "grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": np.arange(2, 10, 1)}\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "plot_classification(tree_clf, data_clf[data_clf_columns],\n",
    "                    data_clf[target_clf_column], ax=axs[0])\n",
    "plot_regression(tree_reg, data_reg[data_reg_columns],\n",
    "                data_reg[target_reg_column], ax=axs[1])\n",
    "axs[0].set_title(f\"Optimal depth found via CV: \"\n",
    "                 f\"{tree_clf.best_params_['max_depth']}\")\n",
    "axs[1].set_title(f\"Optimal depth found via CV: \"\n",
    "                 f\"{tree_reg.best_params_['max_depth']}\")\n",
    "plt.subplots_adjust(wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other parameters are used to fine tune the decision tree and have less\n",
    "impact than `max_depth`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
