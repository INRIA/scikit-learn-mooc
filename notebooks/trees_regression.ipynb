{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "architectural-cassette",
   "metadata": {},
   "source": [
    "# Decision tree for regression\n",
    "\n",
    "In this notebook, we present how decision trees are working in regression\n",
    "problems. We show differences with the decision trees previously presented in\n",
    "a classification setting.\n",
    "\n",
    "First, we will load the regression dataset presented at the beginning of this\n",
    "chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "penguins = pd.read_csv(\"../datasets/penguins_regression.csv\")\n",
    "\n",
    "data_columns = [\"Flipper Length (mm)\"]\n",
    "target_column = \"Body Mass (g)\"\n",
    "\n",
    "data_train, target_train = penguins[data_columns], penguins[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-kennedy",
   "metadata": {},
   "source": [
    "To illustrate how decision trees are predicting in a regression setting, we\n",
    "will create a synthetic dataset containing all possible flipper length from\n",
    "the minimum to the maximum of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_test = pd.DataFrame(np.arange(data_train[data_columns[0]].min(),\n",
    "                                   data_train[data_columns[0]].max()),\n",
    "                         columns=data_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_ = sns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-florida",
   "metadata": {},
   "source": [
    "We will first illustrate the difference between a linear model and a decision\n",
    "tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(data_train, target_train)\n",
    "target_predicted = linear_model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data_test, target_predicted, linewidth=4, label=\"Linear regression\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-sixth",
   "metadata": {},
   "source": [
    "On the plot above, we see that a non-regularized `LinearRegression` is able\n",
    "to fit the data. A feature of this model is that all new predictions\n",
    "will be on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data_test, target_predicted, linewidth=4, label=\"Linear regression\")\n",
    "ax.plot(data_test[::3], target_predicted[::3], label=\"Test predictions\",\n",
    "        color=\"tab:orange\", marker=\".\", markersize=15, linestyle=\"\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-phoenix",
   "metadata": {},
   "source": [
    "Contrary to linear models, decision trees are non-parametric models:\n",
    "they do not make assumptions about the way data is distributed.\n",
    "This will affect the prediction scheme. Repeating the above experiment\n",
    "will highlight the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=1)\n",
    "tree.fit(data_train, target_train)\n",
    "target_predicted = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data_test, target_predicted, linewidth=4, label=\"Decision tree\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-rolling",
   "metadata": {},
   "source": [
    "We see that the decision tree model does not have an *a priori* distribution\n",
    "for the data and we do not end-up with a straight line to regress flipper\n",
    "length and body mass.\n",
    "\n",
    "Instead, we observe that the predictions of the tree are piecewise constant.\n",
    "Indeed, our feature space was split into two partitions. Let's check the\n",
    "tree structure to see what was the threshold found during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "_ = plot_tree(tree, feature_names=data_columns, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-emission",
   "metadata": {},
   "source": [
    "The threshold for our feature (flipper length) is 202.5 mm. The predicted\n",
    "values on each side of the split are two constants: 3683.50 g and 5023.62 g.\n",
    "These values corresponds to the mean values of the training samples in each\n",
    "partition.\n",
    "\n",
    "In classification, we saw that increasing the depth of the tree allowed us to\n",
    "get more complex decision boundaries.\n",
    "Let's check the effect of increasing the depth in a regression setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=3)\n",
    "tree.fit(data_train, target_train)\n",
    "target_predicted = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\",\n",
    "                     color=\"black\", alpha=0.5)\n",
    "ax.plot(data_test, target_predicted, linewidth=4, label=\"Decision tree\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-background",
   "metadata": {},
   "source": [
    "Increasing the depth of the tree will increase the number of partition and\n",
    "thus the number of constant values that the tree is capable of predicting.\n",
    "\n",
    "In this notebook, we highlighted the differences in behavior of a decision\n",
    "tree used in a classification problem in contrast to a regression problem."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
